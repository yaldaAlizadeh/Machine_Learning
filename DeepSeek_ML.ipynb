{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yaldaAlizadeh/Machine_Learning/blob/main/DeepSeek_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-w7FEyB00oe_"
      },
      "source": [
        "I want you to generate below content, which includes everything. Convert all of the answe (exact content) into a runnable Jupyter notebook with different cells for text content and for code cells that I can run the codes and see the output and then go to the next cell. If you can not generate the ipynb, Generate regular answer that I can copy that and not json file.# Comprehensive Machine Learning Algorithms Guide\n",
        "I'll create a comprehensive guide covering all algorithms from your cheatsheet. Since I can't generate actual .ipynb files, I'll structure this as a complete Jupyter notebook with markdown cells and code cells that you can copy into a new notebook."
      ],
      "id": "-w7FEyB00oe_"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31-j3NLk0ofB"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Cell 1: Installation and Setup\n",
        "# Run this cell first to install required packages\n",
        "!pip install numpy pandas matplotlib seaborn scikit-learn xgboost tensorflow torch transformers plotly graphviz missingno -q\n",
        "print(\"âœ… All packages installed successfully!\")"
      ],
      "id": "31-j3NLk0ofB"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b63cqCFZ0ofC"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Cell 2: Import All Required Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "# Scikit-learn imports\n",
        "from sklearn import datasets, linear_model, model_selection, metrics, tree, ensemble, svm, neighbors, naive_bayes, cluster, decomposition, neural_network\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, PolynomialFeatures\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.tree import plot_tree, export_text\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.calibration import calibration_curve\n",
        "# Advanced models\n",
        "import xgboost as xgb\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, Sequential\n",
        "from transformers import pipeline\n",
        "# Statistical analysis\n",
        "import scipy.stats as stats\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "print(\"ðŸ“š All libraries imported successfully!\")"
      ],
      "id": "b63cqCFZ0ofC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNiE77OA0ofC"
      },
      "source": [
        "# 1. LINEAR REGRESSION\n",
        "## Algorithm Background & Mathematical Foundation\n",
        "**Core Concept:** Linear Regression models the relationship between a dependent variable (Y) and one or more independent variables (X) using a linear approach. It's the foundation of predictive modeling and statistical analysis.\n",
        "**Mathematical Formulation:**\n",
        "**Model Equation:**\n",
        "$$Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p + \\epsilon$$\n",
        "**Matrix Form:**\n",
        "$$\\mathbf{Y} = \\mathbf{X}\\beta + \\epsilon$$\n",
        "**Cost Function (Mean Squared Error):**\n",
        "$$J(\\beta) = \\frac{1}{2m}\\sum_{i=1}^{m}(h_\\beta(x^{(i)}) - y^{(i)})^2 = \\frac{1}{2m}(X\\beta - y)^T(X\\beta - y)$$\n",
        "**Closed-form Solution (Normal Equation):**\n",
        "$$\\hat{\\beta} = (X^TX)^{-1}X^Ty$$\n",
        "**Gradient Descent Update:**\n",
        "$$\\beta_j := \\beta_j - \\alpha\\frac{\\partial}{\\partial\\beta_j}J(\\beta) = \\beta_j - \\alpha\\frac{1}{m}\\sum_{i=1}^m(h_\\beta(x^{(i)}) - y^{(i)})x_j^{(i)}$$\n",
        "## Key Assumptions (Gauss-Markov Theorem)\n",
        "1. **Linearity:** Relationship between X and Y is linear\n",
        "2. **Independence:** Observations are independent of each other\n",
        "3. **Homoscedasticity:** Constant variance of errors\n",
        "4. **No Autocorrelation:** Errors are uncorrelated\n",
        "5. **No Perfect Multicollinearity:** Predictors not perfectly correlated\n",
        "6. **Normality:** Errors normally distributed (for inference)\n",
        "## Types of Linear Regression\n",
        "- **Simple Linear Regression:** One predictor variable\n",
        "- **Multiple Linear Regression:** Multiple predictors\n",
        "- **Polynomial Regression:** Non-linear relationships via polynomial features\n",
        "- **Regularized Regression:** Ridge (L2), Lasso (L1), Elastic Net"
      ],
      "id": "ZNiE77OA0ofC"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcFujAIF0ofD"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Cell 3: Linear Regression - Comprehensive Implementation\n",
        "print(\"ðŸš€ LINEAR REGRESSION: COMPREHENSIVE IMPLEMENTATION\\n\")\n",
        "# Load dataset\n",
        "boston = datasets.fetch_california_housing()\n",
        "X = boston.data\n",
        "y = boston.target\n",
        "feature_names = boston.feature_names\n",
        "print(\"ðŸ“Š Dataset Overview:\")\n",
        "print(f\"â€¢ Dataset: California Housing\")\n",
        "print(f\"â€¢ Samples: {X.shape[0]}, Features: {X.shape[1]}\")\n",
        "print(f\"â€¢ Features: {list(feature_names)}\")\n",
        "print(f\"â€¢ Target: Median House Value\")\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(X, columns=feature_names)\n",
        "df['MedHouseVal'] = y\n",
        "# Exploratory Data Analysis\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "# Distribution of target variable\n",
        "axes[0,0].hist(df['MedHouseVal'], bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "axes[0,0].set_title('Distribution of Median House Values')\n",
        "axes[0,0].set_xlabel('Median House Value')\n",
        "axes[0,0].set_ylabel('Frequency')\n",
        "# Correlation heatmap\n",
        "correlation_matrix = df.corr()\n",
        "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0, ax=axes[0,1])\n",
        "axes[0,1].set_title('Feature Correlation Matrix')\n",
        "# Feature vs Target relationships\n",
        "axes[1,0].scatter(df['MedInc'], df['MedHouseVal'], alpha=0.5)\n",
        "axes[1,0].set_xlabel('Median Income')\n",
        "axes[1,0].set_ylabel('Median House Value')\n",
        "axes[1,0].set_title('Income vs House Value')\n",
        "# Feature distribution\n",
        "axes[1,1].boxplot([df['MedInc'], df['HouseAge'], df['AveRooms']])\n",
        "axes[1,1].set_xticklabels(['MedInc', 'HouseAge', 'AveRooms'])\n",
        "axes[1,1].set_title('Feature Distributions')\n",
        "axes[1,1].set_ylabel('Value')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "# Data Preprocessing\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
        "X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "# Train Multiple Linear Regression Models\n",
        "models = {\n",
        "'Ordinary Least Squares': linear_model.LinearRegression(),\n",
        "'Ridge Regression (L2)': linear_model.Ridge(alpha=1.0),\n",
        "'Lasso Regression (L1)': linear_model.Lasso(alpha=0.1),\n",
        "'Elastic Net': linear_model.ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
        "}\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "print(f\"\\nðŸ“Š Training {name}...\")\n",
        "model.fit(X_train_scaled, y_train)\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "mse = metrics.mean_squared_error(y_test, y_pred)\n",
        "r2 = metrics.r2_score(y_test, y_pred)\n",
        "mae = metrics.mean_absolute_error(y_test, y_pred)\n",
        "results[name] = {\n",
        "'mse': mse,\n",
        "'r2': r2,\n",
        "'mae': mae,\n",
        "'model': model,\n",
        "'y_pred': y_pred\n",
        "}\n",
        "print(f\" â€¢ MSE: {mse:.4f}\")\n",
        "print(f\" â€¢ RÂ²: {r2:.4f}\")\n",
        "print(f\" â€¢ MAE: {mae:.4f}\")\n",
        "# Model Comparison\n",
        "comparison_df = pd.DataFrame({\n",
        "'Model': list(results.keys()),\n",
        "'MSE': [results[name]['mse'] for name in results.keys()],\n",
        "'R2': [results[name]['r2'] for name in results.keys()],\n",
        "'MAE': [results[name]['mae'] for name in results.keys()]\n",
        "})\n",
        "print(\"\\nðŸ“‹ Model Performance Comparison:\")\n",
        "print(comparison_df.round(4))\n",
        "# Visualization of Results\n",
        "best_model_name = max(results.keys(), key=lambda x: results[x]['r2'])\n",
        "best_model = results[best_model_name]['model']\n",
        "y_pred_best = results[best_model_name]['y_pred']\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "# Predictions vs Actual\n",
        "axes[0,0].scatter(y_test, y_pred_best, alpha=0.6)\n",
        "axes[0,0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "axes[0,0].set_xlabel('Actual Values')\n",
        "axes[0,0].set_ylabel('Predicted Values')\n",
        "axes[0,0].set_title(f'{best_model_name}: Predictions vs Actual')\n",
        "axes[0,0].grid(True)\n",
        "# Residuals plot\n",
        "residuals = y_test - y_pred_best\n",
        "axes[0,1].scatter(y_pred_best, residuals, alpha=0.6)\n",
        "axes[0,1].axhline(y=0, color='r', linestyle='--')\n",
        "axes[0,1].set_xlabel('Predicted Values')\n",
        "axes[0,1].set_ylabel('Residuals')\n",
        "axes[0,1].set_title('Residuals Plot')\n",
        "axes[0,1].grid(True)\n",
        "# Distribution of residuals\n",
        "axes[1,0].hist(residuals, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "axes[1,0].set_xlabel('Residuals')\n",
        "axes[1,0].set_ylabel('Frequency')\n",
        "axes[1,0].set_title('Distribution of Residuals')\n",
        "axes[1,0].grid(True)\n",
        "# Feature importance for interpretable models\n",
        "if hasattr(best_model, 'coef_'):\n",
        "feature_importance = pd.DataFrame({\n",
        "'feature': feature_names,\n",
        "'coefficient': best_model.coef_\n",
        "}).sort_values('coefficient', key=abs, ascending=False)\n",
        "axes[1,1].barh(feature_importance['feature'][:10], feature_importance['coefficient'][:10])\n",
        "axes[1,1].set_xlabel('Coefficient Value')\n",
        "axes[1,1].set_title('Top 10 Feature Coefficients')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "print(\"âœ… Linear Regression Analysis Complete!\")"
      ],
      "id": "AcFujAIF0ofD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NUo8uRB0ofD"
      },
      "source": [
        "## Linear Regression Interview Questions & Answers\n",
        "**Q1: What is the difference between L1 (Lasso) and L2 (Ridge) regularization?**\n",
        "**Answer:**\n",
        "- **L1 Regularization (Lasso):** Adds absolute value of coefficients as penalty: $J(\\beta) = MSE + \\lambda\\sum|\\beta_i|$\n",
        "- Can shrink coefficients to exactly zero, performing feature selection\n",
        "- Creates sparse models\n",
        "- **L2 Regularization (Ridge):** Adds squared value of coefficients as penalty: $J(\\beta) = MSE + \\lambda\\sum\\beta_i^2$\n",
        "- Shrinks coefficients but rarely to exactly zero\n",
        "- Handles multicollinearity well\n",
        "**Q2: How do you interpret the R-squared value?**\n",
        "**Answer:**\n",
        "R-squared measures the proportion of variance in the dependent variable that is predictable from the independent variables.\n",
        "- RÂ² = 1: Perfect fit\n",
        "- RÂ² = 0: Model explains none of the variance\n",
        "- RÂ² < 0: Model is worse than horizontal line\n",
        "- **Important:** High RÂ² doesn't guarantee model quality\n",
        "**Q3: What is multicollinearity and why is it problematic?**\n",
        "**Answer:**\n",
        "Multicollinearity occurs when independent variables are highly correlated. Problems include:\n",
        "- Unstable coefficient estimates\n",
        "- Difficulty interpreting individual feature importance\n",
        "- Increased variance of coefficient estimates\n",
        "- Detection: Variance Inflation Factor (VIF) > 10 indicates severe multicollinearity\n",
        "**Q4: Explain the bias-variance tradeoff.**\n",
        "**Answer:**\n",
        "Total Error = BiasÂ² + Variance + Irreducible Error\n",
        "- **Bias:** Error from erroneous assumptions (underfitting)\n",
        "- **Variance:** Error from sensitivity to training data (overfitting)\n",
        "- Simple models: High bias, low variance\n",
        "- Complex models: Low bias, high variance\n",
        "**Q5: How do you validate a Linear Regression model?**\n",
        "**Answer:**\n",
        "1. Check assumptions (linearity, homoscedasticity, normality)\n",
        "2. Use train/test split or cross-validation\n",
        "3. Analyze residual plots\n",
        "4. Calculate performance metrics (MSE, RÂ², MAE)\n",
        "5. Check for influential points using Cook's distance\n",
        "# 2. LOGISTIC REGRESSION\n",
        "## Algorithm Background & Mathematical Foundation\n",
        "**Core Concept:** Despite the name, Logistic Regression is a classification algorithm that estimates probabilities using a logistic function.\n",
        "**Mathematical Formulation:**\n",
        "**Sigmoid Function:**\n",
        "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
        "**Model Equation:**\n",
        "$$P(Y=1|X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p)}}$$\n",
        "**Log-Odds (Logit Function):**\n",
        "$$\\log\\left(\\frac{P(Y=1)}{1 - P(Y=1)}\\right) = \\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p$$\n",
        "**Cost Function (Log Loss):**\n",
        "$$J(\\beta) = -\\frac{1}{m}\\sum_{i=1}^{m}[y_i\\log(\\hat{y}_i) + (1-y_i)\\log(1-\\hat{y}_i)]$$\n",
        "## Key Assumptions\n",
        "1. **Binary outcome** variable\n",
        "2. **Linearity** between independent variables and log-odds\n",
        "3. **No multicollinearity** among independent variables\n",
        "4. **Independent** observations\n",
        "5. **Large sample** size"
      ],
      "id": "7NUo8uRB0ofD"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5K1xevvd0ofD"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Cell 4: Logistic Regression - Comprehensive Implementation\n",
        "print(\"ðŸš€ LOGISTIC REGRESSION: COMPREHENSIVE IMPLEMENTATION\\n\")\n",
        "# Load dataset\n",
        "breast_cancer = datasets.load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "feature_names = breast_cancer.feature_names\n",
        "target_names = breast_cancer.target_names\n",
        "print(\"ðŸ“Š Dataset Overview:\")\n",
        "print(f\"â€¢ Dataset: Wisconsin Breast Cancer\")\n",
        "print(f\"â€¢ Samples: {X.shape[0]}, Features: {X.shape[1]}\")\n",
        "print(f\"â€¢ Classes: {target_names}\")\n",
        "print(f\"â€¢ Class distribution: {np.bincount(y)}\")\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(X, columns=feature_names)\n",
        "df['diagnosis'] = y\n",
        "# Exploratory Data Analysis\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "# Class distribution\n",
        "class_counts = df['diagnosis'].value_counts()\n",
        "axes[0,0].pie(class_counts.values, labels=target_names, autopct='%1.1f%%',\n",
        "colors=['lightcoral', 'lightgreen'])\n",
        "axes[0,0].set_title('Class Distribution')\n",
        "# Feature correlation with target\n",
        "correlation_with_target = df.corr()['diagnosis'].sort_values(ascending=False)\n",
        "top_features = correlation_with_target[1:11]\n",
        "axes[0,1].barh(range(len(top_features)), top_features.values)\n",
        "axes[0,1].set_yticks(range(len(top_features)))\n",
        "axes[0,1].set_yticklabels(top_features.index)\n",
        "axes[0,1].set_xlabel('Correlation with Diagnosis')\n",
        "axes[0,1].set_title('Top 10 Features Correlated with Diagnosis')\n",
        "# Feature distribution by class\n",
        "for diagnosis in [0, 1]:\n",
        "subset = df[df['diagnosis'] == diagnosis]\n",
        "axes[1,0].scatter(subset['worst radius'], subset['worst texture'],\n",
        "label=target_names[diagnosis], alpha=0.6)\n",
        "axes[1,0].set_xlabel('Worst Radius')\n",
        "axes[1,0].set_ylabel('Worst Texture')\n",
        "axes[1,0].set_title('Feature Space: Malignant vs Benign')\n",
        "axes[1,0].legend()\n",
        "axes[1,0].grid(True)\n",
        "# Data preprocessing\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
        "X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "# Train Logistic Regression models\n",
        "logistic_models = {\n",
        "'Logistic Regression (L2)': linear_model.LogisticRegression(penalty='l2', C=1.0, random_state=42, max_iter=1000),\n",
        "'Logistic Regression (L1)': linear_model.LogisticRegression(penalty='l1', solver='liblinear', C=1.0, random_state=42, max_iter=1000),\n",
        "'Logistic Regression (Balanced)': linear_model.LogisticRegression(class_weight='balanced', random_state=42, max_iter=1000)\n",
        "}\n",
        "logistic_results = {}\n",
        "for name, model in logistic_models.items():\n",
        "print(f\"\\nðŸ“Š Training {name}...\")\n",
        "model.fit(X_train_scaled, y_train)\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
        "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
        "precision = metrics.precision_score(y_test, y_pred)\n",
        "recall = metrics.recall_score(y_test, y_pred)\n",
        "f1 = metrics.f1_score(y_test, y_pred)\n",
        "roc_auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
        "logistic_results[name] = {\n",
        "'accuracy': accuracy,\n",
        "'precision': precision,\n",
        "'recall': recall,\n",
        "'f1': f1,\n",
        "'roc_auc': roc_auc,\n",
        "'model': model,\n",
        "'y_pred_proba': y_pred_proba\n",
        "}\n",
        "print(f\" â€¢ Accuracy: {accuracy:.4f}\")\n",
        "print(f\" â€¢ Precision: {precision:.4f}\")\n",
        "print(f\" â€¢ Recall: {recall:.4f}\")\n",
        "print(f\" â€¢ F1-Score: {f1:.4f}\")\n",
        "print(f\" â€¢ ROC-AUC: {roc_auc:.4f}\")\n",
        "# Model Comparison\n",
        "comparison_logistic = pd.DataFrame({\n",
        "'Model': list(logistic_results.keys()),\n",
        "'Accuracy': [logistic_results[name]['accuracy'] for name in logistic_results.keys()],\n",
        "'Precision': [logistic_results[name]['precision'] for name in logistic_results.keys()],\n",
        "'Recall': [logistic_results[name]['recall'] for name in logistic_results.keys()],\n",
        "'F1-Score': [logistic_results[name]['f1'] for name in logistic_results.keys()],\n",
        "'ROC-AUC': [logistic_results[name]['roc_auc'] for name in logistic_results.keys()]\n",
        "})\n",
        "print(\"\\nðŸ“‹ Model Performance Comparison:\")\n",
        "print(comparison_logistic.round(4))\n",
        "# Comprehensive Visualization\n",
        "best_logistic_name = max(logistic_results.keys(), key=lambda x: logistic_results[x]['roc_auc'])\n",
        "best_logistic_model = logistic_results[best_logistic_name]['model']\n",
        "y_pred_proba_best = logistic_results[best_logistic_name]['y_pred_proba']\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "# ROC Curve\n",
        "fpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba_best)\n",
        "roc_auc = metrics.auc(fpr, tpr)\n",
        "axes[0,0].plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "axes[0,0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
        "axes[0,0].set_xlabel('False Positive Rate')\n",
        "axes[0,0].set_ylabel('True Positive Rate')\n",
        "axes[0,0].set_title('ROC Curve')\n",
        "axes[0,0].legend()\n",
        "axes[0,0].grid(True)\n",
        "# Precision-Recall Curve\n",
        "precision, recall, _ = metrics.precision_recall_curve(y_test, y_pred_proba_best)\n",
        "average_precision = metrics.average_precision_score(y_test, y_pred_proba_best)\n",
        "axes[0,1].plot(recall, precision, color='blue', lw=2, label=f'Avg Precision = {average_precision:.2f}')\n",
        "axes[0,1].set_xlabel('Recall')\n",
        "axes[0,1].set_ylabel('Precision')\n",
        "axes[0,1].set_title('Precision-Recall Curve')\n",
        "axes[0,1].legend()\n",
        "axes[0,1].grid(True)\n",
        "# Probability Distribution\n",
        "axes[0,2].hist(y_pred_proba_best[y_test == 0], bins=30, alpha=0.7, color='red', label='Malignant', density=True)\n",
        "axes[0,2].hist(y_pred_proba_best[y_test == 1], bins=30, alpha=0.7, color='green', label='Benign', density=True)\n",
        "axes[0,2].axvline(x=0.5, color='black', linestyle='--', label='Decision Boundary')\n",
        "axes[0,2].set_xlabel('Predicted Probability')\n",
        "axes[0,2].set_ylabel('Density')\n",
        "axes[0,2].set_title('Probability Distribution by Class')\n",
        "axes[0,2].legend()\n",
        "# Confusion Matrix\n",
        "y_pred_best = best_logistic_model.predict(X_test_scaled)\n",
        "cm = metrics.confusion_matrix(y_test, y_pred_best)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1,0],\n",
        "xticklabels=target_names, yticklabels=target_names)\n",
        "axes[1,0].set_xlabel('Predicted')\n",
        "axes[1,0].set_ylabel('Actual')\n",
        "axes[1,0].set_title('Confusion Matrix')\n",
        "# Feature Importance\n",
        "if hasattr(best_logistic_model, 'coef_'):\n",
        "feature_importance = pd.DataFrame({\n",
        "'feature': feature_names,\n",
        "'coefficient': best_logistic_model.coef_[0],\n",
        "'abs_coefficient': np.abs(best_logistic_model.coef_[0])\n",
        "}).sort_values('abs_coefficient', ascending=True).tail(10)\n",
        "axes[1,1].barh(feature_importance['feature'], feature_importance['coefficient'])\n",
        "axes[1,1].set_xlabel('Coefficient Value')\n",
        "axes[1,1].set_title('Top 10 Feature Coefficients')\n",
        "# Model Comparison\n",
        "models_list = list(logistic_results.keys())\n",
        "roc_auc_scores = [logistic_results[name]['roc_auc'] for name in models_list]\n",
        "bars = axes[1,2].bar(models_list, roc_auc_scores, color=['blue', 'green', 'orange'])\n",
        "axes[1,2].set_ylabel('ROC-AUC Score')\n",
        "axes[1,2].set_title('Model Comparison: ROC-AUC Scores')\n",
        "axes[1,2].tick_params(axis='x', rotation=45)\n",
        "for bar, score in zip(bars, roc_auc_scores):\n",
        "axes[1,2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "f'{score:.3f}', ha='center', va='bottom')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "print(\"âœ… Logistic Regression Analysis Complete!\")"
      ],
      "id": "5K1xevvd0ofD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Je8sCM040ofE"
      },
      "source": [
        "## Logistic Regression Interview Questions & Answers\n",
        "**Q1: Why can't we use Linear Regression for classification?**\n",
        "**Answer:**\n",
        "- Linear Regression predicts continuous values outside [0,1] range\n",
        "- It doesn't provide probability estimates naturally\n",
        "- Sensitive to outliers which can drastically affect decision boundary\n",
        "- Assumes linear relationship which doesn't hold for classification boundaries\n",
        "**Q2: How do you interpret coefficients in Logistic Regression?**\n",
        "**Answer:**\n",
        "- **Coefficient Î²â±¼:** Change in log-odds for one unit increase in Xâ±¼\n",
        "- **Odds Ratio e^{Î²â±¼}:** Multiplicative change in odds for one unit increase\n",
        "- Example: Î² = 0.7 means odds increase by 100% (e^0.7 â‰ˆ 2.01)\n",
        "**Q3: What is the cost function and why is it used?**\n",
        "**Answer:**\n",
        "**Log Loss:** $J(\\beta) = -\\frac{1}{m}\\sum_{i=1}^{m}[y_i\\log(\\hat{y}_i) + (1-y_i)\\log(1-\\hat{y}_i)]$\n",
        "- Convex function guaranteeing convergence\n",
        "- Heavily penalizes confident wrong predictions\n",
        "- Directly related to maximum likelihood estimation\n",
        "**Q4: How do you handle multiclass classification?**\n",
        "**Answer:**\n",
        "1. **One-vs-Rest (OvR):** Train K binary classifiers\n",
        "2. **One-vs-One (OvO):** Train $\\binom{K}{2}$ binary classifiers\n",
        "3. **Multinomial/Softmax:** Direct extension using softmax function\n",
        "- Softmax: $P(Y=k|X) = \\frac{e^{\\beta_k^TX}}{\\sum_{j=1}^{K}e^{\\beta_j^TX}}$\n",
        "**Q5: What evaluation metrics are appropriate?**\n",
        "**Answer:**\n",
        "- **Threshold-dependent:** Accuracy, Precision, Recall, F1-Score\n",
        "- **Threshold-independent:** ROC-AUC, Precision-Recall AUC\n",
        "- **Probability calibration:** Log Loss, Brier Score\n",
        "- Choose based on business objectives and class balance\n",
        "# 3. DECISION TREE\n",
        "## Algorithm Background & Mathematical Foundation\n",
        "**Core Concept:** Decision Trees learn hierarchical if-else rules by recursively partitioning the feature space based on feature values.\n",
        "**Mathematical Formulation:**\n",
        "**For Classification:**\n",
        "**Gini Impurity:**\n",
        "$$Gini(t) = 1 - \\sum_{i=1}^{c} p(i|t)^2$$\n",
        "**Information Gain (Entropy):**\n",
        "$$Entropy(t) = -\\sum_{i=1}^{c} p(i|t)\\log_2 p(i|t)$$\n",
        "$$Information\\ Gain = Entropy(parent) - \\sum_{j=1}^{k} \\frac{N_j}{N} Entropy(child_j)$$\n",
        "**For Regression:**\n",
        "**Variance Reduction:**\n",
        "$$Variance(t) = \\frac{1}{N_t}\\sum_{i=1}^{N_t} (y_i - \\bar{y}_t)^2$$\n",
        "## Key Features\n",
        "- **Non-parametric:** No assumptions about data distribution\n",
        "- **Handles mixed data types:** Numerical and categorical\n",
        "- **Interpretable:** Easy to understand and explain\n",
        "- **No feature scaling required**"
      ],
      "id": "Je8sCM040ofE"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1MGUs7t0ofE"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Cell 5: Decision Tree - Comprehensive Implementation\n",
        "print(\"ðŸš€ DECISION TREE: COMPREHENSIVE IMPLEMENTATION\\n\")\n",
        "# Load dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "class_names = iris.target_names\n",
        "print(\"ðŸ“Š Dataset Overview:\")\n",
        "print(f\"â€¢ Dataset: Iris Flowers\")\n",
        "print(f\"â€¢ Samples: {X.shape[0]}, Features: {X.shape[1]}\")\n",
        "print(f\"â€¢ Classes: {list(class_names)}\")\n",
        "print(f\"â€¢ Class distribution: {np.bincount(y)}\")\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(X, columns=feature_names)\n",
        "df['species'] = y\n",
        "df['species_name'] = df['species'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n",
        "# Exploratory Data Analysis\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "# Feature relationships\n",
        "scatter = axes[0,0].scatter(df['sepal length (cm)'], df['sepal width (cm)'],\n",
        "c=df['species'], cmap='viridis')\n",
        "axes[0,0].set_xlabel('Sepal Length (cm)')\n",
        "axes[0,0].set_ylabel('Sepal Width (cm)')\n",
        "axes[0,0].set_title('Sepal Length vs Width')\n",
        "plt.colorbar(scatter, ax=axes[0,0])\n",
        "# Feature distributions by class\n",
        "for species in range(3):\n",
        "subset = df[df['species'] == species]\n",
        "axes[0,1].hist(subset['petal length (cm)'], alpha=0.7, label=class_names[species])\n",
        "axes[0,1].set_xlabel('Petal Length (cm)')\n",
        "axes[0,1].set_ylabel('Frequency')\n",
        "axes[0,1].set_title('Petal Length Distribution by Species')\n",
        "axes[0,1].legend()\n",
        "# Correlation heatmap\n",
        "correlation_matrix = df.iloc[:, :4].corr()\n",
        "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0, ax=axes[1,0])\n",
        "axes[1,0].set_title('Feature Correlation Matrix')\n",
        "# Class distribution\n",
        "class_counts = df['species'].value_counts()\n",
        "axes[1,1].bar(class_names, class_counts.values, color=['skyblue', 'lightgreen', 'lightcoral'])\n",
        "axes[1,1].set_title('Class Distribution')\n",
        "axes[1,1].set_ylabel('Count')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "# Data preprocessing\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
        "X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "# Train Decision Tree models with different parameters\n",
        "tree_models = {\n",
        "'Decision Tree (Gini, depth=3)': tree.DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=42),\n",
        "'Decision Tree (Entropy, depth=5)': tree.DecisionTreeClassifier(criterion='entropy', max_depth=5, random_state=42),\n",
        "'Decision Tree (Unlimited)': tree.DecisionTreeClassifier(random_state=42)\n",
        "}\n",
        "tree_results = {}\n",
        "for name, model in tree_models.items():\n",
        "print(f\"\\nðŸ“Š Training {name}...\")\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_proba = model.predict_proba(X_test)\n",
        "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
        "precision = metrics.precision_score(y_test, y_pred, average='weighted')\n",
        "recall = metrics.recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = metrics.f1_score(y_test, y_pred, average='weighted')\n",
        "tree_results[name] = {\n",
        "'accuracy': accuracy,\n",
        "'precision': precision,\n",
        "'recall': recall,\n",
        "'f1': f1,\n",
        "'model': model,\n",
        "'y_pred': y_pred\n",
        "}\n",
        "print(f\" â€¢ Accuracy: {accuracy:.4f}\")\n",
        "print(f\" â€¢ Precision: {precision:.4f}\")\n",
        "print(f\" â€¢ Recall: {recall:.4f}\")\n",
        "print(f\" â€¢ F1-Score: {f1:.4f}\")\n",
        "print(f\" â€¢ Tree Depth: {model.get_depth()}\")\n",
        "print(f\" â€¢ Number of Leaves: {model.get_n_leaves()}\")\n",
        "# Model Comparison\n",
        "comparison_tree = pd.DataFrame({\n",
        "'Model': list(tree_results.keys()),\n",
        "'Accuracy': [tree_results[name]['accuracy'] for name in tree_results.keys()],\n",
        "'Precision': [tree_results[name]['precision'] for name in tree_results.keys()],\n",
        "'Recall': [tree_results[name]['recall'] for name in tree_results.keys()],\n",
        "'F1-Score': [tree_results[name]['f1'] for name in tree_results.keys()],\n",
        "'Depth': [tree_results[name]['model'].get_depth() for name in tree_results.keys()],\n",
        "'Leaves': [tree_results[name]['model'].get_n_leaves() for name in tree_results.keys()]\n",
        "})\n",
        "print(\"\\nðŸ“‹ Model Performance Comparison:\")\n",
        "print(comparison_tree.round(4))\n",
        "# Comprehensive Visualization\n",
        "best_tree_name = max(tree_results.keys(), key=lambda x: tree_results[x]['accuracy'])\n",
        "best_tree_model = tree_results[best_tree_name]['model']\n",
        "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
        "# Visualize the decision tree\n",
        "plt.figure(figsize=(15, 10))\n",
        "plot_tree(best_tree_model, feature_names=feature_names, class_names=class_names,\n",
        "filled=True, rounded=True, fontsize=10)\n",
        "plt.title(f'Decision Tree Visualization - {best_tree_name}')\n",
        "plt.show()\n",
        "# Feature Importance\n",
        "feature_importance = pd.DataFrame({\n",
        "'feature': feature_names,\n",
        "'importance': best_tree_model.feature_importances_\n",
        "}).sort_values('importance', ascending=True)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(feature_importance['feature'], feature_importance['importance'])\n",
        "plt.xlabel('Feature Importance')\n",
        "plt.title('Decision Tree Feature Importance')\n",
        "plt.grid(True, axis='x')\n",
        "plt.show()\n",
        "# Decision Boundary Visualization (using first two features)\n",
        "X_2d = X[:, :2] # Use only first two features for visualization\n",
        "X_train_2d, X_test_2d, y_train_2d, y_test_2d = model_selection.train_test_split(\n",
        "X_2d, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "# Train on 2D data for visualization\n",
        "tree_2d = tree.DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "tree_2d.fit(X_train_2d, y_train_2d)\n",
        "# Create mesh grid\n",
        "x_min, x_max = X_2d[:, 0].min() - 1, X_2d[:, 0].max() + 1\n",
        "y_min, y_max = X_2d[:, 1].min() - 1, X_2d[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
        "np.arange(y_min, y_max, 0.02))\n",
        "# Predict on mesh grid\n",
        "Z = tree_2d.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "# Plot decision boundaries\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.contourf(xx, yy, Z, alpha=0.8, cmap='viridis')\n",
        "scatter = plt.scatter(X_2d[:, 0], X_2d[:, 1], c=y, edgecolor='black', s=50, cmap='viridis')\n",
        "plt.xlabel(feature_names[0])\n",
        "plt.ylabel(feature_names[1])\n",
        "plt.title('Decision Tree Decision Boundaries (First Two Features)')\n",
        "plt.colorbar(scatter)\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "# Overfitting Analysis\n",
        "depths = range(1, 15)\n",
        "train_scores = []\n",
        "test_scores = []\n",
        "for depth in depths:\n",
        "dt_temp = tree.DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
        "dt_temp.fit(X_train, y_train)\n",
        "train_scores.append(dt_temp.score(X_train, y_train))\n",
        "test_scores.append(dt_temp.score(X_test, y_test))\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(depths, train_scores, 'o-', label='Training Score', color='blue')\n",
        "plt.plot(depths, test_scores, 'o-', label='Test Score', color='red')\n",
        "plt.xlabel('Tree Depth')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Decision Tree: Overfitting Analysis')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "print(\"âœ… Decision Tree Analysis Complete!\")"
      ],
      "id": "l1MGUs7t0ofE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeFG6W2M0ofE"
      },
      "source": [
        "## Decision Tree Interview Questions & Answers\n",
        "**Q1: What are the splitting criteria and when to use each?**\n",
        "**Answer:**\n",
        "- **Gini Impurity:** $Gini(t) = 1 - \\sum p(i|t)^2$\n",
        "- Faster to compute, default in scikit-learn\n",
        "- Tends to find larger splits\n",
        "- **Information Gain (Entropy):** $Entropy(t) = -\\sum p(i|t)\\log_2 p(i|t)$\n",
        "- More balanced splits\n",
        "- Slightly more computationally expensive\n",
        "- **Choice:** Usually similar results, Gini is slightly faster\n",
        "**Q2: How does a Decision Tree handle numerical vs categorical features?**\n",
        "**Answer:**\n",
        "- **Numerical features:** Finds optimal split point (e.g., age â‰¤ 30)\n",
        "- **Categorical features:**\n",
        "- For binary: Direct split\n",
        "- For multi-category: Finds optimal subset (e.g., color in {red, blue})\n",
        "- **Ordinal features:** Can respect ordering if specified\n",
        "**Q3: What are the advantages of Decision Trees?**\n",
        "**Answer:**\n",
        "- **Interpretable:** Easy to understand and explain\n",
        "- **No data preprocessing:** Handles missing values, mixed types\n",
        "- **Non-parametric:** No assumptions about data distribution\n",
        "- **Feature importance:** Natural feature selection\n",
        "- **Handles non-linear relationships**\n",
        "**Q4: What are the main limitations and how to address them?**\n",
        "**Answer:**\n",
        "- **Overfitting:** Use pruning, max depth, min samples per leaf\n",
        "- **Unstable:** Small data changes can cause different trees (use ensembles)\n",
        "- **Biased towards features with more levels:** Use feature selection\n",
        "- **Poor extrapolation:** Doesn't predict well outside training range\n",
        "**Q5: What is tree pruning and why is it important?**\n",
        "**Answer:**\n",
        "**Pruning** removes branches that have little power in predicting target values.\n",
        "- **Pre-pruning:** Stop growing tree early (max_depth, min_samples_leaf)\n",
        "- **Post-pruning:** Grow full tree then remove unnecessary branches\n",
        "- **Benefits:** Reduces overfitting, improves generalization, smaller trees\n",
        "# 4. RANDOM FOREST\n",
        "## Algorithm Background & Mathematical Foundation\n",
        "**Core Concept:** Random Forest is an ensemble method that builds multiple decision trees and combines their predictions using bagging and feature randomness.\n",
        "**Key Concepts:**\n",
        "**Bagging (Bootstrap Aggregating):**\n",
        "- Create multiple datasets by sampling with replacement\n",
        "- Train a decision tree on each bootstrap sample\n",
        "- Combine predictions by majority vote (classification) or averaging (regression)\n",
        "**Feature Randomness:**\n",
        "- At each split, consider only a random subset of features\n",
        "- Typically $\\sqrt{p}$ features for classification, $p/3$ for regression (where p = total features)\n",
        "**Mathematical Formulation:**\n",
        "**Final Prediction:**\n",
        "- **Classification:** $\\hat{y} = \\text{mode}\\{T_1(x), T_2(x), ..., T_B(x)\\}$\n",
        "- **Regression:** $\\hat{y} = \\frac{1}{B}\\sum_{b=1}^{B} T_b(x)$\n",
        "**Out-of-Bag (OOB) Error:**\n",
        "- Each tree is trained on ~63% of data, remaining 37% used for validation\n",
        "- OOB error provides unbiased estimate of generalization error"
      ],
      "id": "IeFG6W2M0ofE"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kU29IyKg0ofE"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Cell 6: Random Forest - Comprehensive Implementation\n",
        "print(\"ðŸš€ RANDOM FOREST: COMPREHENSIVE IMPLEMENTATION\\n\")\n",
        "# Load dataset\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "feature_names = wine.feature_names\n",
        "class_names = wine.target_names\n",
        "print(\"ðŸ“Š Dataset Overview:\")\n",
        "print(f\"â€¢ Dataset: Wine Recognition\")\n",
        "print(f\"â€¢ Samples: {X.shape[0]}, Features: {X.shape[1]}\")\n",
        "print(f\"â€¢ Classes: {list(class_names)}\")\n",
        "print(f\"â€¢ Class distribution: {np.bincount(y)}\")\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(X, columns=feature_names)\n",
        "df['wine_class'] = y\n",
        "# Data preprocessing\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
        "X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "# Train Random Forest models with different parameters\n",
        "rf_models = {\n",
        "'Random Forest (10 trees)': ensemble.RandomForestClassifier(n_estimators=10, random_state=42),\n",
        "'Random Forest (100 trees)': ensemble.RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "'Random Forest (500 trees)': ensemble.RandomForestClassifier(n_estimators=500, random_state=42),\n",
        "'Random Forest (max_features=sqrt)': ensemble.RandomForestClassifier(n_estimators=100, max_features='sqrt', random_state=42),\n",
        "'Random Forest (max_features=log2)': ensemble.RandomForestClassifier(n_estimators=100, max_features='log2', random_state=42)\n",
        "}\n",
        "rf_results = {}\n",
        "for name, model in rf_models.items():\n",
        "print(f\"\\nðŸ“Š Training {name}...\")\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_proba = model.predict_proba(X_test)\n",
        "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
        "precision = metrics.precision_score(y_test, y_pred, average='weighted')\n",
        "recall = metrics.recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = metrics.f1_score(y_test, y_pred, average='weighted')\n",
        "rf_results[name] = {\n",
        "'accuracy': accuracy,\n",
        "'precision': precision,\n",
        "'recall': recall,\n",
        "'f1': f1,\n",
        "'model': model,\n",
        "'y_pred': y_pred\n",
        "}\n",
        "print(f\" â€¢ Accuracy: {accuracy:.4f}\")\n",
        "print(f\" â€¢ Precision: {precision:.4f}\")\n",
        "print(f\" â€¢ Recall: {recall:.4f}\")\n",
        "print(f\" â€¢ F1-Score: {f1:.4f}\")\n",
        "print(f\" â€¢ OOB Score: {model.oob_score_ if hasattr(model, 'oob_score_') else 'N/A':.4f}\")\n",
        "# Model Comparison\n",
        "comparison_rf = pd.DataFrame({\n",
        "'Model': list(rf_results.keys()),\n",
        "'Accuracy': [rf_results[name]['accuracy'] for name in rf_results.keys()],\n",
        "'Precision': [rf_results[name]['precision'] for name in rf_results.keys()],\n",
        "'Recall': [rf_results[name]['recall'] for name in rf_results.keys()],\n",
        "'F1-Score': [rf_results[name]['f1'] for name in rf_results.keys()],\n",
        "'OOB_Score': [rf_results[name]['model'].oob_score_ if hasattr(rf_results[name]['model'], 'oob_score_') else np.nan for name in rf_results.keys()]\n",
        "})\n",
        "print(\"\\nðŸ“‹ Model Performance Comparison:\")\n",
        "print(comparison_rf.round(4))\n",
        "# Comprehensive Visualization\n",
        "best_rf_name = max(rf_results.keys(), key=lambda x: rf_results[x]['accuracy'])\n",
        "best_rf_model = rf_results[best_rf_name]['model']\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "# Feature Importance\n",
        "feature_importance = pd.DataFrame({\n",
        "'feature': feature_names,\n",
        "'importance': best_rf_model.feature_importances_\n",
        "}).sort_values('importance', ascending=True)\n",
        "axes[0,0].barh(feature_importance['feature'], feature_importance['importance'])\n",
        "axes[0,0].set_xlabel('Feature Importance')\n",
        "axes[0,0].set_title('Random Forest Feature Importance')\n",
        "axes[0,0].grid(True, axis='x')\n",
        "# Confusion Matrix\n",
        "y_pred_best = best_rf_model.predict(X_test)\n",
        "cm = metrics.confusion_matrix(y_test, y_pred_best)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0,1],\n",
        "xticklabels=class_names, yticklabels=class_names)\n",
        "axes[0,1].set_xlabel('Predicted')\n",
        "axes[0,1].set_ylabel('Actual')\n",
        "axes[0,1].set_title('Confusion Matrix')\n",
        "# Number of Trees vs Performance\n",
        "n_trees_range = [1, 5, 10, 25, 50, 100, 200]\n",
        "train_scores = []\n",
        "test_scores = []\n",
        "oob_scores = []\n",
        "for n_trees in n_trees_range:\n",
        "rf_temp = ensemble.RandomForestClassifier(n_estimators=n_trees, random_state=42, oob_score=True)\n",
        "rf_temp.fit(X_train, y_train)\n",
        "train_scores.append(rf_temp.score(X_train, y_train))\n",
        "test_scores.append(rf_temp.score(X_test, y_test))\n",
        "oob_scores.append(rf_temp.oob_score_)\n",
        "axes[1,0].plot(n_trees_range, train_scores, 'o-', label='Training Score', color='blue')\n",
        "axes[1,0].plot(n_trees_range, test_scores, 'o-', label='Test Score', color='red')\n",
        "axes[1,0].plot(n_trees_range, oob_scores, 'o-', label='OOB Score', color='green')\n",
        "axes[1,0].set_xlabel('Number of Trees')\n",
        "axes[1,0].set_ylabel('Accuracy')\n",
        "axes[1,0].set_title('Random Forest: Number of Trees vs Performance')\n",
        "axes[1,0].legend()\n",
        "axes[1,0].grid(True)\n",
        "# Model Comparison\n",
        "models_rf = [name for name in rf_results.keys() if '100 trees' in name or 'max_features' in name]\n",
        "accuracy_scores = [rf_results[name]['accuracy'] for name in models_rf]\n",
        "bars = axes[1,1].bar(models_rf, accuracy_scores, color=['blue', 'green', 'orange'])\n",
        "axes[1,1].set_ylabel('Accuracy Score')\n",
        "axes[1,1].set_title('Random Forest: Different Configurations')\n",
        "axes[1,1].tick_params(axis='x', rotation=45)\n",
        "for bar, score in zip(bars, accuracy_scores):\n",
        "axes[1,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "f'{score:.3f}', ha='center', va='bottom')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "# Individual Tree Analysis (for first few trees)\n",
        "print(\"\\nðŸŒ³ Individual Tree Analysis (First 3 Trees):\")\n",
        "for i in range(3):\n",
        "single_tree = best_rf_model.estimators_[i]\n",
        "single_tree_score = single_tree.score(X_test, y_test)\n",
        "print(f\"â€¢ Tree {i+1}: Accuracy = {single_tree_score:.4f}, Depth = {single_tree.get_depth()}\")\n",
        "# Compare with Single Decision Tree\n",
        "single_dt = tree.DecisionTreeClassifier(random_state=42)\n",
        "single_dt.fit(X_train, y_train)\n",
        "single_dt_score = single_dt.score(X_test, y_test)\n",
        "print(f\"\\nðŸ“Š Comparison with Single Decision Tree:\")\n",
        "print(f\"â€¢ Single Decision Tree Accuracy: {single_dt_score:.4f}\")\n",
        "print(f\"â€¢ Random Forest Accuracy: {rf_results[best_rf_name]['accuracy']:.4f}\")\n",
        "print(f\"â€¢ Improvement: {rf_results[best_rf_name]['accuracy'] - single_dt_score:.4f}\")\n",
        "print(\"âœ… Random Forest Analysis Complete!\")"
      ],
      "id": "kU29IyKg0ofE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRUrB_xw0ofF"
      },
      "source": [
        "## Random Forest Interview Questions & Answers\n",
        "**Q1: How does Random Forest reduce overfitting compared to a single Decision Tree?**\n",
        "**Answer:**\n",
        "- **Bagging:** Each tree trained on different bootstrap sample\n",
        "- **Feature Randomness:** Each split considers random feature subset\n",
        "- **Averaging:** Combines predictions from multiple trees\n",
        "- **Result:** Reduces variance while maintaining low bias\n",
        "**Q2: What is the Out-of-Bag (OOB) error and why is it useful?**\n",
        "**Answer:**\n",
        "- **OOB Error:** Error calculated on samples not in bootstrap sample (~37% of data)\n",
        "- **Benefits:**\n",
        "- Provides unbiased estimate of generalization error\n",
        "- No need for separate validation set\n",
        "- Can be used for hyperparameter tuning\n",
        "3: How do you choose the number of trees and features per split?\n",
        "**Answer:**\n",
        "- **Number of trees:** More trees generally better, but diminishing returns (100-500 typical)\n",
        "- **Features per split:**\n",
        "- Classification: $\\sqrt{p}$ (default)\n",
        "- Regression: $p/3$\n",
        "- Can tune as hyperparameter\n",
        "- **Rule:** Increase trees until OOB error stabilizes\n",
        "**Q4: What are the advantages of Random Forest over other algorithms?**\n",
        "**Answer:**\n",
        "- **High accuracy:** Often state-of-the-art for tabular data\n",
        "- **Robust:** Handles outliers, missing values, irrelevant features\n",
        "- **Feature importance:** Natural feature selection\n",
        "- **Parallelizable:** Trees can be built independently\n",
        "- **No overfitting:** More trees cannot overfit (but can memorize noise)\n",
        "**Q5: When should you NOT use Random Forest?**\n",
        "**Answer:**\n",
        "- **Extrapolation:** Poor at predicting outside training range\n",
        "- **Sparse data:** May not perform well with very high-dimensional sparse data\n",
        "- **Interpretability:** Less interpretable than single tree\n",
        "- **Large datasets:** Memory intensive for very large datasets\n",
        "- **Real-time applications:** Slower prediction than single tree\n",
        "# 5. GRADIENT BOOSTING\n",
        "## Algorithm Background & Mathematical Foundation\n",
        "**Core Concept:** Gradient Boosting builds an ensemble of weak learners (typically decision trees) sequentially, where each new model corrects the errors made by previous models.\n",
        "**Mathematical Formulation:**\n",
        "**General Boosting Algorithm:**\n",
        "1. Initialize model with constant value: $F_0(x) = \\arg\\min_\\gamma \\sum_{i=1}^n L(y_i, \\gamma)$\n",
        "2. For m = 1 to M:\n",
        "a. Compute pseudo-residuals: $r_{im} = -\\left[\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}\\right]_{F(x)=F_{m-1}(x)}$\n",
        "b. Fit weak learner to pseudo-residuals: $h_m(x)$\n",
        "c. Compute multiplier: $\\gamma_m = \\arg\\min_\\gamma \\sum_{i=1}^n L(y_i, F_{m-1}(x_i) + \\gamma h_m(x_i))$\n",
        "d. Update model: $F_m(x) = F_{m-1}(x) + \\nu \\gamma_m h_m(x)$\n",
        "**For Regression (MSE Loss):**\n",
        "- Loss: $L(y_i, F(x_i)) = \\frac{1}{2}(y_i - F(x_i))^2$\n",
        "- Pseudo-residuals: $r_{im} = y_i - F_{m-1}(x_i)$\n",
        "- Each tree fits the residuals from previous model\n",
        "**For Classification (Log Loss):**\n",
        "- More complex due to probability transformations\n",
        "- Uses log-odds and sigmoid transformations\n",
        "**Key Parameters:**\n",
        "- **Learning rate (Î½):** Shrinks contribution of each tree\n",
        "- **Number of estimators (M):** Number of boosting stages\n",
        "- **Max depth:** Complexity of weak learners\n",
        "- **Subsample:** Fraction of samples used for each tree"
      ],
      "id": "NRUrB_xw0ofF"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCScyNpR0ofF"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Cell 7: Gradient Boosting - Comprehensive Implementation\n",
        "print(\"ðŸš€ GRADIENT BOOSTING: COMPREHENSIVE IMPLEMENTATION\\n\")\n",
        "# Load dataset\n",
        "from sklearn.datasets import make_classification\n",
        "# Create a more complex dataset for demonstration\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15,\n",
        "n_redundant=5, n_clusters_per_class=1, random_state=42)\n",
        "print(\"ðŸ“Š Dataset Overview:\")\n",
        "print(f\"â€¢ Samples: {X.shape[0]}, Features: {X.shape[1]}\")\n",
        "print(f\"â€¢ Class distribution: {np.bincount(y)}\")\n",
        "# Data preprocessing\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
        "X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "# Train Gradient Boosting models with different parameters\n",
        "gb_models = {\n",
        "'Gradient Boosting (default)': ensemble.GradientBoostingClassifier(random_state=42),\n",
        "'Gradient Boosting (high learning rate)': ensemble.GradientBoostingClassifier(learning_rate=0.1, random_state=42),\n",
        "'Gradient Boosting (low learning rate)': ensemble.GradientBoostingClassifier(learning_rate=0.01, n_estimators=500, random_state=42),\n",
        "'Gradient Boosting (shallow trees)': ensemble.GradientBoostingClassifier(max_depth=2, random_state=42),\n",
        "'Gradient Boosting (subsample)': ensemble.GradientBoostingClassifier(subsample=0.8, random_state=42)\n",
        "}\n",
        "gb_results = {}\n",
        "for name, model in gb_models.items():\n",
        "print(f\"\\nðŸ“Š Training {name}...\")\n",
        "model.fit(X_train_scaled, y_train)\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
        "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
        "precision = metrics.precision_score(y_test, y_pred)\n",
        "recall = metrics.recall_score(y_test, y_pred)\n",
        "f1 = metrics.f1_score(y_test, y_pred)\n",
        "roc_auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
        "gb_results[name] = {\n",
        "'accuracy': accuracy,\n",
        "'precision': precision,\n",
        "'recall': recall,\n",
        "'f1': f1,\n",
        "'roc_auc': roc_auc,\n",
        "'model': model,\n",
        "'y_pred_proba': y_pred_proba\n",
        "}\n",
        "print(f\" â€¢ Accuracy: {accuracy:.4f}\")\n",
        "print(f\" â€¢ Precision: {precision:.4f}\")\n",
        "print(f\" â€¢ Recall: {recall:.4f}\")\n",
        "print(f\" â€¢ F1-Score: {f1:.4f}\")\n",
        "print(f\" â€¢ ROC-AUC: {roc_auc:.4f}\")\n",
        "# Model Comparison\n",
        "comparison_gb = pd.DataFrame({\n",
        "'Model': list(gb_results.keys()),\n",
        "'Accuracy': [gb_results[name]['accuracy'] for name in gb_results.keys()],\n",
        "'Precision': [gb_results[name]['precision'] for name in gb_results.keys()],\n",
        "'Recall': [gb_results[name]['recall'] for name in gb_results.keys()],\n",
        "'F1-Score': [gb_results[name]['f1'] for name in gb_results.keys()],\n",
        "'ROC-AUC': [gb_results[name]['roc_auc'] for name in gb_results.keys()]\n",
        "})\n",
        "print(\"\\nðŸ“‹ Model Performance Comparison:\")\n",
        "print(comparison_gb.round(4))\n",
        "# XGBoost Implementation\n",
        "print(\"\\nðŸ”¥ XGBOOST IMPLEMENTATION\")\n",
        "xgb_models = {\n",
        "'XGBoost (default)': xgb.XGBClassifier(random_state=42),\n",
        "'XGBoost (high learning rate)': xgb.XGBClassifier(learning_rate=0.1, random_state=42),\n",
        "'XGBoost (with regularization)': xgb.XGBClassifier(learning_rate=0.1, reg_alpha=1.0, reg_lambda=1.0, random_state=42)\n",
        "}\n",
        "xgb_results = {}\n",
        "for name, model in xgb_models.items():\n",
        "print(f\"\\nðŸ“Š Training {name}...\")\n",
        "model.fit(X_train_scaled, y_train)\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
        "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
        "precision = metrics.precision_score(y_test, y_pred)\n",
        "recall = metrics.recall_score(y_test, y_pred)\n",
        "f1 = metrics.f1_score(y_test, y_pred)\n",
        "roc_auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
        "xgb_results[name] = {\n",
        "'accuracy': accuracy,\n",
        "'precision': precision,\n",
        "'recall': recall,\n",
        "'f1': f1,\n",
        "'roc_auc': roc_auc,\n",
        "'model': model\n",
        "}\n",
        "print(f\" â€¢ Accuracy: {accuracy:.4f}\")\n",
        "print(f\" â€¢ Precision: {precision:.4f}\")\n",
        "print(f\" â€¢ Recall: {recall:.4f}\")\n",
        "print(f\" â€¢ F1-Score: {f1:.4f}\")\n",
        "print(f\" â€¢ ROC-AUC: {roc_auc:.4f}\")\n",
        "# Comprehensive Visualization\n",
        "best_gb_name = max(gb_results.keys(), key=lambda x: gb_results[x]['roc_auc'])\n",
        "best_gb_model = gb_results[best_gb_name]['model']\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "# 1. Feature Importance (Gradient Boosting)\n",
        "feature_importance_gb = pd.DataFrame({\n",
        "'feature': [f'Feature_{i}' for i in range(X.shape[1])],\n",
        "'importance': best_gb_model.feature_importances_\n",
        "}).sort_values('importance', ascending=True).tail(10)\n",
        "axes[0,0].barh(feature_importance_gb['feature'], feature_importance_gb['importance'])\n",
        "axes[0,0].set_xlabel('Feature Importance')\n",
        "axes[0,0].set_title('Gradient Boosting - Top 10 Features')\n",
        "# 2. Feature Importance (XGBoost)\n",
        "best_xgb_name = max(xgb_results.keys(), key=lambda x: xgb_results[x]['roc_auc'])\n",
        "best_xgb_model = xgb_results[best_xgb_name]['model']\n",
        "feature_importance_xgb = pd.DataFrame({\n",
        "'feature': [f'Feature_{i}' for i in range(X.shape[1])],\n",
        "'importance': best_xgb_model.feature_importances_\n",
        "}).sort_values('importance', ascending=True).tail(10)\n",
        "axes[0,1].barh(feature_importance_xgb['feature'], feature_importance_xgb['importance'])\n",
        "axes[0,1].set_xlabel('Feature Importance')\n",
        "axes[0,1].set_title('XGBoost - Top 10 Features')\n",
        "# 3. Learning Curve - Training Deviance\n",
        "train_score = best_gb_model.train_score_\n",
        "test_score = best_gb_model.loss_(y_test, best_gb_model.decision_function(X_test_scaled))\n",
        "axes[0,2].plot(range(1, len(train_score) + 1), train_score, 'b-', label='Training Deviance')\n",
        "axes[0,2].set_xlabel('Boosting Iterations')\n",
        "axes[0,2].set_ylabel('Deviance')\n",
        "axes[0,2].set_title('Training Deviance')\n",
        "axes[0,2].legend()\n",
        "axes[0,2].grid(True)\n",
        "# 4. ROC Curve Comparison\n",
        "# Gradient Boosting\n",
        "fpr_gb, tpr_gb, _ = metrics.roc_curve(y_test, gb_results[best_gb_name]['y_pred_proba'])\n",
        "roc_auc_gb = metrics.auc(fpr_gb, tpr_gb)\n",
        "# XGBoost\n",
        "y_pred_proba_xgb = best_xgb_model.predict_proba(X_test_scaled)[:, 1]\n",
        "fpr_xgb, tpr_xgb, _ = metrics.roc_curve(y_test, y_pred_proba_xgb)\n",
        "roc_auc_xgb = metrics.auc(fpr_xgb, tpr_xgb)\n",
        "axes[1,0].plot(fpr_gb, tpr_gb, color='blue', lw=2, label=f'Gradient Boosting (AUC = {roc_auc_gb:.2f})')\n",
        "axes[1,0].plot(fpr_xgb, tpr_xgb, color='red', lw=2, label=f'XGBoost (AUC = {roc_auc_xgb:.2f})')\n",
        "axes[1,0].plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--')\n",
        "axes[1,0].set_xlabel('False Positive Rate')\n",
        "axes[1,0].set_ylabel('True Positive Rate')\n",
        "axes[1,0].set_title('ROC Curve Comparison')\n",
        "axes[1,0].legend()\n",
        "axes[1,0].grid(True)\n",
        "# 5. Learning Rate Comparison\n",
        "learning_rates = [0.01, 0.05, 0.1, 0.2]\n",
        "lr_scores = []\n",
        "for lr in learning_rates:\n",
        "gb_lr = ensemble.GradientBoostingClassifier(learning_rate=lr, n_estimators=100, random_state=42)\n",
        "gb_lr.fit(X_train_scaled, y_train)\n",
        "lr_scores.append(gb_lr.score(X_test_scaled, y_test))\n",
        "axes[1,1].plot(learning_rates, lr_scores, 'o-', color='green')\n",
        "axes[1,1].set_xlabel('Learning Rate')\n",
        "axes[1,1].set_ylabel('Accuracy')\n",
        "axes[1,1].set_title('Learning Rate vs Performance')\n",
        "axes[1,1].grid(True)\n",
        "# 6. Model Comparison\n",
        "all_models = {**gb_results, **xgb_results}\n",
        "model_names = list(all_models.keys())\n",
        "accuracy_scores = [all_models[name]['accuracy'] for name in model_names]\n",
        "bars = axes[1,2].bar(range(len(model_names)), accuracy_scores, color=['blue']*5 + ['red']*3)\n",
        "axes[1,2].set_ylabel('Accuracy')\n",
        "axes[1,2].set_title('All Models Comparison')\n",
        "axes[1,2].set_xticks(range(len(model_names)))\n",
        "axes[1,2].set_xticklabels(model_names, rotation=45, ha='right')\n",
        "for bar, score in zip(bars, accuracy_scores):\n",
        "axes[1,2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "f'{score:.3f}', ha='center', va='bottom', fontsize=8)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "# Stage-wise Analysis\n",
        "print(\"\\nðŸ“ˆ Stage-wise Performance Analysis:\")\n",
        "gb_staged = ensemble.GradientBoostingClassifier(n_estimators=200, random_state=42)\n",
        "gb_staged.fit(X_train_scaled, y_train)\n",
        "# Get staged predictions\n",
        "staged_accuracy = []\n",
        "for i, y_pred in enumerate(gb_staged.staged_predict(X_test_scaled)):\n",
        "staged_accuracy.append(metrics.accuracy_score(y_test, y_pred))\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, len(staged_accuracy) + 1), staged_accuracy, 'b-')\n",
        "plt.xlabel('Number of Boosting Stages')\n",
        "plt.ylabel('Test Accuracy')\n",
        "plt.title('Gradient Boosting: Performance vs Number of Stages')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "print(\"âœ… Gradient Boosting Analysis Complete!\")"
      ],
      "id": "nCScyNpR0ofF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zpss-UxY0ofF"
      },
      "source": [
        "## Gradient Boosting Interview Questions & Answers\n",
        "**Q1: What is the fundamental difference between Random Forest and Gradient Boosting?**\n",
        "**Answer:**\n",
        "- **Random Forest:** Parallel ensemble (bagging) - trees built independently\n",
        "- **Gradient Boosting:** Sequential ensemble (boosting) - trees built sequentially to correct errors\n",
        "- **RF:** Reduces variance, maintains bias\n",
        "- **GB:** Reduces bias, can increase variance\n",
        "- **RF:** Trees can be deep and overfit individually\n",
        "- **GB:** Trees are typically shallow (weak learners)\n",
        "**Q2: Explain the role of the learning rate in Gradient Boosting.**\n",
        "**Answer:**\n",
        "- **Learning rate (Î½):** Shrinks the contribution of each tree\n",
        "- **Low learning rate (0.01-0.1):**\n",
        "- Requires more trees\n",
        "- More robust, less prone to overfitting\n",
        "- Better generalization\n",
        "- **High learning rate (0.1-0.3):**\n",
        "- Fewer trees needed\n",
        "- Faster training\n",
        "- Higher risk of overfitting\n",
        "- **Typical strategy:** Use small learning rate with many trees\n",
        "**Q3: What is XGBoost and how does it improve upon basic Gradient Boosting?**\n",
        "**Answer:**\n",
        "**XGBoost (Extreme Gradient Boosting) enhancements:**\n",
        "- **Regularization:** L1 (Lasso) and L2 (Ridge) regularization\n",
        "- **Handling missing values:** Automatically learns direction for missing values\n",
        "- **Tree pruning:** More efficient pruning strategy\n",
        "- **Parallel processing:** Faster training\n",
        "- **Cross-validation:** Built-in cross-validation\n",
        "- **Early stopping:** Stop training when no improvement\n",
        "**Q4: How does Gradient Boosting handle overfitting?**\n",
        "**Answer:**\n",
        "1. **Learning rate shrinkage:** Reduces each tree's influence\n",
        "2. **Subsampling:** Use random subsets of data for each tree\n",
        "3. **Tree constraints:** Limit tree depth, min samples per leaf\n",
        "4. **Early stopping:** Stop when validation performance stops improving\n",
        "5. **Regularization** (XGBoost): L1/L2 regularization on weights\n",
        "**Q5: What are the main hyperparameters to tune in Gradient Boosting?**\n",
        "**Answer:**\n",
        "- **n_estimators:** Number of boosting stages\n",
        "- **learning_rate:** Shrinks contribution of each tree\n",
        "- **max_depth:** Maximum depth of individual trees\n",
        "- **min_samples_split:** Minimum samples required to split\n",
        "- **subsample:** Fraction of samples used for fitting\n",
        "- **max_features:** Number of features to consider for splits\n",
        "# 6. SUPPORT VECTOR MACHINES (SVM)\n",
        "## Algorithm Background & Mathematical Foundation\n",
        "**Core Concept:** SVM finds the optimal hyperplane that maximizes the margin between classes in feature space.\n",
        "**Mathematical Formulation:**\n",
        "**Linear SVM:**\n",
        "- **Decision function:** $f(x) = w^T x + b$\n",
        "- **Prediction:** $\\text{sign}(f(x))$\n",
        "- **Margin:** $\\frac{2}{\\|w\\|}$\n",
        "**Optimization Problem (Hard Margin):**\n",
        "$$\\min_{w,b} \\frac{1}{2}\\|w\\|^2$$\n",
        "$$\\text{subject to } y_i(w^T x_i + b) \\geq 1 \\quad \\forall i$$\n",
        "**Optimization Problem (Soft Margin):**\n",
        "$$\\min_{w,b,\\xi} \\frac{1}{2}\\|w\\|^2 + C\\sum_{i=1}^n \\xi_i$$\n",
        "$$\\text{subject to } y_i(w^T x_i + b) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0 \\quad \\forall i$$\n",
        "**Kernel Trick:**\n",
        "- Maps data to higher-dimensional space: $\\phi(x)$\n",
        "- **Kernel function:** $K(x_i, x_j) = \\phi(x_i)^T \\phi(x_j)$\n",
        "- **Common kernels:**\n",
        "- Linear: $K(x_i, x_j) = x_i^T x_j$\n",
        "- Polynomial: $K(x_i, x_j) = (\\gamma x_i^T x_j + r)^d$\n",
        "- RBF: $K(x_i, x_j) = \\exp(-\\gamma \\|x_i - x_j\\|^2)$"
      ],
      "id": "Zpss-UxY0ofF"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RN0FZrBA0ofG"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Cell 8: Support Vector Machines - Comprehensive Implementation\n",
        "print(\"ðŸš€ SUPPORT VECTOR MACHINES: COMPREHENSIVE IMPLEMENTATION\\n\")\n",
        "# Create a non-linearly separable dataset for demonstration\n",
        "from sklearn.datasets import make_circles\n",
        "X, y = make_circles(n_samples=300, noise=0.1, factor=0.3, random_state=42)\n",
        "print(\"ðŸ“Š Dataset Overview:\")\n",
        "print(f\"â€¢ Samples: {X.shape[0]}, Features: {X.shape[1]}\")\n",
        "print(f\"â€¢ Class distribution: {np.bincount(y)}\")\n",
        "# Visualize the original data\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X[y == 0, 0], X[y == 0, 1], color='red', alpha=0.6, label='Class 0')\n",
        "plt.scatter(X[y == 1, 0], X[y == 1, 1], color='blue', alpha=0.6, label='Class 1')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('Original Data - Non-linearly Separable')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "# Data preprocessing\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
        "X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "# Train SVM models with different kernels\n",
        "svm_models = {\n",
        "'SVM (Linear Kernel)': svm.SVC(kernel='linear', random_state=42),\n",
        "'SVM (RBF Kernel)': svm.SVC(kernel='rbf', random_state=42),\n",
        "'SVM (Polynomial Kernel)': svm.SVC(kernel='poly', degree=3, random_state=42),\n",
        "'SVM (Sigmoid Kernel)': svm.SVC(kernel='sigmoid', random_state=42),\n",
        "'LinearSVC': svm.LinearSVC(random_state=42)\n",
        "}\n",
        "svm_results = {}\n",
        "for name, model in svm_models.items():\n",
        "print(f\"\\nðŸ“Š Training {name}...\")\n",
        "model.fit(X_train_scaled, y_train)\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
        "precision = metrics.precision_score(y_test, y_pred)\n",
        "recall = metrics.recall_score(y_test, y_pred)\n",
        "f1 = metrics.f1_score(y_test, y_pred)\n",
        "# For kernels that support decision function\n",
        "if hasattr(model, 'decision_function'):\n",
        "y_decision = model.decision_function(X_test_scaled)\n",
        "roc_auc = metrics.roc_auc_score(y_test, y_decision)\n",
        "else:\n",
        "roc_auc = metrics.roc_auc_score(y_test, y_pred)\n",
        "svm_results[name] = {\n",
        "'accuracy': accuracy,\n",
        "'precision': precision,\n",
        "'recall': recall,\n",
        "'f1': f1,\n",
        "'roc_auc': roc_auc,\n",
        "'model': model,\n",
        "'y_pred': y_pred\n",
        "}\n",
        "print(f\" â€¢ Accuracy: {accuracy:.4f}\")\n",
        "print(f\" â€¢ Precision: {precision:.4f}\")\n",
        "print(f\" â€¢ Recall: {recall:.4f}\")\n",
        "print(f\" â€¢ F1-Score: {f1:.4f}\")\n",
        "print(f\" â€¢ ROC-AUC: {roc_auc:.4f}\")\n",
        "# Model Comparison\n",
        "comparison_svm = pd.DataFrame({\n",
        "'Model': list(svm_results.keys()),\n",
        "'Accuracy': [svm_results[name]['accuracy'] for name in svm_results.keys()],\n",
        "'Precision': [svm_results[name]['precision'] for name in svm_results.keys()],\n",
        "'Recall': [svm_results[name]['recall'] for name in svm_results.keys()],\n",
        "'F1-Score': [svm_results[name]['f1'] for name in svm_results.keys()],\n",
        "'ROC-AUC': [svm_results[name]['roc_auc'] for name in svm_results.keys()]\n",
        "})\n",
        "print(\"\\nðŸ“‹ Model Performance Comparison:\")\n",
        "print(comparison_svm.round(4))\n",
        "# Comprehensive Visualization\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "# Create mesh grid for decision boundaries\n",
        "x_min, x_max = X_train_scaled[:, 0].min() - 0.5, X_train_scaled[:, 0].max() + 0.5\n",
        "y_min, y_max = X_train_scaled[:, 1].min() - 0.5, X_train_scaled[:, 1].max() + 0.5\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
        "np.arange(y_min, y_max, 0.02))\n",
        "# Plot decision boundaries for different kernels\n",
        "kernels_to_plot = ['Linear Kernel', 'RBF Kernel', 'Polynomial Kernel']\n",
        "for i, kernel_name in enumerate(kernels_to_plot):\n",
        "model_key = f'SVM ({kernel_name})'\n",
        "if model_key in svm_results:\n",
        "model = svm_results[model_key]['model']\n",
        "# Predict on mesh grid\n",
        "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "# Plot decision boundary\n",
        "axes[0,i].contourf(xx, yy, Z, alpha=0.8, cmap='RdYlBu')\n",
        "axes[0,i].scatter(X_train_scaled[:, 0], X_train_scaled[:, 1], c=y_train,\n",
        "edgecolor='black', s=50, cmap='RdYlBu')\n",
        "axes[0,i].set_xlabel('Feature 1 (scaled)')\n",
        "axes[0,i].set_ylabel('Feature 2 (scaled)')\n",
        "axes[0,i].set_title(f'SVM with {kernel_name}')\n",
        "axes[0,i].grid(True)\n",
        "# Support Vectors Visualization\n",
        "best_svm_name = max(svm_results.keys(), key=lambda x: svm_results[x]['accuracy'])\n",
        "best_svm_model = svm_results[best_svm_name]['model']\n",
        "if hasattr(best_svm_model, 'support_vectors_'):\n",
        "axes[1,0].scatter(X_train_scaled[:, 0], X_train_scaled[:, 1], c=y_train,\n",
        "alpha=0.3, cmap='RdYlBu')\n",
        "axes[1,0].scatter(best_svm_model.support_vectors_[:, 0],\n",
        "best_svm_model.support_vectors_[:, 1],\n",
        "s=100, facecolors='none', edgecolors='black',\n",
        "linewidths=1, label='Support Vectors')\n",
        "axes[1,0].set_xlabel('Feature 1 (scaled)')\n",
        "axes[1,0].set_ylabel('Feature 2 (scaled)')\n",
        "axes[1,0].set_title('Support Vectors')\n",
        "axes[1,0].legend()\n",
        "axes[1,0].grid(True)\n",
        "# C Parameter Analysis\n",
        "C_values = [0.1, 1, 10, 100, 1000]\n",
        "C_scores = []\n",
        "for C_val in C_values:\n",
        "svm_temp = svm.SVC(kernel='rbf', C=C_val, random_state=42)\n",
        "svm_temp.fit(X_train_scaled, y_train)\n",
        "C_scores.append(svm_temp.score(X_test_scaled, y_test))\n",
        "axes[1,1].semilogx(C_values, C_scores, 'o-', color='green')\n",
        "axes[1,1].set_xlabel('C (Regularization Parameter)')\n",
        "axes[1,1].set_ylabel('Accuracy')\n",
        "axes[1,1].set_title('SVM: C Parameter vs Performance')\n",
        "axes[1,1].grid(True)\n",
        "# Gamma Parameter Analysis (for RBF kernel)\n",
        "gamma_values = [0.001, 0.01, 0.1, 1, 10, 100]\n",
        "gamma_scores = []\n",
        "for gamma_val in gamma_values:\n",
        "svm_temp = svm.SVC(kernel='rbf', gamma=gamma_val, random_state=42)\n",
        "svm_temp.fit(X_train_scaled, y_train)\n",
        "gamma_scores.append(svm_temp.score(X_test_scaled, y_test))\n",
        "axes[1,2].semilogx(gamma_values, gamma_scores, 'o-', color='purple')\n",
        "axes[1,2].set_xlabel('Gamma (Kernel Coefficient)')\n",
        "axes[1,2].set_ylabel('Accuracy')\n",
        "axes[1,2].set_title('SVM: Gamma Parameter vs Performance (RBF Kernel)')\n",
        "axes[1,2].grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "# Kernel Comparison on Different Datasets\n",
        "print(\"\\nðŸ” Kernel Performance on Different Data Patterns:\")\n",
        "# Create different datasets\n",
        "datasets_info = {\n",
        "'Linear': datasets.make_classification(n_samples=100, n_features=2, n_redundant=0,\n",
        "n_informative=2, n_clusters_per_class=1, random_state=42),\n",
        "'Moons': datasets.make_moons(n_samples=100, noise=0.1, random_state=42),\n",
        "'Circles': make_circles(n_samples=100, noise=0.1, factor=0.3, random_state=42)\n",
        "}\n",
        "kernel_performance = {}\n",
        "for data_name, (X_data, y_data) in datasets_info.items():\n",
        "print(f\"\\nAnalyzing {data_name} dataset...\")\n",
        "X_train_d, X_test_d, y_train_d, y_test_d = model_selection.train_test_split(\n",
        "X_data, y_data, test_size=0.2, random_state=42\n",
        ")\n",
        "# Scale features\n",
        "scaler_d = StandardScaler()\n",
        "X_train_d_scaled = scaler_d.fit_transform(X_train_d)\n",
        "X_test_d_scaled = scaler_d.transform(X_test_d)\n",
        "kernels = ['linear', 'rbf', 'poly']\n",
        "for kernel in kernels:\n",
        "svm_temp = svm.SVC(kernel=kernel, random_state=42)\n",
        "svm_temp.fit(X_train_d_scaled, y_train_d)\n",
        "score = svm_temp.score(X_test_d_scaled, y_test_d)\n",
        "kernel_performance[(data_name, kernel)] = score\n",
        "print(f\" â€¢ {kernel} kernel: {score:.4f}\")\n",
        "# SVM for Regression (SVR)\n",
        "print(\"\\nðŸ“ˆ Support Vector Regression (SVR) Example:\")\n",
        "# Create regression dataset\n",
        "X_reg, y_reg = datasets.make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n",
        "# Train SVR models\n",
        "svr_models = {\n",
        "'SVR (Linear)': svm.SVR(kernel='linear'),\n",
        "'SVR (RBF)': svm.SVR(kernel='rbf'),\n",
        "'SVR (Poly)': svm.SVR(kernel='poly')\n",
        "}\n",
        "plt.figure(figsize=(15, 5))\n",
        "for i, (name, model) in enumerate(svr_models.items()):\n",
        "model.fit(X_reg, y_reg)\n",
        "y_pred_reg = model.predict(X_reg)\n",
        "plt.subplot(1, 3, i+1)\n",
        "plt.scatter(X_reg, y_reg, alpha=0.6, label='Data')\n",
        "# Sort for nice plotting\n",
        "X_sorted = np.sort(X_reg, axis=0)\n",
        "y_pred_sorted = model.predict(X_sorted)\n",
        "plt.plot(X_sorted, y_pred_sorted, 'r-', linewidth=2, label='SVR Prediction')\n",
        "plt.xlabel('Feature')\n",
        "plt.ylabel('Target')\n",
        "plt.title(name)\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "print(\"âœ… Support Vector Machines Analysis Complete!\")"
      ],
      "id": "RN0FZrBA0ofG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRj2MB4X0ofG"
      },
      "source": [
        "## Support Vector Machines Interview Questions & Answers\n",
        "**Q1: What is the kernel trick and why is it important?**\n",
        "**Answer:**\n",
        "- **Kernel trick:** Method to operate in high-dimensional feature space without computing coordinates\n",
        "- **Mathematical basis:** $K(x_i, x_j) = \\phi(x_i)^T \\phi(x_j)$\n",
        "- **Benefits:**\n",
        "- Computationally efficient\n",
        "- Enables non-linear decision boundaries\n",
        "- Works with infinite-dimensional features\n",
        "- **Common kernels:** Linear, Polynomial, RBF, Sigmoid\n",
        "**Q2: Explain the role of the C parameter in SVM.**\n",
        "**Answer:**\n",
        "- **C parameter:** Regularization parameter that controls trade-off between margin maximization and error minimization\n",
        "- **Small C:** Large margin, more misclassifications allowed (underfitting)\n",
        "- **Large C:** Small margin, fewer misclassifications (overfitting)\n",
        "- **Effect:**\n",
        "- C â†’ 0: Very soft margin, many support vectors\n",
        "- C â†’ âˆž: Hard margin, few support vectors\n",
        "**Q3: What are support vectors and why are they important?**\n",
        "**Answer:**\n",
        "- **Support vectors:** Data points that lie on the margin boundaries or are misclassified\n",
        "- **Importance:**\n",
        "- Determine the decision boundary\n",
        "- Only support vectors affect the model\n",
        "- Model is sparse - depends only on support vectors\n",
        "- Number of support vectors indicates model complexity\n",
        "**Q4: Compare Linear SVM and Logistic Regression.**\n",
        "**Answer:**\n",
        "- **Similarities:** Both find linear decision boundaries\n",
        "- **Differences:**\n",
        "- **SVM:** Maximizes margin, focuses on boundary points\n",
        "- **LR:** Maximizes likelihood, uses all data points\n",
        "- **SVM:** Better with clear margin of separation\n",
        "- **LR:** Provides probability estimates\n",
        "- **SVM:** More robust to outliers\n",
        "- **LR:** Faster training for large datasets\n",
        "**Q5: When should you use SVM vs other classifiers?**\n",
        "**Answer:**\n",
        "**Use SVM when:**\n",
        "- Clear margin of separation exists\n",
        "- High-dimensional spaces\n",
        "- Non-linear relationships (with kernels)\n",
        "- Number of features > number of samples\n",
        "- Need robust model to outliers\n",
        "**Avoid SVM when:**\n",
        "- Very large datasets (slow training)\n",
        "- Noisy datasets with overlapping classes\n",
        "- Need probability estimates\n",
        "- Interpretability is crucial\n",
        "- Multi-class problems with many classes\n",
        "# 7. K-NEAREST NEIGHBORS (KNN)\n",
        "## Algorithm Background & Mathematical Foundation\n",
        "**Core Concept:** KNN is an instance-based learning algorithm that classifies data points based on the majority class among their k-nearest neighbors.\n",
        "**Mathematical Formulation:**\n",
        "**Distance Metrics:**\n",
        "- **Euclidean:** $d(x,y) = \\sqrt{\\sum_{i=1}^n (x_i - y_i)^2}$\n",
        "- **Manhattan:** $d(x,y) = \\sum_{i=1}^n |x_i - y_i|$\n",
        "- **Minkowski:** $d(x,y) = (\\sum_{i=1}^n |x_i - y_i|^p)^{1/p}$\n",
        "- **Cosine:** $d(x,y) = 1 - \\frac{x \\cdot y}{\\|x\\|\\|y\\|}$\n",
        "**Algorithm:**\n",
        "1. Choose the number k of neighbors\n",
        "2. Calculate distance between query instance and all training samples\n",
        "3. Sort distances and find k nearest neighbors\n",
        "4. Gather categories of k nearest neighbors\n",
        "5. Use majority vote or averaging for prediction\n",
        "**For Classification:**\n",
        "$$\\hat{y} = \\text{mode}(y_{i_1}, y_{i_2}, ..., y_{i_k})$$\n",
        "**For Regression:**\n",
        "$$\\hat{y} = \\frac{1}{k}\\sum_{j=1}^k y_{i_j}$$\n",
        "**Weighted KNN:**\n",
        "$$w_i = \\frac{1}{d(x, x_i)^2}$$\n",
        "$$\\hat{y} = \\frac{\\sum_{i=1}^k w_i y_i}{\\sum_{i=1}^k w_i}$$"
      ],
      "id": "eRj2MB4X0ofG"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SqGzmzk0ofG"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Cell 9: K-Nearest Neighbors - Comprehensive Implementation\n",
        "print(\"ðŸš€ K-NEAREST NEIGHBORS: COMPREHENSIVE IMPLEMENTATION\\n\")\n",
        "# Load dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data[:, :2] # Use only first two features for visualization\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names[:2]\n",
        "class_names = iris.target_names\n",
        "print(\"ðŸ“Š Dataset Overview:\")\n",
        "print(f\"â€¢ Dataset: Iris Flowers (first two features)\")\n",
        "print(f\"â€¢ Samples: {X.shape[0]}, Features: {X.shape[1]}\")\n",
        "print(f\"â€¢ Classes: {list(class_names)}\")\n",
        "print(f\"â€¢ Class distribution: {np.bincount(y)}\")\n",
        "# Visualize the data\n",
        "plt.figure(figsize=(10, 8))\n",
        "scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', s=50, alpha=0.7)\n",
        "plt.xlabel(feature_names[0])\n",
        "plt.ylabel(feature_names[1])\n",
        "plt.title('Iris Dataset - First Two Features')\n",
        "plt.colorbar(scatter, label='Class')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "# Data preprocessing\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
        "X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "# Scale features (important for KNN!)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "# Train KNN models with different parameters\n",
        "knn_models = {\n",
        "'KNN (k=1)': neighbors.KNeighborsClassifier(n_neighbors=1),\n",
        "'KNN (k=3)': neighbors.KNeighborsClassifier(n_neighbors=3),\n",
        "'KNN (k=5)': neighbors.KNeighborsClassifier(n_neighbors=5),\n",
        "'KNN (k=10)': neighbors.KNeighborsClassifier(n_neighbors=10),\n",
        "'KNN (k=20)': neighbors.KNeighborsClassifier(n_neighbors=20),\n",
        "'KNN (k=50)': neighbors.KNeighborsClassifier(n_neighbors=50)\n",
        "}\n",
        "knn_results = {}\n",
        "for name, model in knn_models.items():\n",
        "print(f\"\\nðŸ“Š Training {name}...\")\n",
        "model.fit(X_train_scaled, y_train)\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
        "precision = metrics.precision_score(y_test, y_pred, average='weighted')\n",
        "recall = metrics.recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = metrics.f1_score(y_test, y_pred, average='weighted')\n",
        "knn_results[name] = {\n",
        "'accuracy': accuracy,\n",
        "'precision': precision,\n",
        "'recall': recall,\n",
        "'f1': f1,\n",
        "'model': model,\n",
        "'y_pred': y_pred\n",
        "}\n",
        "print(f\" â€¢ Accuracy: {accuracy:.4f}\")\n",
        "print(f\" â€¢ Precision: {precision:.4f}\")\n",
        "print(f\" â€¢ Recall: {recall:.4f}\")\n",
        "print(f\" â€¢ F1-Score: {f1:.4f}\")\n",
        "# Model Comparison\n",
        "comparison_knn = pd.DataFrame({\n",
        "'Model': list(knn_results.keys()),\n",
        "'Accuracy': [knn_results[name]['accuracy'] for name in knn_results.keys()],\n",
        "'Precision': [knn_results[name]['precision'] for name in knn_results.keys()],\n",
        "'Recall': [knn_results[name]['recall'] for name in knn_results.keys()],\n",
        "'F1-Score': [knn_results[name]['f1'] for name in knn_results.keys()]\n",
        "})\n",
        "print(\"\\nðŸ“‹ Model Performance Comparison:\")\n",
        "print(comparison_knn.round(4))\n",
        "# Comprehensive Visualization\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "# 1. K Value vs Performance\n",
        "k_values = [1, 3, 5, 10, 20, 50]\n",
        "train_scores = []\n",
        "test_scores = []\n",
        "for k in k_values:\n",
        "knn_temp = neighbors.KNeighborsClassifier(n_neighbors=k)\n",
        "knn_temp.fit(X_train_scaled, y_train)\n",
        "train_scores.append(knn_temp.score(X_train_scaled, y_train))\n",
        "test_scores.append(knn_temp.score(X_test_scaled, y_test))\n",
        "axes[0,0].plot(k_values, train_scores, 'o-', label='Training Score', color='blue')\n",
        "axes[0,0].plot(k_values, test_scores, 'o-', label='Test Score', color='red')\n",
        "axes[0,0].set_xlabel('k (Number of Neighbors)')\n",
        "axes[0,0].set_ylabel('Accuracy')\n",
        "axes[0,0].set_title('KNN: k Value vs Performance')\n",
        "axes[0,0].legend()\n",
        "axes[0,0].grid(True)\n",
        "# 2. Distance Metrics Comparison\n",
        "distance_metrics = ['euclidean', 'manhattan', 'minkowski']\n",
        "distance_scores = []\n",
        "for metric in distance_metrics:\n",
        "knn_temp = neighbors.KNeighborsClassifier(n_neighbors=5, metric=metric)\n",
        "knn_temp.fit(X_train_scaled, y_train)\n",
        "distance_scores.append(knn_temp.score(X_test_scaled, y_test))\n",
        "axes[0,1].bar(distance_metrics, distance_scores, color=['blue', 'green', 'orange'])\n",
        "axes[0,1].set_ylabel('Accuracy')\n",
        "axes[0,1].set_title('KNN: Distance Metrics Comparison')\n",
        "axes[0,1].set_ylim(0, 1)\n",
        "for i, score in enumerate(distance_scores):\n",
        "axes[0,1].text(i, score + 0.01, f'{score:.3f}', ha='center', va='bottom')\n",
        "# 3. Decision Boundaries for different k values\n",
        "k_values_boundaries = [1, 5, 15, 50]\n",
        "for i, k in enumerate(k_values_boundaries):\n",
        "knn_temp = neighbors.KNeighborsClassifier(n_neighbors=k)\n",
        "knn_temp.fit(X_train_scaled, y_train)\n",
        "# Create mesh grid\n",
        "x_min, x_max = X_train_scaled[:, 0].min() - 0.5, X_train_scaled[:, 0].max() + 0.5\n",
        "y_min, y_max = X_train_scaled[:, 1].min() - 0.5, X_train_scaled[:, 1].max() + 0.5\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
        "np.arange(y_min, y_max, 0.02))\n",
        "# Predict on mesh grid\n",
        "Z = knn_temp.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "# Plot decision boundary\n",
        "row = 1 if i < 2 else 2\n",
        "col = i % 2\n",
        "if row == 1:\n",
        "ax = axes[1, col]\n",
        "else:\n",
        "# Create additional subplot if needed\n",
        "if i == 2:\n",
        "fig.add_subplot(2, 3, 5)\n",
        "ax = plt.gca()\n",
        "else:\n",
        "fig.add_subplot(2, 3, 6)\n",
        "ax = plt.gca()\n",
        "ax.contourf(xx, yy, Z, alpha=0.8, cmap='viridis')\n",
        "ax.scatter(X_train_scaled[:, 0], X_train_scaled[:, 1], c=y_train,\n",
        "edgecolor='black', s=50, cmap='viridis')\n",
        "ax.set_xlabel('Feature 1 (scaled)')\n",
        "ax.set_ylabel('Feature 2 (scaled)')\n",
        "ax.set_title(f'KNN Decision Boundary (k={k})')\n",
        "ax.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "# Weighted KNN vs Standard KNN\n",
        "print(\"\\nâš–ï¸ Weighted KNN vs Standard KNN:\")\n",
        "k_values_compare = [3, 5, 10, 20]\n",
        "standard_scores = []\n",
        "weighted_scores = []\n",
        "for k in k_values_compare:\n",
        "# Standard KNN\n",
        "knn_standard = neighbors.KNeighborsClassifier(n_neighbors=k, weights='uniform')\n",
        "knn_standard.fit(X_train_scaled, y_train)\n",
        "standard_scores.append(knn_standard.score(X_test_scaled, y_test))\n",
        "# Weighted KNN\n",
        "knn_weighted = neighbors.KNeighborsClassifier(n_neighbors=k, weights='distance')\n",
        "knn_weighted.fit(X_train_scaled, y_train)\n",
        "weighted_scores.append(knn_weighted.score(X_test_scaled, y_test))\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(k_values_compare, standard_scores, 'o-', label='Standard KNN', color='blue')\n",
        "plt.plot(k_values_compare, weighted_scores, 'o-', label='Weighted KNN', color='red')\n",
        "plt.xlabel('k (Number of Neighbors)')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Standard vs Weighted KNN')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "# KNN Regression Example\n",
        "print(\"\\nðŸ“ˆ KNN Regression Example:\")\n",
        "# Create regression dataset\n",
        "X_reg, y_reg = datasets.make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = model_selection.train_test_split(\n",
        "X_reg, y_reg, test_size=0.2, random_state=42\n",
        ")\n",
        "# Scale features\n",
        "scaler_reg = StandardScaler()\n",
        "X_train_reg_scaled = scaler_reg.fit_transform(X_train_reg)\n",
        "X_test_reg_scaled = scaler_reg.transform(X_test_reg)\n",
        "# Train KNN Regressor\n",
        "knn_reg = neighbors.KNeighborsRegressor(n_neighbors=5)\n",
        "knn_reg.fit(X_train_reg_scaled, y_train_reg)\n",
        "y_pred_reg = knn_reg.predict(X_test_reg_scaled)\n",
        "mse = metrics.mean_squared_error(y_test_reg, y_pred_reg)\n",
        "r2 = metrics.r2_score(y_test_reg, y_pred_reg)\n",
        "print(f\"KNN Regression Results:\")\n",
        "print(f\"â€¢ MSE: {mse:.4f}\")\n",
        "print(f\"â€¢ RÂ²: {r2:.4f}\")\n",
        "# Plot regression results\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X_train_reg, y_train_reg, color='blue', alpha=0.6, label='Training Data')\n",
        "plt.scatter(X_test_reg, y_test_reg, color='red', alpha=0.6, label='Test Data')\n",
        "plt.xlabel('Feature')\n",
        "plt.ylabel('Target')\n",
        "plt.title('KNN Regression - Data')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.subplot(1, 2, 2)\n",
        "# Sort for nice plotting\n",
        "sort_idx = np.argsort(X_test_reg.ravel())\n",
        "X_sorted = X_test_reg[sort_idx]\n",
        "y_pred_sorted = y_pred_reg[sort_idx]\n",
        "plt.scatter(X_test_reg, y_test_reg, color='red', alpha=0.6, label='Actual')\n",
        "plt.plot(X_sorted, y_pred_sorted, 'black', linewidth=2, label='KNN Prediction')\n",
        "plt.xlabel('Feature')\n",
        "plt.ylabel('Target')\n",
        "plt.title('KNN Regression - Predictions')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "# Curse of Dimensionality Demonstration\n",
        "print(\"\\nâš ï¸ Curse of Dimensionality in KNN:\")\n",
        "# Generate data with increasing dimensions\n",
        "dimensions = range(1, 101, 10)\n",
        "dimension_scores = []\n",
        "for dim in dimensions:\n",
        "# Generate high-dimensional data\n",
        "X_high_dim, y_high_dim = datasets.make_classification(\n",
        "n_samples=1000, n_features=dim, n_informative=dim//2,\n",
        "n_redundant=dim//2, random_state=42\n",
        ")\n",
        "X_train_hd, X_test_hd, y_train_hd, y_test_hd = model_selection.train_test_split(\n",
        "X_high_dim, y_high_dim, test_size=0.2, random_state=42\n",
        ")\n",
        "# Scale features\n",
        "scaler_hd = StandardScaler()\n",
        "X_train_hd_scaled = scaler_hd.fit_transform(X_train_hd)\n",
        "X_test_hd_scaled = scaler_hd.transform(X_test_hd)\n",
        "# Train KNN\n",
        "knn_hd = neighbors.KNeighborsClassifier(n_neighbors=5)\n",
        "knn_hd.fit(X_train_hd_scaled, y_train_hd)\n",
        "score = knn_hd.score(X_test_hd_scaled, y_test_hd)\n",
        "dimension_scores.append(score)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(dimensions, dimension_scores, 'o-', color='purple')\n",
        "plt.xlabel('Number of Dimensions')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('KNN Performance vs Dimensionality (Curse of Dimensionality)')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "print(\"âœ… K-Nearest Neighbors Analysis Complete!\")"
      ],
      "id": "8SqGzmzk0ofG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dllcqx4C0ofG"
      },
      "source": [
        "## K-Nearest Neighbors Interview Questions & Answers\n",
        "**Q1: What is the curse of dimensionality and how does it affect KNN?**\n",
        "**Answer:**\n",
        "- **Curse of dimensionality:** As number of features increases, data becomes sparse in high-dimensional space\n",
        "- **Effect on KNN:**\n",
        "- Distances between points become similar\n",
        "- Nearest neighbors may not be meaningful\n",
        "- Performance degrades with irrelevant features\n",
        "- Requires exponentially more data\n",
        "- **Solutions:** Feature selection, dimensionality reduction, careful feature engineering\n",
        "**Q2: How do you choose the optimal k value?**\n",
        "**Answer:**\n",
        "**Methods for choosing k:**\n",
        "1. **Cross-validation:** Try different k values and choose best performer\n",
        "2. **Rule of thumb:** $k = \\sqrt{n}$ where n is number of samples\n",
        "3. **Odd numbers:** Prefer odd k to avoid ties in binary classification\n",
        "4. **Domain knowledge:** Consider data characteristics\n",
        "5. **Elbow method:** Plot k vs error and choose elbow point\n",
        "**Q3: What are the advantages and disadvantages of weighted KNN?**\n",
        "**Answer:**\n",
        "- **Weighted KNN:** Closer neighbors have more influence\n",
        "- **Advantages:**\n",
        "- More nuanced predictions\n",
        "- Better handling of unevenly distributed data\n",
        "- Can improve performance\n",
        "- **Disadvantages:**\n",
        "- More computationally expensive\n",
        "- Sensitive to distance metric choice\n",
        "- May overfit with small k\n",
        "**Q4: How does KNN handle categorical features?**\n",
        "**Answer:**\n",
        "**For categorical features:**\n",
        "- **Use appropriate distance metrics:**\n",
        "- Hamming distance: For binary/categorical data\n",
        "- Jaccard distance: For set-based data\n",
        "- Custom distance functions\n",
        "- **One-hot encoding:** Convert to binary features\n",
        "- **Label encoding:** May not be appropriate (implies ordering)\n",
        "- **Best practice:** Use distance metrics designed for categorical data\n",
        "**Q5: When should you use KNN vs other classifiers?**\n",
        "**Answer:**\n",
        "**Use KNN when:**\n",
        "- Simple, interpretable model needed\n",
        "- Data has clear local patterns\n",
        "- Small to medium datasets\n",
        "- No explicit training time available\n",
        "- Want lazy learning (defer processing until prediction)\n",
        "**Avoid KNN when:**\n",
        "- Very large datasets (slow prediction)\n",
        "- High-dimensional data\n",
        "- Need fast predictions\n",
        "- Data has many irrelevant features\n",
        "- Clear global patterns exist\n",
        "# 8. NAIVE BAYES\n",
        "## Algorithm Background & Mathematical Foundation\n",
        "**Core Concept:** Naive Bayes is a probabilistic classifier based on Bayes' theorem with strong (naive) independence assumptions between features.\n",
        "**Mathematical Formulation:**\n",
        "**Bayes' Theorem:**\n",
        "$$P(Y|X) = \\frac{P(X|Y)P(Y)}{P(X)}$$\n",
        "**For Classification:**\n",
        "$$P(Y=k|X_1,X_2,...,X_n) = \\frac{P(Y=k)\\prod_{i=1}^n P(X_i|Y=k)}{P(X_1,X_2,...,X_n)}$$\n",
        "**Prediction Rule:**\n",
        "$$\\hat{y} = \\arg\\max_k P(Y=k)\\prod_{i=1}^n P(X_i|Y=k)$$\n",
        "**Types of Naive Bayes:**\n",
        "1. **Gaussian Naive Bayes:** Continuous features assumed normally distributed\n",
        "$$P(X_i|Y=k) = \\frac{1}{\\sqrt{2\\pi\\sigma_k^2}}\\exp\\left(-\\frac{(X_i - \\mu_k)^2}{2\\sigma_k^2}\\right)$$\n",
        "2. **Multinomial Naive Bayes:** Discrete features (word counts in text)\n",
        "$$P(X_i|Y=k) = \\frac{\\text{count}(X_i, Y=k) + \\alpha}{\\text{count}(Y=k) + \\alpha n}$$\n",
        "3. **Bernoulli Naive Bayes:** Binary features (word presence/absence)\n",
        "**Laplace Smoothing:**\n",
        "- Prevents zero probabilities: $P(X_i|Y=k) = \\frac{\\text{count} + \\alpha}{\\text{total} + \\alpha n}$\n",
        "- $\\alpha = 1$ (Laplace), $\\alpha < 1$ (Lidstone)"
      ],
      "id": "Dllcqx4C0ofG"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-MwxIJQ0ofG"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Cell 10: Naive Bayes - Comprehensive Implementation\n",
        "print(\"ðŸš€ NAIVE BAYES: COMPREHENSIVE IMPLEMENTATION\\n\")\n",
        "# Load dataset for text classification example\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "# We'll use a simpler dataset for demonstration\n",
        "categories = ['alt.atheism', 'sci.space', 'comp.graphics']\n",
        "newsgroups = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'), random_state=42)\n",
        "print(\"ðŸ“Š Dataset Overview:\")\n",
        "print(f\"â€¢ Dataset: 20 Newsgroups (subset)\")\n",
        "print(f\"â€¢ Samples: {len(newsgroups.data)}\")\n",
        "print(f\"â€¢ Categories: {newsgroups.target_names}\")\n",
        "print(f\"â€¢ Class distribution: {np.bincount(newsgroups.target)}\")\n",
        "# Sample of the data\n",
        "print(\"\\nðŸ“ Sample Documents:\")\n",
        "for i in range(2):\n",
        "print(f\"Category: {newsgroups.target_names[newsgroups.target[i]]}\")\n",
        "print(f\"Text preview: {newsgroups.data[i][:200]}...\")\n",
        "print()\n",
        "# Text preprocessing and feature extraction\n",
        "vectorizer = TfidfVectorizer(max_features=1000, stop_words='english', min_df=2, max_df=0.8)\n",
        "X_text = vectorizer.fit_transform(newsgroups.data)\n",
        "y_text = newsgroups.target\n",
        "print(f\"â€¢ Vocabulary size: {len(vectorizer.get_feature_names_out())}\")\n",
        "print(f\"â€¢ Feature matrix shape: {X_text.shape}\")\n",
        "# Split the data\n",
        "X_train_text, X_test_text, y_train_text, y_test_text = model_selection.train_test_split(\n",
        "X_text, y_text, test_size=0.2, random_state=42, stratify=y_text\n",
        ")\n",
        "# Train different Naive Bayes models\n",
        "nb_models = {\n",
        "'Multinomial NB': naive_bayes.MultinomialNB(),\n",
        "'Bernoulli NB': naive_bayes.BernoulliNB(),\n",
        "'Gaussian NB': naive_bayes.GaussianNB(),\n",
        "'Complement NB': naive_bayes.ComplementNB()\n",
        "}\n",
        "nb_results = {}\n",
        "for name, model in nb_models.items():\n",
        "print(f\"\\nðŸ“Š Training {name}...\")\n",
        "# Gaussian NB expects dense arrays, others work with sparse\n",
        "if name == 'Gaussian NB':\n",
        "X_train_dense = X_train_text.toarray()\n",
        "X_test_dense = X_test_text.toarray()\n",
        "model.fit(X_train_dense, y_train_text)\n",
        "y_pred = model.predict(X_test_dense)\n",
        "y_pred_proba = model.predict_proba(X_test_dense)\n",
        "else:\n",
        "model.fit(X_train_text, y_train_text)\n",
        "y_pred = model.predict(X_test_text)\n",
        "y_pred_proba = model.predict_proba(X_test_text)\n",
        "accuracy = metrics.accuracy_score(y_test_text, y_pred)\n",
        "precision = metrics.precision_score(y_test_text, y_pred, average='weighted')\n",
        "recall = metrics.recall_score(y_test_text, y_pred, average='weighted')\n",
        "f1 = metrics.f1_score(y_test_text, y_pred, average='weighted')\n",
        "nb_results[name] = {\n",
        "'accuracy': accuracy,\n",
        "'precision': precision,\n",
        "'recall': recall,\n",
        "'f1': f1,\n",
        "'model': model,\n",
        "'y_pred': y_pred,\n",
        "'y_pred_proba': y_pred_proba\n",
        "}\n",
        "print(f\" â€¢ Accuracy: {accuracy:.4f}\")\n",
        "print(f\" â€¢ Precision: {precision:.4f}\")\n",
        "print(f\" â€¢ Recall: {recall:.4f}\")\n",
        "print(f\" â€¢ F1-Score: {f1:.4f}\")\n",
        "# Model Comparison\n",
        "comparison_nb = pd.DataFrame({\n",
        "'Model': list(nb_results.keys()),\n",
        "'Accuracy': [nb_results[name]['accuracy'] for name in nb_results.keys()],\n",
        "'Precision': [nb_results[name]['precision'] for name in nb_results.keys()],\n",
        "'Recall': [nb_results[name]['recall'] for name in nb_results.keys()],\n",
        "'F1-Score': [nb_results[name]['f1'] for name in nb_results.keys()]\n",
        "})\n",
        "print(\"\\nðŸ“‹ Model Performance Comparison:\")\n",
        "print(comparison_nb.round(4))\n",
        "# Comprehensive Visualization\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "# 1. Confusion Matrix for Best Model\n",
        "best_nb_name = max(nb_results.keys(), key=lambda x: nb_results[x]['accuracy'])\n",
        "best_nb_model = nb_results[best_nb_name]['model']\n",
        "y_pred_best = nb_results[best_nb_name]['y_pred']\n",
        "cm = metrics.confusion_matrix(y_test_text, y_pred_best)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0,0],\n",
        "xticklabels=newsgroups.target_names, yticklabels=newsgroups.target_names)\n",
        "axes[0,0].set_xlabel('Predicted')\n",
        "axes[0,0].set_ylabel('Actual')\n",
        "axes[0,0].set_title(f'Confusion Matrix - {best_nb_name}')\n",
        "# 2. Feature Importance (Top Words per Class)\n",
        "if hasattr(best_nb_model, 'feature_log_prob_'):\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "n_top_words = 10\n",
        "for i, class_label in enumerate(newsgroups.target_names):\n",
        "top_features = np.argsort(best_nb_model.feature_log_prob_[i])[-n_top_words:]\n",
        "top_words = [feature_names[j] for j in top_features]\n",
        "print(f\"\\nðŸ”¤ Top words for '{class_label}':\")\n",
        "print(\", \".join(top_words))\n",
        "# 3. Model Comparison Bar Chart\n",
        "models_nb = list(nb_results.keys())\n",
        "accuracy_scores = [nb_results[name]['accuracy'] for name in models_nb]\n",
        "bars = axes[0,1].bar(models_nb, accuracy_scores, color=['blue', 'green', 'orange', 'red'])\n",
        "axes[0,1].set_ylabel('Accuracy')\n",
        "axes[0,1].set_title('Naive Bayes Variants Comparison')\n",
        "axes[0,1].tick_params(axis='x', rotation=45)\n",
        "for bar, score in zip(bars, accuracy_scores):\n",
        "axes[0,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "f'{score:.3f}', ha='center', va='bottom')\n",
        "# 4. Probability Calibration\n",
        "if hasattr(best_nb_model, 'predict_proba'):\n",
        "y_pred_proba_best = nb_results[best_nb_name]['y_pred_proba']\n",
        "# Binarize labels for calibration curve\n",
        "from sklearn.preprocessing import label_binarize\n",
        "y_test_bin = label_binarize(y_test_text, classes=range(len(newsgroups.target_names)))\n",
        "# Plot calibration curves for each class\n",
        "for i in range(len(newsgroups.target_names)):\n",
        "fraction_of_positives, mean_predicted_value = calibration_curve(\n",
        "y_test_bin[:, i], y_pred_proba_best[:, i], n_bins=10\n",
        ")\n",
        "axes[1,0].plot(mean_predicted_value, fraction_of_positives, \"s-\",\n",
        "label=newsgroups.target_names[i])\n",
        "axes[1,0].plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
        "axes[1,0].set_xlabel('Mean Predicted Probability')\n",
        "axes[1,0].set_ylabel('Fraction of Positives')\n",
        "axes[1,0].set_title('Probability Calibration')\n",
        "axes[1,0].legend()\n",
        "axes[1,0].grid(True)\n",
        "# 5. Alpha Parameter Tuning (Smoothing)\n",
        "alphas = [0.001, 0.01, 0.1, 1, 10, 100]\n",
        "alpha_scores = []\n",
        "for alpha in alphas:\n",
        "nb_temp = naive_bayes.MultinomialNB(alpha=alpha)\n",
        "nb_temp.fit(X_train_text, y_train_text)\n",
        "alpha_scores.append(nb_temp.score(X_test_text, y_test_text))\n",
        "axes[1,1].semilogx(alphas, alpha_scores, 'o-', color='purple')\n",
        "axes[1,1].set_xlabel('Alpha (Smoothing Parameter)')\n",
        "axes[1,1].set_ylabel('Accuracy')\n",
        "axes[1,1].set_title('Alpha Parameter Tuning')\n",
        "axes[1,1].grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "# Real-time Prediction Example\n",
        "print(\"\\nðŸŽ¯ Real-time Text Classification Example:\")\n",
        "# Train final model on all data\n",
        "final_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english', min_df=2, max_df=0.8)\n",
        "X_final = final_vectorizer.fit_transform(newsgroups.data)\n",
        "final_model = naive_bayes.MultinomialNB(alpha=1.0)\n",
        "final_model.fit(X_final, newsgroups.target)\n",
        "# Test sentences\n",
        "test_sentences = [\n",
        "\"The space shuttle launched successfully into orbit\",\n",
        "\"Computer graphics and animation techniques\",\n",
        "\"Religious beliefs and atheism discussion\"\n",
        "]\n",
        "for i, sentence in enumerate(test_sentences):\n",
        "sentence_vec = final_vectorizer.transform([sentence])\n",
        "prediction = final_model.predict(sentence_vec)[0]\n",
        "probabilities = final_model.predict_proba(sentence_vec)[0]\n",
        "print(f\"\\nSentence {i+1}: '{sentence}'\")\n",
        "print(f\"Predicted category: {newsgroups.target_names[prediction]}\")\n",
        "print(\"Probabilities:\")\n",
        "for j, category in enumerate(newsgroups.target_names):\n",
        "print(f\" {category}: {probabilities[j]:.4f}\")\n",
        "# Numerical Data Example with Gaussian Naive Bayes\n",
        "print(\"\\nðŸ“Š Gaussian Naive Bayes on Numerical Data:\")\n",
        "# Use iris dataset for Gaussian NB demonstration\n",
        "iris = datasets.load_iris()\n",
        "X_iris = iris.data\n",
        "y_iris = iris.target\n",
        "X_train_iris, X_test_iris, y_train_iris, y_test_iris = model_selection.train_test_split(\n",
        "X_iris, y_iris, test_size=0.2, random_state=42, stratify=y_iris\n",
        ")\n",
        "# Scale features\n",
        "scaler_iris = StandardScaler()\n",
        "X_train_iris_scaled = scaler_iris.fit_transform(X_train_iris)\n",
        "X_test_iris_scaled = scaler_iris.transform(X_test_iris)\n",
        "# Train Gaussian NB\n",
        "gnb = naive_bayes.GaussianNB()\n",
        "gnb.fit(X_train_iris_scaled, y_train_iris)\n",
        "y_pred_iris = gnb.predict(X_test_iris_scaled)\n",
        "accuracy_iris = metrics.accuracy_score(y_test_iris, y_pred_iris)\n",
        "print(f\"Gaussian NB on Iris dataset: Accuracy = {accuracy_iris:.4f}\")\n",
        "# Visualize Gaussian distributions\n",
        "plt.figure(figsize=(12, 8))\n",
        "features = iris.feature_names\n",
        "classes = iris.target_names\n",
        "for i in range(4): # For each feature\n",
        "plt.subplot(2, 2, i+1)\n",
        "for class_idx in range(3): # For each class\n",
        "# Get feature values for this class\n",
        "feature_values = X_iris[y_iris == class_idx, i]\n",
        "# Plot histogram\n",
        "plt.hist(feature_values, alpha=0.7, density=True,\n",
        "label=classes[class_idx], bins=15)\n",
        "# Plot fitted normal distribution\n",
        "xmin, xmax = plt.xlim()\n",
        "x = np.linspace(xmin, xmax, 100)\n",
        "mean = gnb.theta_[class_idx, i]\n",
        "std = np.sqrt(gnb.var_[class_idx, i])\n",
        "p = np.exp(-0.5 * ((x - mean) / std) ** 2) / (std * np.sqrt(2 * np.pi))\n",
        "plt.plot(x, p, 'k', linewidth=2)\n",
        "plt.xlabel(features[i])\n",
        "plt.ylabel('Density')\n",
        "plt.title(f'Feature: {features[i]}')\n",
        "if i == 0:\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "print(\"âœ… Naive Bayes Analysis Complete!\")"
      ],
      "id": "O-MwxIJQ0ofG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlqVUf9c0ofH"
      },
      "source": [
        "## Naive Bayes Interview Questions & Answers\n",
        "**Q1: Why is it called \"Naive\" Bayes?**\n",
        "**Answer:**\n",
        "- **Naive assumption:** Features are conditionally independent given the class\n",
        "- **Mathematically:** $P(X_1,X_2,...,X_n|Y) = \\prod_{i=1}^n P(X_i|Y)$\n",
        "- **Why naive:** This assumption is rarely true in real-world data\n",
        "- **Why it works:** Even with violated assumptions, it often performs well\n",
        "- **Benefits:** Simple, fast, works well with high-dimensional data\n",
        "**Q2: What is Laplace smoothing and why is it important?**\n",
        "**Answer:**\n",
        "- **Problem:** Zero probabilities when feature doesn't appear in class\n",
        "- **Laplace smoothing:** Add small constant Î± to all counts\n",
        "- **Formula:** $P(X_i|Y=k) = \\frac{\\text{count}(X_i,Y=k) + \\alpha}{\\text{count}(Y=k) + \\alpha n}$\n",
        "- **Purpose:** Prevents zero probabilities, improves generalization\n",
        "- **Typical values:** Î± = 1 (Laplace), Î± < 1 (Lidstone)\n",
        "**Q3: Compare different Naive Bayes variants.**\n",
        "**Answer:**\n",
        "- **Gaussian NB:** Continuous features, assumes normal distribution\n",
        "- **Multinomial NB:** Discrete counts (text classification, word frequencies)\n",
        "- **Bernoulli NB:** Binary features (word presence/absence)\n",
        "- **Complement NB:** Improved version for imbalanced datasets\n",
        "- **Choice depends on:** Feature type and distribution\n",
        "**Q4: What are the advantages of Naive Bayes?**\n",
        "**Answer:**\n",
        "- **Fast training and prediction:** Simple probability calculations\n",
        "- **Works well with high dimensions:** Text classification with thousands of features\n",
        "- **Handles missing data:** Naturally through probability estimation\n",
        "- **Incremental learning:** Can update with new data easily\n",
        "- **Interpretable:** Probabilities have clear meaning\n",
        "- **Good baseline:** Often works surprisingly well despite naive assumption\n",
        "**Q5: When should you use Naive Bayes vs other classifiers?**\n",
        "**Answer:**\n",
        "**Use Naive Bayes when:**\n",
        "- Text classification problems\n",
        "- High-dimensional feature spaces\n",
        "- Need fast training/prediction\n",
        "- Want probability estimates\n",
        "- Dealing with categorical features\n",
        "- Need a simple baseline model\n",
        "**Avoid Naive Bayes when:**\n",
        "- Strong feature dependencies exist\n",
        "- Need very high accuracy\n",
        "- Features have complex relationships\n",
        "- Probability calibration is critical\n",
        "- Dealing with small datasets with continuous features\n",
        "# 9. K-MEANS CLUSTERING\n",
        "## Algorithm Background & Mathematical Foundation\n",
        "**Core Concept:** K-means partitions data into K clusters by minimizing within-cluster variance (sum of squared distances).\n",
        "**Mathematical Formulation:**\n",
        "**Objective Function:**\n",
        "$$J = \\sum_{i=1}^k \\sum_{x \\in C_i} \\|x - \\mu_i\\|^2$$\n",
        "Where:\n",
        "- $C_i$ is the i-th cluster\n",
        "- $\\mu_i$ is the centroid of cluster i\n",
        "- $\\|x - \\mu_i\\|^2$ is squared Euclidean distance\n",
        "**Algorithm Steps:**\n",
        "1. **Initialize:** Randomly select K centroids\n",
        "2. **Assignment:** Assign each point to nearest centroid\n",
        "$$C_i = \\{x : \\|x - \\mu_i\\|^2 \\leq \\|x - \\mu_j\\|^2 \\ \\forall j\\}$$\n",
        "3. **Update:** Recompute centroids as mean of assigned points\n",
        "$$\\mu_i = \\frac{1}{|C_i|} \\sum_{x \\in C_i} x$$\n",
        "4. **Repeat** steps 2-3 until convergence\n",
        "**Initialization Methods:**\n",
        "- **Random:** Pure random selection\n",
        "- **K-means++:** Smart initialization to spread out centroids\n",
        "- **Deterministic:** Pre-specified initial centroids"
      ],
      "id": "hlqVUf9c0ofH"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teV3He1S0ofH"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Cell 11: K-Means Clustering - Comprehensive Implementation\n",
        "print(\"ðŸš€ K-MEANS CLUSTERING: COMPREHENSIVE IMPLEMENTATION\\n\")\n",
        "# Create sample dataset for clustering\n",
        "from sklearn.datasets import make_blobs\n",
        "X, y_true = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=42)\n",
        "print(\"ðŸ“Š Dataset Overview:\")\n",
        "print(f\"â€¢ Samples: {X.shape[0]}, Features: {X.shape[1]}\")\n",
        "print(f\"â€¢ True clusters: {len(np.unique(y_true))}\")\n",
        "print(f\"â€¢ Cluster distribution: {np.bincount(y_true)}\")\n",
        "# Visualize the true clusters\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_true, cmap='viridis', s=50, alpha=0.8)\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('True Clusters')\n",
        "plt.grid(True)\n",
        "# Apply K-means with different K values\n",
        "k_values = [2, 3, 4, 5, 6]\n",
        "kmeans_models = {}\n",
        "for k in k_values:\n",
        "kmeans = cluster.KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "y_pred = kmeans.fit_predict(X)\n",
        "kmeans_models[k] = {\n",
        "'model': kmeans,\n",
        "'labels': y_pred,\n",
        "'inertia': kmeans.inertia_,\n",
        "'centroids': kmeans.cluster_centers_\n",
        "}\n",
        "# Elbow Method for optimal K\n",
        "inertias = [kmeans_models[k]['inertia'] for k in k_values]\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(k_values, inertias, 'bo-')\n",
        "plt.xlabel('Number of Clusters (K)')\n",
        "plt.ylabel('Within-cluster Sum of Squares (Inertia)')\n",
        "plt.title('Elbow Method for Optimal K')\n",
        "plt.grid(True)\n",
        "# Silhouette Analysis\n",
        "from sklearn.metrics import silhouette_score\n",
        "silhouette_scores = []\n",
        "for k in k_values:\n",
        "if k > 1: # Silhouette score requires at least 2 clusters\n",
        "score = silhouette_score(X, kmeans_models[k]['labels'])\n",
        "silhouette_scores.append(score)\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.plot(k_values[1:], silhouette_scores, 'ro-')\n",
        "plt.xlabel('Number of Clusters (K)')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.title('Silhouette Analysis')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "# Comprehensive K-means Analysis\n",
        "optimal_k = k_values[np.argmin(inertias)] # Simple elbow detection\n",
        "print(f\"ðŸŽ¯ Optimal K based on elbow method: {optimal_k}\")\n",
        "# Visualize clustering results for different K\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "for i, k in enumerate(k_values):\n",
        "row = i // 3\n",
        "col = i % 3\n",
        "model_info = kmeans_models[k]\n",
        "# Plot clusters\n",
        "scatter = axes[row, col].scatter(X[:, 0], X[:, 1], c=model_info['labels'],\n",
        "cmap='viridis', s=50, alpha=0.7)\n",
        "# Plot centroids\n",
        "axes[row, col].scatter(model_info['centroids'][:, 0], model_info['centroids'][:, 1],\n",
        "marker='x', s=200, linewidths=3, color='red', label='Centroids')\n",
        "axes[row, col].set_xlabel('Feature 1')\n",
        "axes[row, col].set_ylabel('Feature 2')\n",
        "axes[row, col].set_title(f'K-means (K={k})\\nInertia: {model_info[\"inertia\"]:.2f}')\n",
        "axes[row, col].legend()\n",
        "axes[row, col].grid(True)\n",
        "# Remove empty subplot\n",
        "if len(k_values) < 6:\n",
        "axes[1, 2].set_visible(False)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "# Advanced K-means Analysis\n",
        "print(\"\\nðŸ” Advanced K-means Analysis:\")\n",
        "# Compare different initialization methods\n",
        "init_methods = ['random', 'k-means++']\n",
        "init_results = {}\n",
        "for init_method in init_methods:\n",
        "kmeans_temp = cluster.KMeans(n_clusters=4, init=init_method, random_state=42, n_init=10)\n",
        "y_pred_temp = kmeans_temp.fit_predict(X)\n",
        "inertia = kmeans_temp.inertia_\n",
        "silhouette = silhouette_score(X, y_pred_temp)\n",
        "init_results[init_method] = {\n",
        "'inertia': inertia,\n",
        "'silhouette': silhouette,\n",
        "'labels': y_pred_temp\n",
        "}\n",
        "print(f\"\\nInitialization: {init_method}\")\n",
        "print(f\"â€¢ Inertia: {inertia:.4f}\")\n",
        "print(f\"â€¢ Silhouette Score: {silhouette:.4f}\")\n",
        "# K-means on different dataset shapes\n",
        "print(\"\\nðŸ“Š K-means on Different Data Distributions:\")\n",
        "# Create different dataset shapes\n",
        "datasets_cluster = {\n",
        "'Blobs': make_blobs(n_samples=300, centers=3, random_state=42),\n",
        "'Moons': datasets.make_moons(n_samples=300, noise=0.05, random_state=42),\n",
        "'Circles': datasets.make_circles(n_samples=300, noise=0.05, factor=0.5, random_state=42),\n",
        "'Anisotropic': datasets.make_blobs(n_samples=300, centers=3, random_state=42)\n",
        "}\n",
        "# Make anisotropic data\n",
        "transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
        "X_aniso = np.dot(datasets_cluster['Anisotropic'][0], transformation)\n",
        "datasets_cluster['Anisotropic'] = (X_aniso, datasets_cluster['Anisotropic'][1])\n",
        "# Apply K-means to different datasets\n",
        "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
        "for i, (dataset_name, (X_data, y_true_data)) in enumerate(datasets_cluster.items()):\n",
        "# True labels\n",
        "axes[0, i].scatter(X_data[:, 0], X_data[:, 1], c=y_true_data, cmap='viridis', s=50, alpha=0.8)\n",
        "axes[0, i].set_title(f'{dataset_name}\\n(True Clusters)')\n",
        "axes[0, i].set_xlabel('Feature 1')\n",
        "axes[0, i].set_ylabel('Feature 2')\n",
        "axes[0, i].grid(True)\n",
        "# K-means results\n",
        "kmeans_data = cluster.KMeans(n_clusters=3, random_state=42)\n",
        "y_pred_data = kmeans_data.fit_predict(X_data)\n",
        "axes[1, i].scatter(X_data[:, 0], X_data[:, 1], c=y_pred_data, cmap='viridis', s=50, alpha=0.8)\n",
        "axes[1, i].scatter(kmeans_data.cluster_centers_[:, 0], kmeans_data.cluster_centers_[:, 1],\n",
        "marker='x', s=200, linewidths=3, color='red')\n",
        "axes[1, i].set_title(f'{dataset_name}\\n(K-means Clusters)')\n",
        "axes[1, i].set_xlabel('Feature 1')\n",
        "axes[1, i].set_ylabel('Feature 2')\n",
        "axes[1, i].grid(True)\n",
        "# Calculate metrics\n",
        "inertia = kmeans_data.inertia_\n",
        "silhouette = silhouette_score(X_data, y_pred_data)\n",
        "print(f\"\\n{dataset_name}:\")\n",
        "print(f\"â€¢ Inertia: {inertia:.4f}\")\n",
        "print(f\"â€¢ Silhouette Score: {silhouette:.4f}\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "# K-means for Image Compression\n",
        "print(\"\\nðŸ–¼ï¸ K-means for Image Compression:\")\n",
        "# Load a sample image\n",
        "try:\n",
        "from sklearn.datasets import load_sample_image\n",
        "china = load_sample_image(\"china.jpg\")\n",
        "# Convert to float and reshape\n",
        "china = china / 255.0\n",
        "w, h, d = original_shape = tuple(china.shape)\n",
        "image_array = np.reshape(china, (w * h, d))\n",
        "print(f\"Image shape: {china.shape}\")\n",
        "print(f\"Reshaped for clustering: {image_array.shape}\")\n",
        "# Apply K-means for color quantization\n",
        "n_colors = 16\n",
        "kmeans_image = cluster.KMeans(n_clusters=n_colors, random_state=42)\n",
        "labels = kmeans_image.fit_predict(image_array)\n",
        "colors = kmeans_image.cluster_centers_\n",
        "# Recreate compressed image\n",
        "compressed_image = colors[labels].reshape(w, h, d)\n",
        "# Plot original and compressed images\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "axes[0].imshow(china)\n",
        "axes[0].set_title('Original Image')\n",
        "axes[0].axis('off')\n",
        "axes[1].imshow(compressed_image)\n",
        "axes[1].set_title(f'Compressed Image ({n_colors} colors)')\n",
        "axes[1].axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "print(f\"Original image: {china.shape[0]}x{china.shape[1]} pixels, {china.shape[2]} channels\")\n",
        "print(f\"Compressed to: {n_colors} colors\")\n",
        "except ImportError:\n",
        "print(\"Sample image dataset not available. Using alternative demonstration.\")\n",
        "# Create synthetic image data\n",
        "synthetic_image = np.random.rand(100, 100, 3)\n",
        "w, h, d = synthetic_image.shape\n",
        "image_array = np.reshape(synthetic_image, (w * h, d))\n",
        "n_colors = 8\n",
        "kmeans_image = cluster.KMeans(n_clusters=n_colors, random_state=42)\n",
        "labels = kmeans_image.fit_predict(image_array)\n",
        "colors = kmeans_image.cluster_centers_\n",
        "compressed_image = colors[labels].reshape(w, h, d)\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "axes[0].imshow(synthetic_image)\n",
        "axes[0].set_title('Synthetic Image')\n",
        "axes[0].axis('off')\n",
        "axes[1].imshow(compressed_image)\n",
        "axes[1].set_title(f'Compressed ({n_colors} colors)')\n",
        "axes[1].axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "# Customer Segmentation Example\n",
        "print(\"\\nðŸ‘¥ Customer Segmentation with K-means:\")\n",
        "# Create synthetic customer data\n",
        "np.random.seed(42)\n",
        "n_customers = 200\n",
        "# Generate customer features: age, annual income, spending score\n",
        "age = np.random.normal(35, 10, n_customers).clip(18, 70)\n",
        "income = np.random.normal(50000, 20000, n_customers).clip(20000, 100000)\n",
        "spending = np.random.normal(50, 20, n_customers).clip(1, 100)\n",
        "customer_data = np.column_stack([age, income, spending])\n",
        "# Scale the data\n",
        "scaler_customer = StandardScaler()\n",
        "customer_data_scaled = scaler_customer.fit_transform(customer_data)\n",
        "# Apply K-means\n",
        "kmeans_customer = cluster.KMeans(n_clusters=4, random_state=42)\n",
        "customer_labels = kmeans_customer.fit_predict(customer_data_scaled)\n",
        "# Visualize customer segments\n",
        "fig = plt.figure(figsize=(15, 5))\n",
        "# Age vs Income\n",
        "plt.subplot(1, 3, 1)\n",
        "scatter = plt.scatter(age, income, c=customer_labels, cmap='viridis', alpha=0.7)\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Annual Income ($)')\n",
        "plt.title('Customer Segments: Age vs Income')\n",
        "plt.colorbar(scatter, label='Cluster')\n",
        "plt.grid(True)\n",
        "# Age vs Spending\n",
        "plt.subplot(1, 3, 2)\n",
        "scatter = plt.scatter(age, spending, c=customer_labels, cmap='viridis', alpha=0.7)\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Spending Score')\n",
        "plt.title('Customer Segments: Age vs Spending')\n",
        "plt.colorbar(scatter, label='Cluster')\n",
        "plt.grid(True)\n",
        "# Income vs Spending\n",
        "plt.subplot(1, 3, 3)\n",
        "scatter = plt.scatter(income, spending, c=customer_labels, cmap='viridis', alpha=0.7)\n",
        "plt.xlabel('Annual Income ($)')\n",
        "plt.ylabel('Spending Score')\n",
        "plt.title('Customer Segments: Income vs Spending')\n",
        "plt.colorbar(scatter, label='Cluster')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "# Analyze customer segments\n",
        "print(\"\\nðŸ“ˆ Customer Segment Analysis:\")\n",
        "for cluster_id in range(4):\n",
        "cluster_mask = customer_labels == cluster_id\n",
        "cluster_size = np.sum(cluster_mask)\n",
        "if cluster_size > 0:\n",
        "avg_age = np.mean(age[cluster_mask])\n",
        "avg_income = np.mean(income[cluster_mask])\n",
        "avg_spending = np.mean(spending[cluster_mask])\n",
        "print(f\"\\nSegment {cluster_id} (Size: {cluster_size}):\")\n",
        "print(f\"â€¢ Average Age: {avg_age:.1f} years\")\n",
        "print(f\"â€¢ Average Income: ${avg_income:.0f}\")\n",
        "print(f\"â€¢ Average Spending: {avg_spending:.1f}\")\n",
        "print(\"âœ… K-means Clustering Analysis Complete!\")"
      ],
      "id": "teV3He1S0ofH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eu-fNIY0ofH"
      },
      "source": [
        "## K-means Interview Questions & Answers\n",
        "**Q1: How do you choose the optimal number of clusters K?**\n",
        "**Answer:**\n",
        "**Methods for choosing K:**\n",
        "1. **Elbow Method:** Plot inertia vs K, choose elbow point\n",
        "2. **Silhouette Analysis:** Measure how similar points are to their own cluster vs other clusters\n",
        "3. **Gap Statistic:** Compare inertia with expected inertia from null reference\n",
        "4. **Domain Knowledge:** Use business context or prior knowledge\n",
        "5. **Visual Inspection:** When possible, visualize clusters\n",
        "**Q2: What are the limitations of K-means?**\n",
        "**Answer:**\n",
        "- **Assumes spherical clusters:** Struggles with elongated or irregular shapes\n",
        "- **Sensitive to initialization:** Different runs can give different results\n",
        "- **Requires specifying K:** Number of clusters must be known\n",
        "- **Sensitive to outliers:** Outliers can distort centroids\n",
        "- **Scale-dependent:** Features should be standardized\n",
        "- **Local minima:** Can converge to suboptimal solutions\n",
        "**Q3: Explain K-means++ initialization.**\n",
        "**Answer:**\n",
        "**K-means++ improvement:**\n",
        "1. Choose first centroid randomly from data points\n",
        "2. For each subsequent centroid:\n",
        "- Compute distance from each point to nearest existing centroid\n",
        "- Choose new centroid with probability proportional to squared distance\n",
        "3. Repeat until K centroids chosen\n",
        "**Benefits:**\n",
        "- Spreads out initial centroids\n",
        "- Faster convergence\n",
        "- More consistent results\n",
        "- Better final solutions\n",
        "**Q4: How does K-means handle categorical data?**\n",
        "**Answer:**\n",
        "**K-means limitations with categorical data:**\n",
        "- Designed for continuous numerical data\n",
        "- Euclidean distance doesn't work well with categorical variables\n",
        "- **Solutions:**\n",
        "1. Use K-modes algorithm (designed for categorical data)\n",
        "2. One-hot encoding (but creates high-dimensional space)\n",
        "3. Use appropriate distance metrics (Hamming distance)\n",
        "4. Consider other clustering algorithms\n",
        "**Q5: What is the time and space complexity of K-means?**\n",
        "**Answer:**\n",
        "- **Time complexity:** O(n * K * I * d)\n",
        "- n: number of points\n",
        "- K: number of clusters\n",
        "- I: number of iterations\n",
        "- d: number of dimensions\n",
        "- **Space complexity:** O((n + K) * d)\n",
        "- **Efficient for:** Large n, small K and d\n",
        "- **Inefficient for:** Very high-dimensional data\n",
        "# 10. HIERARCHICAL CLUSTERING\n",
        "## Algorithm Background & Mathematical Foundation\n",
        "**Core Concept:** Hierarchical clustering builds a tree of clusters (dendrogram) by successively merging or splitting clusters based on similarity.\n",
        "**Types of Hierarchical Clustering:**\n",
        "1. **Agglomerative (Bottom-up):** Start with each point as individual cluster, merge closest pairs\n",
        "2. **Divisive (Top-down):** Start with one cluster, recursively split into smaller clusters\n",
        "**Linkage Criteria:**\n",
        "- **Single Linkage:** Minimum distance between clusters\n",
        "$$d(C_i, C_j) = \\min_{x \\in C_i, y \\in C_j} d(x, y)$$\n",
        "- **Complete Linkage:** Maximum distance between clusters\n",
        "$$d(C_i, C_j) = \\max_{x \\in C_i, y \\in C_j} d(x, y)$$\n",
        "- **Average Linkage:** Average distance between clusters\n",
        "$$d(C_i, C_j) = \\frac{1}{|C_i||C_j|} \\sum_{x \\in C_i} \\sum_{y \\in C_j} d(x, y)$$\n",
        "- **Ward's Method:** Minimizes within-cluster variance\n",
        "$$d(C_i, C_j) = \\frac{|C_i||C_j|}{|C_i| + |C_j|} \\|\\mu_i - \\mu_j\\|^2$$\n",
        "**Dendrogram:** Tree diagram showing hierarchical relationships and merge distances."
      ],
      "id": "2eu-fNIY0ofH"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saOFckVs0ofH"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Cell 12: Hierarchical Clustering - Comprehensive Implementation\n",
        "print(\"ðŸš€ HIERARCHICAL CLUSTERING: COMPREHENSIVE IMPLEMENTATION\\n\")\n",
        "# Create sample dataset\n",
        "X, y_true = make_blobs(n_samples=150, centers=3, cluster_std=0.60, random_state=42)\n",
        "print(\"ðŸ“Š Dataset Overview:\")\n",
        "print(f\"â€¢ Samples: {X.shape[0]}, Features: {X.shape[1]}\")\n",
        "print(f\"â€¢ True clusters: {len(np.unique(y_true))}\")\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "# Compare different linkage methods\n",
        "linkage_methods = ['single', 'complete', 'average', 'ward']\n",
        "linkage_results = {}\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i, linkage_method in enumerate(linkage_methods):\n",
        "# Perform hierarchical clustering\n",
        "hierarchical = cluster.AgglomerativeClustering(\n",
        "n_clusters=3, linkage=linkage_method\n",
        ")\n",
        "labels = hierarchical.fit_predict(X_scaled)\n",
        "linkage_results[linkage_method] = {\n",
        "'labels': labels,\n",
        "'model': hierarchical\n",
        "}\n",
        "# Plot clustering results\n",
        "plt.subplot(2, 3, i+1)\n",
        "scatter = plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50, alpha=0.8)\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title(f'Linkage: {linkage_method}\\nSilhouette: {silhouette_score(X_scaled, labels):.3f}')\n",
        "plt.grid(True)\n",
        "# Plot true labels for comparison\n",
        "plt.subplot(2, 3, 5)\n",
        "scatter = plt.scatter(X[:, 0], X[:, 1], c=y_true, cmap='viridis', s=50, alpha=0.8)\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('True Clusters')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "# Dendrogram Visualization\n",
        "print(\"\\nðŸŒ³ Dendrogram Visualization:\")\n",
        "# Create linkage matrix for dendrogram\n",
        "linkage_matrix = linkage(X_scaled, method='ward')\n",
        "plt.figure(figsize=(12, 8))\n",
        "dendrogram(linkage_matrix, truncate_mode='level', p=5)\n",
        "plt.xlabel('Sample Index or (Cluster Size)')\n",
        "plt.ylabel('Distance')\n",
        "plt.title('Hierarchical Clustering Dendrogram (Ward Linkage)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "# Comprehensive linkage comparison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "for i, linkage_method in enumerate(linkage_methods):\n",
        "row = i // 2\n",
        "col = i % 2\n",
        "# Create linkage matrix for this method\n",
        "linkage_matrix_method = linkage(X_scaled, method=linkage_method)\n",
        "# Plot dendrogram\n",
        "dendrogram(linkage_matrix_method, ax=axes[row, col], truncate_mode='level', p=5)\n",
        "axes[row, col].set_title(f'Dendrogram - {linkage_method.capitalize()} Linkage')\n",
        "axes[row, col].set_xlabel('Sample Index')\n",
        "axes[row, col].set_ylabel('Distance')\n",
        "axes[row, col].grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "# Cut dendrogram at different levels\n",
        "print(\"\\nâœ‚ï¸ Cutting Dendrogram at Different Levels:\")\n",
        "n_clusters_range = [2, 3, 4, 5, 6]\n",
        "cluster_results = {}\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i, n_clusters in enumerate(n_clusters_range):\n",
        "# Cut dendrogram to get n clusters\n",
        "labels = fcluster(linkage_matrix, n_clusters, criterion='maxclust')\n",
        "cluster_results[n_clusters] = {\n",
        "'labels': labels,\n",
        "'silhouette': silhouette_score(X_scaled, labels)\n",
        "}\n",
        "# Plot clustering results\n",
        "plt.subplot(2, 3, i+1)\n",
        "scatter = plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50, alpha=0.8)\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title(f'K={n_clusters}\\nSilhouette: {cluster_results[n_clusters][\"silhouette\"]:.3f}')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "# Compare with K-means\n",
        "print(\"\\nðŸ†š Comparison with K-means:\")\n",
        "kmeans_comparison = cluster.KMeans(n_clusters=3, random_state=42)\n",
        "kmeans_labels = kmeans_comparison.fit_predict(X_scaled)\n",
        "hierarchical_ward = cluster.AgglomerativeClustering(n_clusters=3, linkage='ward')\n",
        "hierarchical_labels = hierarchical_ward.fit_predict(X_scaled)\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "# True clusters\n",
        "scatter = axes[0].scatter(X[:, 0], X[:, 1], c=y_true, cmap='viridis', s=50, alpha=0.8)\n",
        "axes[0].set_xlabel('Feature 1')\n",
        "axes[0].set_ylabel('Feature 2')\n",
        "axes[0].set_title('True Clusters')\n",
        "axes[0].grid(True)\n",
        "# K-means\n",
        "scatter = axes[1].scatter(X[:, 0], X[:, 1], c=kmeans_labels, cmap='viridis', s=50, alpha=0.8)\n",
        "axes[1].set_xlabel('Feature 1')\n",
        "axes[1].set_ylabel('Feature 2')\n",
        "axes[1].set_title(f'K-means\\nSilhouette: {silhouette_score(X_scaled, kmeans_labels):.3f}')\n",
        "axes[1].grid(True)\n",
        "# Hierarchical\n",
        "scatter = axes[2].scatter(X[:, 0], X[:, 1], c=hierarchical_labels, cmap='viridis', s=50, alpha=0.8)\n",
        "axes[2].set_xlabel('Feature 1')\n",
        "axes[2].set_ylabel('Feature 2')\n",
        "axes[2].set_title(f'Hierarchical (Ward)\\nSilhouette: {silhouette_score(X_scaled, hierarchical_labels):.3f}')\n",
        "axes[2].grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "# Advanced: Hierarchical clustering on different dataset shapes\n",
        "print(\"\\nðŸ“Š Hierarchical Clustering on Complex Data Shapes:\")\n",
        "complex_datasets = {\n",
        "'Moons': datasets.make_moons(n_samples=150, noise=0.05, random_state=42),\n",
        "'Circles': datasets.make_circles(n_samples=150, noise=0.05, factor=0.5, random_state=42),\n",
        "'Anisotropic': (np.dot(X, [[0.6, -0.6], [-0.4, 0.8]]), y_true)\n",
        "}\n",
        "fig, axes = plt.subplots(3, 4, figsize=(20, 15))\n",
        "for i, (dataset_name, (X_data, y_true_data)) in enumerate(complex_datasets.items()):\n",
        "# Scale data\n",
        "X_data_scaled = StandardScaler().fit_transform(X_data)\n",
        "# True clusters\n",
        "axes[i, 0].scatter(X_data[:, 0], X_data[:, 1], c=y_true_data, cmap='viridis', s=50, alpha=0.8)\n",
        "axes[i, 0].set_title(f'{dataset_name}\\nTrue Clusters')\n",
        "axes[i, 0].set_xlabel('Feature 1')\n",
        "axes[i, 0].set_ylabel('Feature 2')\n",
        "axes[i, 0].grid(True)\n",
        "# Different linkage methods\n",
        "for j, linkage_method in enumerate(['single', 'complete', 'ward']):\n",
        "hierarchical_temp = cluster.AgglomerativeClustering(\n",
        "n_clusters=2, linkage=linkage_method\n",
        ")\n",
        "labels_temp = hierarchical_temp.fit_predict(X_data_scaled)\n",
        "axes[i, j+1].scatter(X_data[:, 0], X_data[:, 1], c=labels_temp, cmap='viridis', s=50, alpha=0.8)\n",
        "axes[i, j+1].set_title(f'{linkage_method.capitalize()} Linkage\\nSilhouette: {silhouette_score(X_data_scaled, labels_temp):.3f}')\n",
        "axes[i, j+1].set_xlabel('Feature 1')\n",
        "axes[i, j+1].set_ylabel('Feature 2')\n",
        "axes[i, j+1].grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "# Practical Application: Customer Segmentation\n",
        "print(\"\\nðŸ‘¥ Customer Segmentation with Hierarchical Clustering:\")\n",
        "# Use the customer data from K-means example\n",
        "if 'customer_data_scaled' in locals():\n",
        "# Perform hierarchical clustering\n",
        "linkage_customer = linkage(customer_data_scaled, method='ward')\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "# Dendrogram\n",
        "dendrogram(linkage_customer, ax=axes[0], truncate_mode='lastp', p=12)\n",
        "axes[0].set_title('Customer Segmentation Dendrogram')\n",
        "axes[0].set_xlabel('Customer Index')\n",
        "axes[0].set_ylabel('Distance')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "# Cut dendrogram to get 4 clusters\n",
        "customer_hierarchical_labels = fcluster(linkage_customer, 4, criterion='maxclust')\n",
        "# Visualize segments\n",
        "scatter = axes[1].scatter(age, income, c=customer_hierarchical_labels,\n",
        "cmap='viridis', alpha=0.7)\n",
        "axes[1].set_xlabel('Age')\n",
        "axes[1].set_ylabel('Annual Income ($)')\n",
        "axes[1].set_title('Hierarchical Clustering: Customer Segments')\n",
        "axes[1].grid(True)\n",
        "plt.colorbar(scatter, ax=axes[1], label='Cluster')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "# Compare with K-means\n",
        "print(\"\\nðŸ“Š Comparison: K-means vs Hierarchical Clustering\")\n",
        "print(f\"K-means Silhouette Score: {silhouette_score(customer_data_scaled, customer_labels):.3f}\")\n",
        "print(f\"Hierarchical Silhouette Score: {silhouette_score(customer_data_scaled, customer_hierarchical_labels):.3f}\")\n",
        "# Memory and Computational Considerations\n",
        "print(\"\\nðŸ’¾ Computational Considerations:\")\n",
        "sample_sizes = [100, 500, 1000, 2000]\n",
        "times_hierarchical = []\n",
        "times_kmeans = []\n",
        "for n_samples in sample_sizes:\n",
        "# Generate data\n",
        "X_temp, _ = make_blobs(n_samples=n_samples, centers=3, random_state=42)\n",
        "X_temp_scaled = StandardScaler().fit_transform(X_temp)\n",
        "# Time hierarchical clustering\n",
        "start_time = time.time()\n",
        "hierarchical_temp = cluster.AgglomerativeClustering(n_clusters=3)\n",
        "hierarchical_temp.fit(X_temp_scaled)\n",
        "times_hierarchical.append(time.time() - start_time)\n",
        "# Time K-means\n",
        "start_time = time.time()\n",
        "kmeans_temp = cluster.KMeans(n_clusters=3, random_state=42)\n",
        "kmeans_temp.fit(X_temp_scaled)\n",
        "times_kmeans.append(time.time() - start_time)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(sample_sizes, times_hierarchical, 'o-', label='Hierarchical Clustering', linewidth=2)\n",
        "plt.plot(sample_sizes, times_kmeans, 'o-', label='K-means', linewidth=2)\n",
        "plt.xlabel('Number of Samples')\n",
        "plt.ylabel('Execution Time (seconds)')\n",
        "plt.title('Computational Complexity: Hierarchical vs K-means')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "print(\"âœ… Hierarchical Clustering Analysis Complete!\")"
      ],
      "id": "saOFckVs0ofH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qa0ucyp0ofH"
      },
      "source": [
        "## Hierarchical Clustering Interview Questions & Answers\n",
        "**Q1: Compare and contrast different linkage methods.**\n",
        "**Answer:**\n",
        "- **Single Linkage:** Minimum distance between clusters\n",
        "- Pros: Can detect non-spherical clusters\n",
        "- Cons: Sensitive to noise, creates elongated chains\n",
        "- **Complete Linkage:** Maximum distance between clusters\n",
        "- Pros: Compact, spherical clusters\n",
        "- Cons: Breaks large clusters, sensitive to outliers\n",
        "- **Average Linkage:** Average distance between clusters\n",
        "- Balanced approach, less sensitive to outliers\n",
        "- **Ward's Method:** Minimizes within-cluster variance\n",
        "- Creates similarly sized clusters, most commonly used\n",
        "**Q2: What are the advantages of hierarchical over partitional clustering (like K-means)?**\n",
        "**Answer:**\n",
        "- **No need to specify K:** Dendrogram shows all possible clusterings\n",
        "- **Hierarchical structure:** Shows relationships between clusters\n",
        "- **Deterministic:** Same result each time (unlike K-means with random initialization)\n",
        "- **Visualization:** Dendrogram provides intuitive visualization\n",
        "- **Flexibility:** Can handle clusters of different shapes and sizes\n",
        "**Q3: How do you interpret a dendrogram?**\n",
        "**Answer:**\n",
        "- **Vertical axis:** Distance between merging clusters\n",
        "- **Horizontal axis:** Data points or clusters\n",
        "- **Height of fusion:** Indicates similarity between clusters\n",
        "- **Long vertical lines:** Significant cluster separations\n",
        "- **Short vertical lines:** Similar clusters merging\n",
        "- **Cutting height:** Determines number of clusters\n",
        "**Q4: What is the time and space complexity of hierarchical clustering?**\n",
        "**Answer:**\n",
        "- **Time complexity:** O(nÂ³) for naive implementation, O(nÂ² log n) with optimizations\n",
        "- **Space complexity:** O(nÂ²) for storing distance matrix\n",
        "- **Comparison:**\n",
        "- Hierarchical: Better for small datasets (n < 10,000)\n",
        "- K-means: Better for large datasets\n",
        "- **Bottleneck:** Distance matrix computation and storage\n",
        "**Q5: When should you use hierarchical vs K-means clustering?**\n",
        "**Answer:**\n",
        "**Use Hierarchical when:**\n",
        "- Small to medium datasets (n < 10,000)\n",
        "- Need to determine number of clusters\n",
        "- Want hierarchical relationships\n",
        "- Need deterministic results\n",
        "- Dealing with non-spherical clusters\n",
        "**Use K-means when:**\n",
        "- Large datasets\n",
        "- Know number of clusters in advance\n",
        "- Need fast computation\n",
        "- Spherical clusters expected\n",
        "- Can handle random initialization variations\n",
        "# 11. PRINCIPAL COMPONENT ANALYSIS (PCA)\n",
        "## Algorithm Background & Mathematical Foundation\n",
        "**Core Concept:** PCA is a dimensionality reduction technique that transforms correlated variables into a set of uncorrelated variables (principal components) that capture maximum variance.\n",
        "**Mathematical Formulation:**\n",
        "**Step 1: Standardize the Data**\n",
        "$$X_{\\text{standardized}} = \\frac{X - \\mu}{\\sigma}$$\n",
        "**Step 2: Compute Covariance Matrix**\n",
        "$$\\Sigma = \\frac{1}{n-1} X^T X$$\n",
        "**Step 3: Eigen Decomposition**\n",
        "$$\\Sigma v = \\lambda v$$\n",
        "Where:\n",
        "- $\\lambda$: eigenvalues (variance explained)\n",
        "- $v$: eigenvectors (principal components)\n",
        "**Step 4: Project Data**\n",
        "$$Z = X V$$\n",
        "Where $V$ contains top-k eigenvectors\n",
        "**Variance Explained:**\n",
        "$$\\text{Variance Explained}_k = \\frac{\\lambda_k}{\\sum_{j=1}^p \\lambda_j}$$\n",
        "**Reconstruction:**\n",
        "$$X_{\\text{reconstructed}} = Z V^T$$"
      ],
      "id": "9qa0ucyp0ofH"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDSK8ArL0ofI"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Cell 13: Principal Component Analysis - Comprehensive Implementation\n",
        "print(\"ðŸš€ PRINCIPAL COMPONENT ANALYSIS: COMPREHENSIVE IMPLEMENTATION\\n\")\n",
        "# Load dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "target_names = iris.target_names\n",
        "print(\"ðŸ“Š Dataset Overview:\")\n",
        "print(f\"â€¢ Samples: {X.shape[0]}, Features: {X.shape[1]}\")\n",
        "print(f\"â€¢ Features: {list(feature_names)}\")\n",
        "print(f\"â€¢ Classes: {list(target_names)}\")\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "# Apply PCA\n",
        "pca = decomposition.PCA()\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "print(\"\\nðŸ” PCA Analysis Results:\")\n",
        "print(f\"â€¢ Explained variance ratio: {pca.explained_variance_ratio_}\")\n",
        "print(f\"â€¢ Cumulative explained variance: {np.cumsum(pca.explained_variance_ratio_)}\")\n",
        "print(f\"â€¢ Principal components shape: {X_pca.shape}\")\n",
        "# Comprehensive Visualization\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "# 1. Original Data (first two features)\n",
        "scatter = axes[0,0].scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', alpha=0.8)\n",
        "axes[0,0].set_xlabel(feature_names[0])\n",
        "axes[0,0].set_ylabel(feature_names[1])\n",
        "axes[0,0].set_title('Original Data (First Two Features)')\n",
        "plt.colorbar(scatter, ax=axes[0,0])\n",
        "# 2. PCA Projection (first two components)\n",
        "scatter = axes[0,1].scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', alpha=0.8)\n",
        "axes[0,1].set_xlabel('Principal Component 1')\n",
        "axes[0,1].set_ylabel('Principal Component 2')\n",
        "axes[0,1].set_title('PCA Projection (PC1 vs PC2)')\n",
        "plt.colorbar(scatter, ax=axes[0,1])\n",
        "# 3. Variance Explained\n",
        "components = range(1, len(pca.explained_variance_ratio_) + 1)\n",
        "variance_ratio = pca.explained_variance_ratio_\n",
        "cumulative_variance = np.cumsum(variance_ratio)\n",
        "axes[0,2].bar(components, variance_ratio, alpha=0.6, label='Individual')\n",
        "axes[0,2].plot(components, cumulative_variance, 'ro-', label='Cumulative')\n",
        "axes[0,2].set_xlabel('Principal Components')\n",
        "axes[0,2].set_ylabel('Explained Variance Ratio')\n",
        "axes[0,2].set_title('Explained Variance by Principal Components')\n",
        "axes[0,2].legend()\n",
        "axes[0,2].grid(True)\n",
        "# 4. Component Loadings (Heatmap)\n",
        "loadings = pca.components_.T\n",
        "sns.heatmap(loadings, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
        "xticklabels=[f'PC{i+1}' for i in range(loadings.shape[1])],\n",
        "yticklabels=feature_names, ax=axes[1,0])\n",
        "axes[1,0].set_title('PCA Component Loadings')\n",
        "# 5. 3D PCA Visualization\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "ax = fig.add_subplot(2, 3, 5, projection='3d')\n",
        "scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=y, cmap='viridis', alpha=0.8)\n",
        "ax.set_xlabel('PC1')\n",
        "ax.set_ylabel('PC2')\n",
        "ax.set_zlabel('PC3')\n",
        "ax.set_title('3D PCA Projection')\n",
        "# 6. Reconstruction Error vs Number of Components\n",
        "reconstruction_errors = []\n",
        "n_components_range = range(1, X.shape[1] + 1)\n",
        "for n_comp in n_components_range:\n",
        "pca_temp = decomposition.PCA(n_components=n_comp)\n",
        "X_temp = pca_temp.fit_transform(X_scaled)\n",
        "X_reconstructed = pca_temp.inverse_transform(X_temp)\n",
        "reconstruction_error = np.mean((X_scaled - X_reconstructed) ** 2)\n",
        "reconstruction_errors.append(reconstruction_error)\n",
        "axes[1,2].plot(n_components_range, reconstruction_errors, 'bo-')\n",
        "axes[1,2].set_xlabel('Number of Components')\n",
        "axes[1,2].set_ylabel('Reconstruction Error (MSE)')\n",
        "axes[1,2].set_title('Reconstruction Error vs Components')\n",
        "axes[1,2].grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "# Advanced PCA Analysis\n",
        "print(\"\\nðŸ”§ Advanced PCA Applications:\")\n",
        "# PCA for Dimensionality Reduction in Classification\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# Create pipeline with PCA and classifier\n",
        "pipeline = Pipeline([\n",
        "('scaler', StandardScaler()),\n",
        "('pca', decomposition.PCA(n_components=2)),\n",
        "('classifier', LogisticRegression(random_state=42))\n",
        "])\n",
        "# Compare performance with different number of components\n",
        "n_components_classification = range(1, 5)\n",
        "classification_scores = []\n",
        "for n_comp in n_components_classification:\n",
        "pipeline.set_params(pca__n_components=n_comp)\n",
        "scores = model_selection.cross_val_score(pipeline, X, y, cv=5)\n",
        "classification_scores.append(scores.mean())\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(n_components_classification, classification_scores, 'ro-', linewidth=2)\n",
        "plt.xlabel('Number of Principal Components')\n",
        "plt.ylabel('Classification Accuracy')\n",
        "plt.title('Classification Performance vs Number of PCA Components')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "print(f\"Best number of components for classification: {np.argmax(classification_scores) + 1}\")\n",
        "# Kernel PCA for Non-linear Dimensionality Reduction\n",
        "print(\"\\nðŸŽ¯ Kernel PCA for Non-linear Data:\")\n",
        "# Create non-linear dataset\n",
        "X_nonlinear, y_nonlinear = datasets.make_circles(n_samples=400, noise=0.05, factor=0.3, random_state=42)\n",
        "# Apply different PCA variants\n",
        "pca_variants = {\n",
        "'Standard PCA': decomposition.PCA(n_components=2),\n",
        "'Kernel PCA (RBF)': decomposition.KernelPCA(n_components=2, kernel='rbf'),\n",
        "'Kernel PCA (Poly)': decomposition.KernelPCA(n_components=2, kernel='poly')\n",
        "}\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "# Original data\n",
        "axes[0,0].scatter(X_nonlinear[:, 0], X_nonlinear[:, 1], c=y_nonlinear, cmap='viridis', alpha=0.7)\n",
        "axes[0,0].set_title('Original Non-linear Data')\n",
        "axes[0,0].set_xlabel('Feature 1')\n",
        "axes[0,0].set_ylabel('Feature 2')\n",
        "axes[0,0].grid(True)\n",
        "for i, (name, pca_method) in enumerate(pca_variants.items()):\n",
        "row = (i + 1) // 2\n",
        "col = (i + 1) % 2\n",
        "try:\n",
        "X_transformed = pca_method.fit_transform(X_nonlinear)\n",
        "scatter = axes[row, col].scatter(X_transformed[:, 0], X_transformed[:, 1],\n",
        "c=y_nonlinear, cmap='viridis', alpha=0.7)\n",
        "axes[row, col].set_title(name)\n",
        "axes[row, col].set_xlabel('Component 1')\n",
        "axes[row, col].set_ylabel('Component 2')\n",
        "axes[row, col].grid(True)\n",
        "except Exception as e:\n",
        "print(f\"Error with {name}: {e}\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "# PCA for Image Compression\n",
        "print(\"\\nðŸ–¼ï¸ PCA for Image Compression:\")\n",
        "try:\n",
        "from sklearn.datasets import load_digits\n",
        "digits = load_digits()\n",
        "X_digits = digits.data\n",
        "y_digits = digits.target\n",
        "print(f\"Digits dataset: {X_digits.shape[0]} images, {X_digits.shape[1]} pixels\")\n",
        "# Apply PCA for image compression\n",
        "n_components_image = [10, 25, 50, 64]\n",
        "fig, axes = plt.subplots(2, 4, figsize=(15, 8))\n",
        "for i, n_comp in enumerate(n_components_image):\n",
        "pca_image = decomposition.PCA(n_components=n_comp)\n",
        "X_compressed = pca_image.fit_transform(X_digits)\n",
        "X_reconstructed = pca_image.inverse_transform(X_compressed)\n",
        "# Calculate compression ratio\n",
        "original_size = X_digits.shape[1] * X_digits.shape[0]\n",
        "compressed_size = n_comp * X_digits.shape[0] + n_comp * X_digits.shape[1]\n",
        "compression_ratio = compressed_size / original_size\n",
        "# Show original and reconstructed\n",
        "axes[0, i].imshow(X_digits[0].reshape(8, 8), cmap='gray')\n",
        "axes[0, i].set_title(f'Original Image\\n(64 features)')\n",
        "axes[0, i].axis('off')\n",
        "axes[1, i].imshow(X_reconstructed[0].reshape(8, 8), cmap='gray')\n",
        "axes[1, i].set_title(f'Compressed: {n_comp} components\\nRatio: {compression_ratio:.2f}')\n",
        "axes[1, i].axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "except ImportError:\n",
        "print(\"Digits dataset not available\")\n",
        "# PCA for Anomaly Detection\n",
        "print(\"\\nðŸš¨ PCA for Anomaly Detection:\")\n",
        "# Create dataset with outliers\n",
        "X_clean, _ = datasets.make_blobs(n_samples=190, centers=2, cluster_std=1.0, random_state=42)\n",
        "outliers = np.random.uniform(low=-10, high=10, size=(10, 2))\n",
        "X_with_outliers = np.vstack([X_clean, outliers])\n",
        "y_outliers = np.array([0]*190 + [1]*10) # 1 indicates outlier\n",
        "# Apply PCA\n",
        "pca_outlier = decomposition.PCA(n_components=2)\n",
        "X_pca_outlier = pca_outlier.fit_transform(StandardScaler().fit_transform(X_with_outliers))\n",
        "# Calculate reconstruction error as outlier score\n",
        "pca_recon = decomposition.PCA(n_components=1)\n",
        "X_pca_recon = pca_recon.fit_transform(StandardScaler().fit_transform(X_with_outliers))\n",
        "X_reconstructed = pca_recon.inverse_transform(X_pca_recon)\n",
        "reconstruction_error = np.sum((X_with_outliers - X_reconstructed) ** 2, axis=1)\n",
        "plt.figure(figsize=(15, 5))\n",
        "# Original data with outliers\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.scatter(X_with_outliers[y_outliers==0, 0], X_with_outliers[y_outliers==0, 1],\n",
        "c='blue', alpha=0.6, label='Normal')\n",
        "plt.scatter(X_with_outliers[y_outliers==1, 0], X_with_outliers[y_outliers==1, 1],\n",
        "c='red', alpha=0.8, label='Outlier', marker='x', s=100)\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('Original Data with Outliers')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "# PCA projection\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.scatter(X_pca_outlier[y_outliers==0, 0], X_pca_outlier[y_outliers==0, 1],\n",
        "c='blue', alpha=0.6, label='Normal')\n",
        "plt.scatter(X_pca_outlier[y_outliers==1, 0], X_pca_outlier[y_outliers==1, 1],\n",
        "c='red', alpha=0.8, label='Outlier', marker='x', s=100)\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.title('PCA Projection')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "# Reconstruction error\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.scatter(range(len(reconstruction_error)), reconstruction_error,\n",
        "c=y_outliers, cmap='coolwarm', alpha=0.7)\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('Reconstruction Error')\n",
        "plt.title('PCA Reconstruction Error\\n(Outlier Score)')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "print(\"âœ… Principal Component Analysis Complete!\")"
      ],
      "id": "dDSK8ArL0ofI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HezSH2B0ofI"
      },
      "source": [
        "## PCA Interview Questions & Answers\n",
        "**Q1: What is the intuition behind PCA?**\n",
        "**Answer:**\n",
        "- **Geometric interpretation:** Find directions of maximum variance in data\n",
        "- **Statistical interpretation:** Transform correlated variables into uncorrelated components\n",
        "- **Information preservation:** Keep most information with fewer dimensions\n",
        "- **Noise reduction:** First components capture signal, later components capture noise\n",
        "**Q2: Why do we need to standardize data before PCA?**\n",
        "**Answer:**\n",
        "- **Scale sensitivity:** PCA is sensitive to feature scales\n",
        "- **Variance domination:** Features with larger scales dominate variance\n",
        "- **Fair comparison:** Standardization puts all features on same scale\n",
        "- **Mathematical requirement:** Covariance matrix calculation assumes comparable scales\n",
        "**Q3: How do you interpret principal components?**\n",
        "**Answer:**\n",
        "- **Eigenvalues:** Amount of variance explained by each component\n",
        "- **Eigenvectors:** Directions of principal components in original feature space\n",
        "- **Loadings:** Correlation between original features and principal components\n",
        "- **Interpretation:** PC1 captures most variance, PC2 captures next most, etc.\n",
        "**Q4: What is the difference between PCA and LDA?**\n",
        "**Answer:**\n",
        "- **PCA:** Unsupervised, maximizes variance (ignores class labels)\n",
        "- **LDA:** Supervised, maximizes separation between classes\n",
        "- **Objective:**\n",
        "- PCA: max variance in data\n",
        "- LDA: max ratio of between-class to within-class variance\n",
        "- **Use cases:** PCA for exploration, LDA for classification\n",
        "**Q5: When should you use PCA vs other dimensionality reduction methods?**\n",
        "**Answer:**\n",
        "**Use PCA when:**\n",
        "- Linear relationships in data\n",
        "- Need interpretable components\n",
        "- Want maximum variance preservation\n",
        "- Dealing with continuous numerical data\n",
        "- Need deterministic results\n",
        "**Consider alternatives when:**\n",
        "- Non-linear relationships (use t-SNE, UMAP, Kernel PCA)\n",
        "- Preserving local structure (use t-SNE)\n",
        "- Categorical data (use MCA, Factor Analysis)\n",
        "- Very high-dimensional sparse data (use Truncated SVD)\n",
        "# 12. NEURAL NETWORKS (MLP)\n",
        "## Algorithm Background & Mathematical Foundation\n",
        "**Core Concept:** Neural networks are computational models inspired by biological neurons, capable of learning complex non-linear relationships through multiple layers of interconnected nodes.\n",
        "**Mathematical Formulation:**\n",
        "**Single Neuron:**\n",
        "$$z = w^T x + b$$\n",
        "$$a = \\sigma(z)$$\n",
        "Where:\n",
        "- $w$: weights\n",
        "- $b$: bias\n",
        "- $\\sigma$: activation function\n",
        "**Common Activation Functions:**\n",
        "- **Sigmoid:** $\\sigma(z) = \\frac{1}{1 + e^{-z}}$\n",
        "- **Tanh:** $\\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$\n",
        "- **ReLU:** $\\text{ReLU}(z) = \\max(0, z)$\n",
        "- **Softmax:** $\\sigma(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}}$\n",
        "**Forward Propagation:**\n",
        "For layer $l$:\n",
        "$$z^{[l]} = W^{[l]} a^{[l-1]} + b^{[l]}$$\n",
        "$$a^{[l]} = \\sigma^{[l]}(z^{[l]})$$\n",
        "**Backpropagation:**\n",
        "Compute gradients using chain rule:\n",
        "$$\\frac{\\partial L}{\\partial W^{[l]}} = \\frac{\\partial L}{\\partial a^{[l]}} \\frac{\\partial a^{[l]}}{\\partial z^{[l]}} \\frac{\\partial z^{[l]}}{\\partial W^{[l]}}$$\n",
        "**Loss Functions:**\n",
        "- **MSE:** $L = \\frac{1}{m} \\sum (y - \\hat{y})^2$\n",
        "- **Cross-Entropy:** $L = -\\frac{1}{m} \\sum [y \\log(\\hat{y}) + (1-y) \\log(1-\\hat{y})]$"
      ],
      "id": "-HezSH2B0ofI"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ky6XCWPF0ofI"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Cell 14: Neural Networks (MLP) - Comprehensive Implementation\n",
        "print(\"ðŸš€ NEURAL NETWORKS (MLP): COMPREHENSIVE IMPLEMENTATION\\n\")\n",
        "# Load dataset\n",
        "X, y = datasets.make_classification(n_samples=1000, n_features=20, n_informative=15,\n",
        "n_redundant=5, n_classes=2, random_state=42)\n",
        "print(\"ðŸ“Š Dataset Overview:\")\n",
        "print(f\"â€¢ Samples: {X.shape[0]}, Features: {X.shape[1]}\")\n",
        "print(f\"â€¢ Classes: {len(np.unique(y))}\")\n",
        "print(f\"â€¢ Class distribution: {np.bincount(y)}\")\n",
        "# Split and scale data\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
        "X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "# Build MLP models with different architectures\n",
        "mlp_models = {\n",
        "'MLP (1 hidden layer)': neural_network.MLPClassifier(\n",
        "hidden_layer_sizes=(50,), activation='relu', random_state=42, max_iter=1000\n",
        "),\n",
        "'MLP (2 hidden layers)': neural_network.MLPClassifier(\n",
        "hidden_layer_sizes=(50, 25), activation='relu', random_state=42, max_iter=1000\n",
        "),\n",
        "'MLP (3 hidden layers)': neural_network.MLPClassifier(\n",
        "hidden_layer_sizes=(50, 25, 10), activation='relu', random_state=42, max_iter=1000\n",
        "),\n",
        "'MLP (tanh activation)': neural_network.MLPClassifier(\n",
        "hidden_layer_sizes=(50, 25), activation='tanh', random_state=42, max_iter=1000\n",
        "),\n",
        "'MLP (with regularization)': neural_network.MLPClassifier(\n",
        "hidden_layer_sizes=(50, 25), activation='relu', alpha=0.1, random_state=42, max_iter=1000\n",
        ")\n",
        "}\n",
        "mlp_results = {}\n",
        "for name, model in mlp_models.items():\n",
        "print(f\"\\nðŸ“Š Training {name}...\")\n",
        "model.fit(X_train_scaled, y_train)\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
        "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
        "precision = metrics.precision_score(y_test, y_pred)\n",
        "recall = metrics.recall_score(y_test, y_pred)\n",
        "f1 = metrics.f1_score(y_test, y_pred)\n",
        "roc_auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
        "mlp_results[name] = {\n",
        "'accuracy': accuracy,\n",
        "'precision': precision,\n",
        "'recall': recall,\n",
        "'f1': f1,\n",
        "'roc_auc': roc_auc,\n",
        "'model': model,\n",
        "'loss_curve': model.loss_curve_ if hasattr(model, 'loss_curve_') else None\n",
        "}\n",
        "print(f\" â€¢ Accuracy: {accuracy:.4f}\")\n",
        "print(f\" â€¢ Precision: {precision:.4f}\")\n",
        "print(f\" â€¢ Recall: {recall:.4f}\")\n",
        "print(f\" â€¢ F1-Score: {f1:.4f}\")\n",
        "print(f\" â€¢ ROC-AUC: {roc_auc:.4f}\")\n",
        "print(f\" â€¢ Iterations: {model.n_iter_}\")\n",
        "# Model Comparison\n",
        "comparison_mlp = pd.DataFrame({\n",
        "'Model': list(mlp_results.keys()),\n",
        "'Accuracy': [mlp_results[name]['accuracy'] for name in mlp_results.keys()],\n",
        "'Precision': [mlp_results[name]['precision'] for name in mlp_results.keys()],\n",
        "'Recall': [mlp_results[name]['recall'] for name in mlp_results.keys()],\n",
        "'F1-Score': [mlp_results[name]['f1'] for name in mlp_results.keys()],\n",
        "'ROC-AUC': [mlp_results[name]['roc_auc'] for name in mlp_results.keys()]\n",
        "})\n",
        "print(\"\\nðŸ“‹ Model Performance Comparison:\")\n",
        "print(comparison_mlp.round(4))\n",
        "# Comprehensive Visualization\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "# 1. Loss Curves\n",
        "for name, result in mlp_results.items():\n",
        "if result['loss_curve'] is not None:\n",
        "axes[0,0].plot(result['loss_curve'], label=name)\n",
        "axes[0,0].set_xlabel('Iterations')\n",
        "axes[0,0].set_ylabel('Loss')\n",
        "axes[0,0].set_title('Training Loss Curves')\n",
        "axes[0,0].legend()\n",
        "axes[0,0].grid(True)\n",
        "# 2. Model Comparison\n",
        "models_mlp = list(mlp_results.keys())\n",
        "accuracy_scores = [mlp_results[name]['accuracy'] for name in models_mlp]\n",
        "bars = axes[0,1].bar(models_mlp, accuracy_scores, color=['blue', 'green', 'orange', 'red', 'purple'])\n",
        "axes[0,1].set_ylabel('Accuracy')\n",
        "axes[0,1].set_title('MLP Architectures Comparison')\n",
        "axes[0,1].tick_params(axis='x', rotation=45)\n",
        "for bar, score in zip(bars, accuracy_scores):\n",
        "axes[0,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "f'{score:.3f}', ha='center', va='bottom')\n",
        "# 3. ROC Curves\n",
        "for name, result in mlp_results.items():\n",
        "y_pred_proba = result['model'].predict_proba(X_test_scaled)[:, 1]\n",
        "fpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba)\n",
        "roc_auc = metrics.auc(fpr, tpr)\n",
        "axes[0,2].plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.3f})')\n",
        "axes[0,2].plot([0, 1], [0, 1], 'k--')\n",
        "axes[0,2].set_xlabel('False Positive Rate')\n",
        "axes[0,2].set_ylabel('True Positive Rate')\n",
        "axes[0,2].set_title('ROC Curves Comparison')\n",
        "axes[0,2].legend()\n",
        "axes[0,2].grid(True)\n",
        "# 4. Activation Functions Comparison\n",
        "activations = ['relu', 'tanh', 'logistic']\n",
        "activation_scores = []\n",
        "for activation in activations:\n",
        "mlp_temp = neural_network.MLPClassifier(\n",
        "hidden_layer_sizes=(50, 25), activation=activation, random_state=42, max_iter=1000\n",
        ")\n",
        "mlp_temp.fit(X_train_scaled, y_train)\n",
        "activation_scores.append(mlp_temp.score(X_test_scaled, y_test))\n",
        "axes[1,0].bar(activations, activation_scores, color=['blue', 'green', 'orange'])\n",
        "axes[1,0].set_ylabel('Accuracy')\n",
        "axes[1,0].set_title('Activation Functions Comparison')\n",
        "axes[1,0].set_ylim(0, 1)\n",
        "for i, score in enumerate(activation_scores):\n",
        "axes[1,0].text(i, score + 0.01, f'{score:.3f}', ha='center', va='bottom')\n",
        "# 5. Learning Rate Analysis\n",
        "learning_rates = [0.001, 0.01, 0.1, 0.5]\n",
        "lr_scores = []\n",
        "for lr in learning_rates:\n",
        "mlp_temp = neural_network.MLPClassifier(\n",
        "hidden_layer_sizes=(50, 25), learning_rate_init=lr, random_state=42, max_iter=1000\n",
        ")\n",
        "mlp_temp.fit(X_train_scaled, y_train)\n",
        "lr_scores.append(mlp_temp.score(X_test_scaled, y_test))\n",
        "axes[1,1].semilogx(learning_rates, lr_scores, 'ro-', linewidth=2)\n",
        "axes[1,1].set_xlabel('Learning Rate')\n",
        "axes[1,1].set_ylabel('Accuracy')\n",
        "axes[1,1].set_title('Learning Rate vs Performance')\n",
        "axes[1,1].grid(True)\n",
        "# 6. Regularization Analysis\n",
        "alphas = [0.0001, 0.001, 0.01, 0.1, 1.0]\n",
        "alpha_scores = []\n",
        "for alpha in alphas:\n",
        "mlp_temp = neural_network.MLPClassifier(\n",
        "hidden_layer_sizes=(50, 25), alpha=alpha, random_state=42, max_iter=1000\n",
        ")\n",
        "mlp_temp.fit(X_train_scaled, y_train)\n",
        "alpha_scores.append(mlp_temp.score(X_test_scaled, y_test))\n",
        "axes[1,2].semilogx(alphas, alpha_scores, 'go-', linewidth=2)\n",
        "axes[1,2].set_xlabel('Alpha (Regularization)')\n",
        "axes[1,2].set_ylabel('Accuracy')\n",
        "axes[1,2].set_title('Regularization vs Performance')\n",
        "axes[1,2].grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "# TensorFlow/Keras Implementation\n",
        "print(\"\\nðŸ”¥ TENSORFLOW/KERAS IMPLEMENTATION:\")\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "# Build model\n",
        "tf_model = Sequential([\n",
        "Dense(50, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "BatchNormalization(),\n",
        "Dropout(0.3),\n",
        "Dense(25, activation='relu'),\n",
        "BatchNormalization(),\n",
        "Dropout(0.3),\n",
        "Dense(1, activation='sigmoid')\n",
        "])\n",
        "# Compile model\n",
        "tf_model.compile(\n",
        "optimizer=Adam(learning_rate=0.001),\n",
        "loss='binary_crossentropy',\n",
        "metrics=['accuracy']\n",
        ")\n",
        "print(\"Model Architecture:\")\n",
        "tf_model.summary()\n",
        "# Train model\n",
        "early_stopping = EarlyStopping(patience=10, restore_best_weights=True)\n",
        "history = tf_model.fit(\n",
        "X_train_scaled, y_train,\n",
        "validation_data=(X_test_scaled, y_test),\n",
        "epochs=100,\n",
        "batch_size=32,\n",
        "callbacks=[early_stopping],\n",
        "verbose=0\n",
        ")\n",
        "# Evaluate model\n",
        "y_pred_tf = (tf_model.predict(X_test_scaled) > 0.5).astype(int).flatten()\n",
        "accuracy_tf = metrics.accuracy_score(y_test, y_pred_tf)\n",
        "print(f\"\\nTensorFlow Model Performance:\")\n",
        "print(f\"â€¢ Accuracy: {accuracy_tf:.4f}\")\n",
        "print(f\"â€¢ Training epochs: {len(history.history['loss'])}\")\n",
        "# Plot training history\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "# Loss\n",
        "axes[0].plot(history.history['loss'], label='Training Loss')\n",
        "axes[0].plot(history.history['val_loss'], label='Validation Loss')\n",
        "axes[0].set_xlabel('Epochs')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].set_title('Training and Validation Loss')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True)\n",
        "# Accuracy\n",
        "axes[1].plot(history.history['accuracy'], label='Training Accuracy')\n",
        "axes[1].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "axes[1].set_xlabel('Epochs')\n",
        "axes[1].set_ylabel('Accuracy')\n",
        "axes[1].set_title('Training and Validation Accuracy')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "# Advanced: Neural Network for Regression\n",
        "print(\"\\nðŸ“ˆ NEURAL NETWORKS FOR REGRESSION:\")\n",
        "# Create regression dataset\n",
        "X_reg, y_reg = datasets.make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = model_selection.train_test_split(\n",
        "X_reg, y_reg, test_size=0.2, random_state=42\n",
        ")\n",
        "# Scale features and target\n",
        "scaler_X_reg = StandardScaler()\n",
        "X_train_reg_scaled = scaler_X_reg.fit_transform(X_train_reg)\n",
        "X_test_reg_scaled = scaler_X_reg.transform(X_test_reg)\n",
        "scaler_y_reg = StandardScaler()\n",
        "y_train_reg_scaled = scaler_y_reg.fit_transform(y_train_reg.reshape(-1, 1)).flatten()\n",
        "y_test_reg_scaled = scaler_y_reg.transform(y_test_reg.reshape(-1, 1)).flatten()\n",
        "# Build regression model\n",
        "mlp_reg = neural_network.MLPRegressor(\n",
        "hidden_layer_sizes=(50, 25), activation='relu', random_state=42, max_iter=1000\n",
        ")\n",
        "mlp_reg.fit(X_train_reg_scaled, y_train_reg_scaled)\n",
        "y_pred_reg_scaled = mlp_reg.predict(X_test_reg_scaled)\n",
        "# Convert back to original scale\n",
        "y_pred_reg = scaler_y_reg.inverse_transform(y_pred_reg_scaled.reshape(-1, 1)).flatten()\n",
        "mse = metrics.mean_squared_error(y_test_reg, y_pred_reg)\n",
        "r2 = metrics.r2_score(y_test_reg, y_pred_reg)\n",
        "print(f\"Neural Network Regression Results:\")\n",
        "print(f\"â€¢ MSE: {mse:.4f}\")\n",
        "print(f\"â€¢ RÂ²: {r2:.4f}\")\n",
        "# Plot regression results\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(y_test_reg, y_pred_reg, alpha=0.6)\n",
        "plt.plot([y_test_reg.min(), y_test_reg.max()], [y_test_reg.min(), y_test_reg.max()], 'r--')\n",
        "plt.xlabel('Actual Values')\n",
        "plt.ylabel('Predicted Values')\n",
        "plt.title('Neural Network Regression: Actual vs Predicted')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "# Neural Network Feature Importance\n",
        "print(\"\\nðŸ” NEURAL NETWORK FEATURE IMPORTANCE:\")\n",
        "# Permutation importance\n",
        "from sklearn.inspection import permutation_importance\n",
        "best_mlp_name = max(mlp_results.keys(), key=lambda x: mlp_results[x]['accuracy'])\n",
        "best_mlp_model = mlp_results[best_mlp_name]['model']\n",
        "result = permutation_importance(best_mlp_model, X_test_scaled, y_test, n_repeats=10, random_state=42)\n",
        "# Plot feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "'feature': [f'Feature_{i}' for i in range(X.shape[1])],\n",
        "'importance': result.importances_mean,\n",
        "'std': result.importances_std\n",
        "}).sort_values('importance', ascending=True)\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.barh(feature_importance['feature'][-15:], feature_importance['importance'][-15:],\n",
        "xerr=feature_importance['std'][-15:])\n",
        "plt.xlabel('Permutation Importance')\n",
        "plt.title('Neural Network Feature Importance (Top 15)')\n",
        "plt.grid(True, axis='x')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "print(\"âœ… Neural Networks (MLP) Analysis Complete!\")"
      ],
      "id": "Ky6XCWPF0ofI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WthEqvyE0ofI"
      },
      "source": [
        "## Neural Networks Interview Questions & Answers\n",
        "**Q1: Explain the vanishing/exploding gradient problem.**\n",
        "**Answer:**\n",
        "- **Vanishing gradients:** Gradients become extremely small during backpropagation, stopping learning in early layers\n",
        "- **Exploding gradients:** Gradients become extremely large, causing unstable training\n",
        "- **Causes:** Deep networks, certain activation functions (sigmoid, tanh), improper weight initialization\n",
        "- **Solutions:** ReLU activation, batch normalization, residual connections, proper initialization\n",
        "**Q2: What are the different activation functions and when to use them?**\n",
        "**Answer:**\n",
        "- **ReLU:** $f(x) = \\max(0, x)$\n",
        "- Pros: Prevents vanishing gradient, computationally efficient\n",
        "- Cons: Dying ReLU problem (negative inputs output zero)\n",
        "- **Sigmoid:** $f(x) = \\frac{1}{1 + e^{-x}}$\n",
        "- Pros: Smooth gradient, output range (0,1)\n",
        "- Cons: Vanishing gradient, not zero-centered\n",
        "- **Tanh:** $f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$\n",
        "- Pros: Zero-centered, stronger gradients than sigmoid\n",
        "- Cons: Still can have vanishing gradients\n",
        "- **Leaky ReLU:** $f(x) = \\max(0.01x, x)$\n",
        "- Solves dying ReLU problem\n",
        "**Q3: What is batch normalization and why is it important?**\n",
        "**Answer:**\n",
        "- **Batch normalization:** Normalize layer inputs to have zero mean and unit variance\n",
        "- **Benefits:**\n",
        "- Faster training convergence\n",
        "- Allows higher learning rates\n",
        "- Reduces sensitivity to initialization\n",
        "- Acts as regularizer\n",
        "- **Formula:** $BN(x) = \\gamma \\frac{x - \\mu}{\\sigma} + \\beta$\n",
        "- **Placement:** Usually after linear transformation, before activation\n",
        "**Q4: Explain different optimization algorithms.**\n",
        "**Answer:**\n",
        "- **SGD:** Basic gradient descent with momentum\n",
        "- **Adam:** Adaptive learning rates for each parameter, combines momentum and RMSprop\n",
        "- **RMSprop:** Adapts learning rate based on recent gradient magnitudes\n",
        "- **Adagrad:** Adapts learning rate for each parameter based on historical gradients\n",
        "- **Choice:** Adam is usually good default, SGD with momentum can generalize better\n",
        "**Q5: How do you prevent overfitting in neural networks?**\n",
        "**Answer:**\n",
        "1. **Regularization:** L1/L2 regularization on weights\n",
        "2. **Dropout:** Randomly disable neurons during training\n",
        "3. **Early stopping:** Stop training when validation performance degrades\n",
        "4. **Data augmentation:** Artificially increase training data\n",
        "5. **Batch normalization:** Acts as regularizer\n",
        "6. **Reduce model complexity:** Fewer layers/neurons\n",
        "7. **Weight constraints:** Limit weight magnitudes\n",
        "# 13. CONVOLUTIONAL NEURAL NETWORKS (CNN)\n",
        "## Algorithm Background & Mathematical Foundation\n",
        "**Core Concept:** CNNs are specialized neural networks for processing grid-like data (images, time series) using convolutional layers that preserve spatial relationships.\n",
        "**Key Components:**\n",
        "**Convolution Operation:**\n",
        "$$(f * g)(t) = \\int f(\\tau) g(t - \\tau) d\\tau$$\n",
        "Discrete 2D convolution:\n",
        "$$(I * K)(i,j) = \\sum_{m} \\sum_{n} I(i+m, j+n) K(m, n)$$\n",
        "**Convolutional Layer:**\n",
        "- **Filters/Kernels:** Small matrices that detect features\n",
        "- **Stride:** Step size for filter movement\n",
        "- **Padding:** Adding zeros around input (same/valid padding)\n",
        "**Pooling Layers:**\n",
        "- **Max Pooling:** $ \\text{MaxPool}(x) = \\max(x_{i:i+p, j:j+p}) $\n",
        "- **Average Pooling:** $ \\text{AvgPool}(x) = \\frac{1}{p^2} \\sum x_{i:i+p, j:j+p} $\n",
        "**Architecture Patterns:**\n",
        "- **Feature extraction:** Conv â†’ Activation â†’ Pooling\n",
        "- **Classification:** Flatten â†’ Fully connected â†’ Output"
      ],
      "id": "WthEqvyE0ofI"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "989oYhWV0ofI"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Cell 15: Convolutional Neural Networks - Comprehensive Implementation\n",
        "print(\"ðŸš€ CONVOLUTIONAL NEURAL NETWORKS: COMPREHENSIVE IMPLEMENTATION\\n\")\n",
        "# Load MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "print(\"ðŸ“Š MNIST Dataset Overview:\")\n",
        "print(f\"â€¢ Training samples: {X_train.shape[0]}\")\n",
        "print(f\"â€¢ Test samples: {X_test.shape[0]}\")\n",
        "print(f\"â€¢ Image shape: {X_train.shape[1:]}\")\n",
        "print(f\"â€¢ Classes: {len(np.unique(y_train))}\")\n",
        "print(f\"â€¢ Class distribution: {np.bincount(y_train)}\")\n",
        "# Preprocess data\n",
        "X_train = X_train.astype('float32') / 255.0\n",
        "X_test = X_test.astype('float32') / 255.0\n",
        "# Add channel dimension\n",
        "X_train = X_train.reshape(-1, 28, 28, 1)\n",
        "X_test = X_test.reshape(-1, 28, 28, 1)\n",
        "# Convert labels to categorical\n",
        "y_train_categorical = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test_categorical = tf.keras.utils.to_categorical(y_test, 10)\n",
        "# Visualize sample images\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i in range(10):\n",
        "plt.subplot(2, 5, i+1)\n",
        "plt.imshow(X_train[i].reshape(28, 28), cmap='gray')\n",
        "plt.title(f'Label: {y_train[i]}')\n",
        "plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "# Build CNN models with different architectures\n",
        "def create_simple_cnn():\n",
        "model = Sequential([\n",
        "layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "layers.MaxPooling2D((2, 2)),\n",
        "layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "layers.MaxPooling2D((2, 2)),\n",
        "layers.Flatten(),\n",
        "layers.Dense(128, activation='relu'),\n",
        "layers.Dropout(0.5),\n",
        "layers.Dense(10, activation='softmax')\n",
        "])\n",
        "return model\n",
        "def create_deeper_cnn():\n",
        "model = Sequential([\n",
        "layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "layers.BatchNormalization(),\n",
        "layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "layers.MaxPooling2D((2, 2)),\n",
        "layers.Dropout(0.25),\n",
        "layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "layers.BatchNormalization(),\n",
        "layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "layers.MaxPooling2D((2, 2)),\n",
        "layers.Dropout(0.25),\n",
        "layers.Flatten(),\n",
        "layers.Dense(256, activation='relu'),\n",
        "layers.BatchNormalization(),\n",
        "layers.Dropout(0.5),\n",
        "layers.Dense(10, activation='softmax')\n",
        "])\n",
        "return model\n",
        "# Compile and train models\n",
        "cnn_models = {\n",
        "'Simple CNN': create_simple_cnn(),\n",
        "'Deeper CNN': create_deeper_cnn()\n",
        "}\n",
        "cnn_histories = {}\n",
        "for name, model in cnn_models.items():\n",
        "print(f\"\\nðŸ“Š Training {name}...\")\n",
        "print(\"Model Architecture:\")\n",
        "model.summary()\n",
        "# Compile model\n",
        "model.compile(\n",
        "optimizer='adam',\n",
        "loss='categorical_crossentropy',\n",
        "metrics=['accuracy']\n",
        ")\n",
        "# Train model\n",
        "history = model.fit(\n",
        "X_train, y_train_categorical,\n",
        "validation_data=(X_test, y_test_categorical),\n",
        "epochs=10,\n",
        "batch_size=128,\n",
        "verbose=0\n",
        ")\n",
        "cnn_histories[name] = {\n",
        "'model': model,\n",
        "'history': history\n",
        "}\n",
        "# Evaluate model\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test_categorical, verbose=0)\n",
        "print(f\"â€¢ Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"â€¢ Test Loss: {test_loss:.4f}\")\n",
        "# Comprehensive Visualization\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "# 1. Training History - Accuracy\n",
        "for name, cnn_data in cnn_histories.items():\n",
        "history = cnn_data['history']\n",
        "axes[0,0].plot(history.history['accuracy'], label=f'{name} - Train')\n",
        "axes[0,0].plot(history.history['val_accuracy'], label=f'{name} - Val', linestyle='--')\n",
        "axes[0,0].set_xlabel('Epochs')\n",
        "axes[0,0].set_ylabel('Accuracy')\n",
        "axes[0,0].set_title('Training and Validation Accuracy')\n",
        "axes[0,0].legend()\n",
        "axes[0,0].grid(True)\n",
        "# 2. Training History - Loss\n",
        "for name, cnn_data in cnn_histories.items():\n",
        "history = cnn_data['history']\n",
        "axes[0,1].plot(history.history['loss'], label=f'{name} - Train')\n",
        "axes[0,1].plot(history.history['val_loss'], label=f'{name} - Val', linestyle='--')\n",
        "axes[0,1].set_xlabel('Epochs')\n",
        "axes[0,1].set_ylabel('Loss')\n",
        "axes[0,1].set_title('Training and Validation Loss')\n",
        "axes[0,1].legend()\n",
        "axes[0,1].grid(True)\n",
        "# 3. Model Comparison\n",
        "models_cnn = list(cnn_histories.keys())\n",
        "final_accuracies = [cnn_histories[name]['history'].history['val_accuracy'][-1] for name in models_cnn]\n",
        "bars = axes[0,2].bar(models_cnn, final_accuracies, color=['blue', 'green'])\n",
        "axes[0,2].set_ylabel('Final Validation Accuracy')\n",
        "axes[0,2].set_title('CNN Architectures Comparison')\n",
        "axes[0,2].set_ylim(0.9, 1.0)\n",
        "for bar, accuracy in zip(bars, final_accuracies):\n",
        "axes[0,2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
        "f'{accuracy:.3f}', ha='center', va='bottom')\n",
        "# 4. Feature Maps Visualization\n",
        "best_cnn_name = max(cnn_histories.keys(), key=lambda x: cnn_histories[x]['history'].history['val_accuracy'][-1])\n",
        "best_cnn_model = cnn_histories[best_cnn_name]['model']\n",
        "# Get first convolutional layer\n",
        "first_conv_layer = best_cnn_model.layers[0]\n",
        "# Create feature map model\n",
        "feature_map_model = tf.keras.Model(\n",
        "inputs=best_cnn_model.inputs,\n",
        "outputs=first_conv_layer.output\n",
        ")\n",
        "# Get feature maps for sample image\n",
        "sample_image = X_test[0:1]\n",
        "feature_maps = feature_map_model.predict(sample_image)\n",
        "# Plot feature maps\n",
        "axes[1,0].imshow(X_test[0].reshape(28, 28), cmap='gray')\n",
        "axes[1,0].set_title('Original Image')\n",
        "axes[1,0].axis('off')\n",
        "# Plot first few feature maps\n",
        "for i in range(min(8, feature_maps.shape[-1])):\n",
        "row = 1 + i // 4\n",
        "col = 1 + i % 4\n",
        "if row < 2 and col < 3: # Only plot in available subplots\n",
        "axes[row, col].imshow(feature_maps[0, :, :, i], cmap='viridis')\n",
        "axes[row, col].set_title(f'Feature Map {i+1}')\n",
        "axes[row,1].axis('off')\n",
        "# Remove empty subplots\n",
        "for i in range(6, 9):\n",
        "row = i // 3\n",
        "col = i % 3\n",
        "if row < 2 and col < 3:\n",
        "if not axes[row, col].has_data():\n",
        "axes[row, col].set_visible(False)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "# Advanced CNN Applications\n",
        "print(\"\\nðŸŽ¯ ADVANCED CNN APPLICATIONS:\")\n",
        "# Data Augmentation\n",
        "print(\"\\nðŸ”„ Data Augmentation:\")\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "layers.RandomRotation(0.1),\n",
        "layers.RandomZoom(0.1),\n",
        "layers.RandomContrast(0.1),\n",
        "])\n",
        "# Visualize augmented images\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i in range(5):\n",
        "plt.subplot(2, 5, i+1)\n",
        "plt.imshow(X_train[i].reshape(28, 28), cmap='gray')\n",
        "plt.title('Original')\n",
        "plt.axis('off')\n",
        "plt.subplot(2, 5, i+6)\n",
        "augmented = data_augmentation(tf.expand_dims(X_train[i], 0))\n",
        "plt.imshow(augmented[0].numpy().reshape(28, 28), cmap='gray')\n",
        "plt.title('Augmented')\n",
        "plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "# Transfer Learning Example (using CIFAR-10)\n",
        "print(\"\\nðŸ”„ Transfer Learning with CIFAR-10:\")\n",
        "try:\n",
        "# Load CIFAR-10 dataset\n",
        "(X_train_cifar, y_train_cifar), (X_test_cifar, y_test_cifar) = tf.keras.datasets.cifar10.load_data()\n",
        "# Preprocess data\n",
        "X_train_cifar = X_train_cifar.astype('float32') / 255.0\n",
        "X_test_cifar = X_test_cifar.astype('float32') / 255.0\n",
        "y_train_cifar_categorical = tf.keras.utils.to_categorical(y_train_cifar, 10)\n",
        "y_test_cifar_categorical = tf.keras.utils.to_categorical(y_test_cifar, 10)\n",
        "print(f\"CIFAR-10 Dataset: {X_train_cifar.shape[0]} training images, {X_test_cifar.shape[0]} test images\")\n",
        "# Build transfer learning model\n",
        "base_model = tf.keras.applications.MobileNetV2(\n",
        "input_shape=(32, 32, 3),\n",
        "include_top=False,\n",
        "weights='imagenet'\n",
        ")\n",
        "base_model.trainable = False # Freeze base model\n",
        "transfer_model = Sequential([\n",
        "base_model,\n",
        "layers.GlobalAveragePooling2D(),\n",
        "layers.Dense(128, activation='relu'),\n",
        "layers.Dropout(0.5),\n",
        "layers.Dense(10, activation='softmax')\n",
        "])\n",
        "transfer_model.compile(\n",
        "optimizer='adam',\n",
        "loss='categorical_crossentropy',\n",
        "metrics=['accuracy']\n",
        ")\n",
        "# Resize CIFAR-10 images to match MobileNet input (not ideal but for demonstration)\n",
        "# In practice, you'd use a model trained on smaller images\n",
        "print(\"Note: For proper transfer learning, use models trained on appropriate input sizes\")\n",
        "except Exception as e:\n",
        "print(f\"Transfer learning demonstration skipped: {e}\")\n",
        "# CNN for Different Filter Sizes\n",
        "print(\"\\nðŸ” CNN Filter Size Analysis:\")\n",
        "filter_sizes = [(2, 2), (3, 3), (5, 5)]\n",
        "filter_results = {}\n",
        "for filter_size in filter_sizes:\n",
        "model = Sequential([\n",
        "layers.Conv2D(32, filter_size, activation='relu', input_shape=(28, 28, 1)),\n",
        "layers.MaxPooling2D((2, 2)),\n",
        "layers.Flatten(),\n",
        "layers.Dense(128, activation='relu'),\n",
        "layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(\n",
        "optimizer='adam',\n",
        "loss='categorical_crossentropy',\n",
        "metrics=['accuracy']\n",
        ")\n",
        "history = model.fit(\n",
        "X_train, y_train_categorical,\n",
        "validation_data=(X_test, y_test_categorical),\n",
        "epochs=5,\n",
        "batch_size=128,\n",
        "verbose=0\n",
        ")\n",
        "final_accuracy = history.history['val_accuracy'][-1]\n",
        "filter_results[filter_size] = final_accuracy\n",
        "print(f\"Filter size {filter_size}: Validation Accuracy = {final_accuracy:.4f}\")\n",
        "# Misclassification Analysis\n",
        "print(\"\\nðŸ” Misclassification Analysis:\")\n",
        "best_model = cnn_histories[best_cnn_name]['model']\n",
        "y_pred_proba = best_model.predict(X_test)\n",
        "y_pred = np.argmax(y_pred_proba, axis=1)\n",
        "# Find misclassified examples\n",
        "misclassified_idx = np.where(y_pred != y_test)[0]\n",
        "if len(misclassified_idx) > 0:\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i in range(min(8, len(misclassified_idx))):\n",
        "idx = misclassified_idx[i]\n",
        "plt.subplot(2, 4, i+1)\n",
        "plt.imshow(X_test[idx].reshape(28, 28), cmap='gray')\n",
        "plt.title(f'True: {y_test[idx]}, Pred: {y_pred[idx]}')\n",
        "plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "print(f\"Misclassification rate: {len(misclassified_idx)/len(y_test):.4f}\")\n",
        "# CNN for Object Detection (Conceptual)\n",
        "print(\"\\nðŸŽ¯ CNN for Object Detection (Conceptual):\")\n",
        "# Demonstrate different pooling strategies\n",
        "sample_image = X_train[0]\n",
        "# Max Pooling\n",
        "max_pooled = tf.keras.layers.MaxPool2D(pool_size=(2, 2))(tf.expand_dims(sample_image, 0))\n",
        "# Average Pooling\n",
        "avg_pooled = tf.keras.layers.AveragePooling2D(pool_size=(2, 2))(tf.expand_dims(sample_image, 0))\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.imshow(sample_image.reshape(28, 28), cmap='gray')\n",
        "plt.title('Original Image')\n",
        "plt.axis('off')\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.imshow(max_pooled[0, :, :, 0], cmap='gray')\n",
        "plt.title('Max Pooling')\n",
        "plt.axis('off')\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.imshow(avg_pooled[0, :, :, 0], cmap='gray')\n",
        "plt.title('Average Pooling')\n",
        "plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "print(\"âœ… Convolutional Neural Networks Analysis Complete!\")"
      ],
      "id": "989oYhWV0ofI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8MnRiy30ofJ"
      },
      "source": [
        "## CNN Interview Questions & Answers\n",
        "**Q1: What is the difference between CNN and fully connected networks?**\n",
        "**Answer:**\n",
        "- **Parameter sharing:** CNN shares weights across spatial locations, FC has separate weights for each connection\n",
        "- **Sparse connectivity:** CNN neurons connect only to local regions, FC connects to all inputs\n",
        "- **Translation invariance:** CNN can detect features regardless of position\n",
        "- **Spatial hierarchy:** CNN learns hierarchical features (edges â†’ patterns â†’ objects)\n",
        "**Q2: Explain the concept of receptive field.**\n",
        "**Answer:**\n",
        "- **Receptive field:** Region in input space that affects a particular neuron\n",
        "- **Local receptive field:** Each neuron connects only to a small region of input\n",
        "- **Increasing receptive field:** Deeper layers have larger receptive fields through:\n",
        "- Larger filter sizes\n",
        "- Pooling layers\n",
        "- Stacking convolutional layers\n",
        "- **Importance:** Allows network to combine local features into more global patterns\n",
        "**Q3: What are the benefits of pooling layers?**\n",
        "**Answer:**\n",
        "- **Dimensionality reduction:** Reduce spatial size, decrease parameters\n",
        "- **Translation invariance:** Small translations don't affect output\n",
        "- **Prevent overfitting:** Reduce model complexity\n",
        "- **Computational efficiency:** Fewer parameters to learn\n",
        "- **Feature robustness:** Makes features more invariant to small variations\n",
        "**Q4: Compare different CNN architectures.**\n",
        "**Answer:**\n",
        "- **LeNet-5:** Early CNN for digit recognition, basic conv-pool structure\n",
        "- **AlexNet:** Deeper network, ReLU activation, dropout\n",
        "- **VGG:** Very deep with small 3x3 filters, uniform architecture\n",
        "- **ResNet:** Residual connections solve vanishing gradient in very deep networks\n",
        "- **Inception:** Multiple filter sizes in parallel, efficient computation\n",
        "**Q5: How do you handle overfitting in CNNs?**\n",
        "**Answer:**\n",
        "1. **Data augmentation:** Rotation, scaling, flipping, color changes\n",
        "2. **Dropout:** Randomly disable neurons during training\n",
        "3. **Batch normalization:** Normalize layer inputs\n",
        "4. **Early stopping:** Stop when validation performance plateaus\n",
        "5. **Weight regularization:** L1/L2 regularization on weights\n",
        "6. **Reduce model complexity:** Fewer layers/filters\n",
        "7. **Transfer learning:** Use pre-trained models\n",
        "# 14. RECURRENT NEURAL NETWORKS (RNN)\n",
        "## Algorithm Background & Mathematical Foundation\n",
        "**Core Concept:** RNNs are designed for sequential data by maintaining a hidden state that captures information about previous elements in the sequence.\n",
        "**Mathematical Formulation:**\n",
        "**Basic RNN:**\n",
        "- **Hidden state update:** $h_t = \\sigma(W_{hh}h_{t-1} + W_{xh}x_t + b_h)$\n",
        "- **Output:** $y_t = \\sigma(W_{hy}h_t + b_y)$\n",
        "Where:\n",
        "- $h_t$: hidden state at time t\n",
        "- $x_t$: input at time t\n",
        "- $y_t$: output at time t\n",
        "- $W$: weight matrices\n",
        "- $b$: bias vectors\n",
        "- $\\sigma$: activation function\n",
        "**Vanishing Gradient Problem:**\n",
        "- Gradients become exponentially small through time steps\n",
        "- Limits learning of long-range dependencies\n",
        "**LSTM (Long Short-Term Memory):**\n",
        "- **Forget gate:** $f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$\n",
        "- **Input gate:** $i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$\n",
        "- **Output gate:** $o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$\n",
        "- **Cell state:** $C_t = f_t \\odot C_{t-1} + i_t \\odot \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)$\n",
        "- **Hidden state:** $h_t = o_t \\odot \\tanh(C_t)$\n",
        "**GRU (Gated Recurrent Unit):**\n",
        "Simplified version with update and reset gates"
      ],
      "id": "d8MnRiy30ofJ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpC-PDIR0ofJ"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Cell 16: Recurrent Neural Networks - Comprehensive Implementation\n",
        "print(\"ðŸš€ RECURRENT NEURAL NETWORKS: COMPREHENSIVE IMPLEMENTATION\\n\")\n",
        "# Create synthetic time series data\n",
        "def generate_time_series_data(n_samples=1000, seq_length=50):\n",
        "\"\"\"Generate synthetic time series data with multiple patterns\"\"\"\n",
        "time = np.linspace(0, 100, seq_length)\n",
        "data = []\n",
        "for i in range(n_samples):\n",
        "# Combine multiple sine waves with different frequencies\n",
        "signal = (np.sin(0.1 * time + i * 0.01) +\n",
        "0.5 * np.sin(0.3 * time + i * 0.02) +\n",
        "0.3 * np.sin(0.7 * time + i * 0.03) +\n",
        "np.random.normal(0, 0.1, seq_length))\n",
        "data.append(signal)\n",
        "return np.array(data)\n",
        "# Generate data\n",
        "X_ts = generate_time_series_data(1000, 50)\n",
        "print(\"ðŸ“Š Time Series Data Overview:\")\n",
        "print(f\"â€¢ Samples: {X_ts.shape[0]}\")\n",
        "print(f\"â€¢ Sequence length: {X_ts.shape[1]}\")\n",
        "print(f\"â€¢ Data range: [{X_ts.min():.2f}, {X_ts.max():.2f}]\")\n",
        "# Create sequences for prediction (predict next value from previous 10)\n",
        "def create_sequences(data, seq_length=10):\n",
        "X, y = [], []\n",
        "for i in range(len(data) - seq_length):\n",
        "X.append(data[i:(i + seq_length)])\n",
        "y.append(data[i + seq_length])\n",
        "return np.array(X), np.array(y)\n",
        "seq_length = 10\n",
        "X_seq, y_seq = create_sequences(X_ts, seq_length)\n",
        "# Reshape for RNN (samples, time steps, features)\n",
        "X_seq = X_seq.reshape(X_seq.shape[0], X_seq.shape[1], 1)\n",
        "y_seq = y_seq.reshape(-1, 1)\n",
        "print(f\"â€¢ Sequence data shape: {X_seq.shape}\")\n",
        "print(f\"â€¢ Target shape: {y_seq.shape}\")\n",
        "# Split data\n",
        "X_train_seq, X_test_seq, y_train_seq, y_test_seq = model_selection.train_test_split(\n",
        "X_seq, y_seq, test_size=0.2, random_state=42\n",
        ")\n",
        "# Build different RNN architectures\n",
        "def create_simple_rnn():\n",
        "model = Sequential([\n",
        "layers.SimpleRNN(50, activation='relu', input_shape=(seq_length, 1)),\n",
        "layers.Dense(1)\n",
        "])\n",
        "return model\n",
        "def create_lstm_model():\n",
        "model = Sequential([\n",
        "layers.LSTM(50, activation='relu', input_shape=(seq_length, 1)),\n",
        "layers.Dense(1)\n",
        "])\n",
        "return model\n",
        "def create_gru_model():\n",
        "model = Sequential([\n",
        "layers.GRU(50, activation='relu', input_shape=(seq_length, 1)),\n",
        "layers.Dense(1)\n",
        "])\n",
        "return model\n",
        "def create_deep_rnn():\n",
        "model = Sequential([\n",
        "layers.LSTM(50, activation='relu', return_sequences=True, input_shape=(seq_length, 1)),\n",
        "layers.Dropout(0.2),\n",
        "layers.LSTM(25, activation='relu'),\n",
        "layers.Dropout(0.2),\n",
        "layers.Dense(1)\n",
        "])\n",
        "return model\n",
        "# Compile and train models\n",
        "rnn_models = {\n",
        "'Simple RNN': create_simple_rnn(),\n",
        "'LSTM': create_lstm_model(),\n",
        "'GRU': create_gru_model(),\n",
        "'Deep LSTM': create_deep_rnn()\n",
        "}\n",
        "rnn_histories = {}\n",
        "for name, model in rnn_models.items():\n",
        "print(f\"\\nðŸ“Š Training {name}...\")\n",
        "# Compile model\n",
        "model.compile(\n",
        "optimizer='adam',\n",
        "loss='mse',\n",
        "metrics=['mae']\n",
        ")\n",
        "# Train model\n",
        "history = model.fit(\n",
        "X_train_seq, y_train_seq,\n",
        "validation_data=(X_test_seq, y_test_seq),\n",
        "epochs=20,\n",
        "batch_size=32,\n",
        "verbose=0\n",
        ")\n",
        "rnn_histories[name] = {\n",
        "'model': model,\n",
        "'history': history\n",
        "}\n",
        "# Evaluate model\n",
        "test_loss, test_mae = model.evaluate(X_test_seq, y_test_seq, verbose=0)\n",
        "print(f\"â€¢ Test MSE: {test_loss:.4f}\")\n",
        "print(f\"â€¢ Test MAE: {test_mae:.4f}\")\n",
        "# Comprehensive Visualization\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "# 1. Training History - Loss\n",
        "for name, rnn_data in rnn_histories.items():\n",
        "history = rnn_data['history']\n",
        "axes[0,0].plot(history.history['loss'], label=f'{name} - Train')\n",
        "axes[0,0].plot(history.history['val_loss'], label=f'{name} - Val', linestyle='--')\n",
        "axes[0,0].set_xlabel('Epochs')\n",
        "axes[0,0].set_ylabel('MSE Loss')\n",
        "axes[0,0].set_title('Training and Validation Loss')\n",
        "axes[0,0].legend()\n",
        "axes[0,0].grid(True)\n",
        "# 2. Model Comparison\n",
        "models_rnn = list(rnn_histories.keys())\n",
        "final_losses = [rnn_histories[name]['history'].history['val_loss'][-1] for name in models_rnn]\n",
        "bars = axes[0,1].bar(models_rnn, final_losses, color=['blue', 'green', 'orange', 'red'])\n",
        "axes[0,1].set_ylabel('Final Validation MSE')\n",
        "axes[0,1].set_title('RNN Architectures Comparison')\n",
        "axes[0,1].tick_params(axis='x', rotation=45)\n",
        "for bar, loss in zip(bars, final_losses):\n",
        "axes[0,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
        "f'{loss:.3f}', ha='center', va='bottom')\n",
        "# 3. Prediction Visualization\n",
        "best_rnn_name = min(rnn_histories.keys(), key=lambda x: rnn_histories[x]['history'].history['val_loss'][-1])\n",
        "best_rnn_model = rnn_histories[best_rnn_name]['model']\n",
        "# Make predictions\n",
        "y_pred_seq = best_rnn_model.predict(X_test_seq)\n",
        "# Plot predictions vs actual for first few samples\n",
        "for i in range(3):\n",
        "axes[0,2].plot(y_test_seq[i], 'b-', alpha=0.7, label='Actual' if i == 0 else \"\")\n",
        "axes[0,2].plot(y_pred_seq[i], 'r--', alpha=0.7, label='Predicted' if i == 0 else \"\")\n",
        "axes[0,2].set_xlabel('Time Step')\n",
        "axes[0,2].set_ylabel('Value')\n",
        "axes[0,2].set_title(f'Predictions vs Actual ({best_rnn_name})')\n",
        "axes[0,2].legend()\n",
        "axes[0,2].grid(True)\n",
        "# 4. Multi-step Prediction\n",
        "def multi_step_prediction(model, initial_sequence, steps=20):\n",
        "\"\"\"Generate multi-step predictions\"\"\"\n",
        "current_sequence = initial_sequence.copy()\n",
        "predictions = []\n",
        "for _ in range(steps):\n",
        "# Predict next value\n",
        "next_pred = model.predict(current_sequence.reshape(1, seq_length, 1), verbose=0)[0, 0]\n",
        "predictions.append(next_pred)\n",
        "# Update sequence (remove first, add prediction)\n",
        "current_sequence = np.roll(current_sequence, -1)\n",
        "current_sequence[-1] = next_pred\n",
        "return np.array(predictions)\n",
        "# Test multi-step prediction\n",
        "initial_seq = X_test_seq[0]\n",
        "true_future = y_test_seq[0:20].flatten()\n",
        "pred_future = multi_step_prediction(best_rnn_model, initial_seq.flatten(), steps=20)\n",
        "axes[1,0].plot(range(len(initial_seq)), initial_seq.flatten(), 'g-', label='Input Sequence')\n",
        "axes[1,0].plot(range(len(initial_seq), len(initial_seq) + len(true_future)), true_future, 'b-', label='True Future')\n",
        "axes[1,0].plot(range(len(initial_seq), len(initial_seq) + len(pred_future)), pred_future, 'r--', label='Predicted Future')\n",
        "axes[1,0].set_xlabel('Time Step')\n",
        "axes[1,0].set_ylabel('Value')\n",
        "axes[1,0].set_title('Multi-step Prediction')\n",
        "axes[1,0].legend()\n",
        "axes[1,0].grid(True)\n",
        "# 5. Sequence Length Analysis\n",
        "sequence_lengths = [5, 10, 15, 20]\n",
        "seq_length_results = {}\n",
        "for seq_len in sequence_lengths:\n",
        "X_temp, y_temp = create_sequences(X_ts, seq_len)\n",
        "X_temp = X_temp.reshape(X_temp.shape[0], X_temp.shape[1], 1)\n",
        "X_train_temp, X_test_temp, y_train_temp, y_test_temp = model_selection.train_test_split(\n",
        "X_temp, y_temp, test_size=0.2, random_state=42\n",
        ")\n",
        "model_temp = create_lstm_model()\n",
        "model_temp.compile(optimizer='adam', loss='mse')\n",
        "history_temp = model_temp.fit(\n",
        "X_train_temp, y_train_temp,\n",
        "validation_data=(X_test_temp, y_test_temp),\n",
        "epochs=10,\n",
        "batch_size=32,\n",
        "verbose=0\n",
        ")\n",
        "final_loss = history_temp.history['val_loss'][-1]\n",
        "seq_length_results[seq_len] = final_loss\n",
        "axes[1,1].plot(sequence_lengths, [seq_length_results[sl] for sl in sequence_lengths], 'bo-')\n",
        "axes[1,1].set_xlabel('Sequence Length')\n",
        "axes[1,1].set_ylabel('Validation MSE')\n",
        "axes[1,1].set_title('Sequence Length vs Performance')\n",
        "axes[1,1].grid(True)\n",
        "# 6. Different Activation Functions\n",
        "activations = ['relu', 'tanh', 'sigmoid']\n",
        "activation_results = {}\n",
        "for activation in activations:\n",
        "model_temp = Sequential([\n",
        "layers.LSTM(50, activation=activation, input_shape=(seq_length, 1)),\n",
        "layers.Dense(1)\n",
        "])\n",
        "model_temp.compile(optimizer='adam', loss='mse')\n",
        "history_temp = model_temp.fit(\n",
        "X_train_seq, y_train_seq,\n",
        "validation_data=(X_test_seq, y_test_seq),\n",
        "epochs=10,\n",
        "batch_size=32,\n",
        "verbose=0\n",
        ")\n",
        "final_loss = history_temp.history['val_loss'][-1]\n",
        "activation_results[activation] = final_loss\n",
        "bars = axes[1,2].bar(activations, [activation_results[act] for act in activations],\n",
        "color=['blue', 'green', 'orange'])\n",
        "axes[1,2].set_ylabel('Validation MSE')\n",
        "axes[1,2].set_title('Activation Functions Comparison')\n",
        "for bar, loss in zip(bars, [activation_results[act] for act in activations]):\n",
        "axes[1,2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
        "f'{loss:.3f}', ha='center', va='bottom')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "# Text Generation Example\n",
        "print(\"\\nðŸ“ RNN for Text Generation:\")\n",
        "# Simple character-level text generation example\n",
        "text = \"\"\"Machine learning is a subset of artificial intelligence that provides systems the ability to automatically learn and improve from experience without being explicitly programmed.\"\"\"\n",
        "text = text.lower()\n",
        "# Create character mapping\n",
        "chars = sorted(list(set(text)))\n",
        "char_to_idx = {char: idx for idx, char in enumerate(chars)}\n",
        "idx_to_char = {idx: char for idx, char in enumerate(chars)}\n",
        "print(f\"â€¢ Unique characters: {len(chars)}\")\n",
        "print(f\"â€¢ Text length: {len(text)}\")\n",
        "# Prepare sequences for training\n",
        "max_sequence_length = 40\n",
        "step = 3\n",
        "sequences = []\n",
        "next_chars = []\n",
        "for i in range(0, len(text) - max_sequence_length, step):\n",
        "sequences.append(text[i:i + max_sequence_length])\n",
        "next_chars.append(text[i + max_sequence_length])\n",
        "print(f\"â€¢ Number of sequences: {len(sequences)}\")\n",
        "# Vectorize sequences\n",
        "X_text = np.zeros((len(sequences), max_sequence_length, len(chars)), dtype=bool)\n",
        "y_text = np.zeros((len(sequences), len(chars)), dtype=bool)\n",
        "for i, sequence in enumerate(sequences):\n",
        "for t, char in enumerate(sequence):\n",
        "X_text[i, t, char_to_idx[char]] = 1\n",
        "y_text[i, char_to_idx[next_chars[i]]] = 1\n",
        "# Build character-level RNN\n",
        "text_model = Sequential([\n",
        "layers.LSTM(128, input_shape=(max_sequence_length, len(chars))),\n",
        "layers.Dense(len(chars), activation='softmax')\n",
        "])\n",
        "text_model.compile(\n",
        "optimizer='adam',\n",
        "loss='categorical_crossentropy',\n",
        "metrics=['accuracy']\n",
        ")\n",
        "# Train for a few epochs\n",
        "history_text = text_model.fit(\n",
        "X_text, y_text,\n",
        "batch_size=128,\n",
        "epochs=50,\n",
        "verbose=0\n",
        ")\n",
        "print(f\"Text model final accuracy: {history_text.history['accuracy'][-1]:.4f}\")\n",
        "# Generate text function\n",
        "def generate_text(model, seed_text, length=100, temperature=1.0):\n",
        "generated = seed_text\n",
        "for _ in range(length):\n",
        "# Prepare input\n",
        "x = np.zeros((1, max_sequence_length, len(chars)))\n",
        "for t, char in enumerate(seed_text):\n",
        "if char in char_to_idx:\n",
        "x[0, t, char_to_idx[char]] = 1\n",
        "# Predict next character\n",
        "preds = model.predict(x, verbose=0)[0]\n",
        "# Apply temperature\n",
        "preds = np.log(preds) / temperature\n",
        "exp_preds = np.exp(preds)\n",
        "preds = exp_preds / np.sum(exp_preds)\n",
        "# Sample next character\n",
        "next_idx = np.random.choice(len(chars), p=preds)\n",
        "next_char = idx_to_char[next_idx]\n",
        "generated += next_char\n",
        "seed_text = seed_text[1:] + next_char\n",
        "return generated\n",
        "# Generate some text\n",
        "seed = \"machine learning is\"\n",
        "generated_text = generate_text(text_model, seed, length=100, temperature=0.5)\n",
        "print(f\"\\nGenerated text:\")\n",
        "print(f\"Seed: '{seed}'\")\n",
        "print(f\"Generated: '{generated_text}'\\n\")\n",
        "# Advanced: Bidirectional RNN\n",
        "print(\"\\nðŸ”„ Bidirectional RNN:\")\n",
        "def create_bidirectional_lstm():\n",
        "model = Sequential([\n",
        "layers.Bidirectional(layers.LSTM(25, activation='relu'), input_shape=(seq_length, 1)),\n",
        "layers.Dense(1)\n",
        "])\n",
        "return model\n",
        "bidirectional_model = create_bidirectional_lstm()\n",
        "bidirectional_model.compile(optimizer='adam', loss='mse')\n",
        "history_bi = bidirectional_model.fit(\n",
        "X_train_seq, y_train_seq,\n",
        "validation_data=(X_test_seq, y_test_seq),\n",
        "epochs=20,\n",
        "batch_size=32,\n",
        "verbose=0\n",
        ")\n",
        "bi_loss = history_bi.history['val_loss'][-1]\n",
        "best_loss = rnn_histories[best_rnn_name]['history'].history['val_loss'][-1]\n",
        "print(f\"Bidirectional LSTM Validation MSE: {bi_loss:.4f}\")\n",
        "print(f\"Best regular LSTM Validation MSE: {best_loss:.4f}\")\n",
        "print(f\"Improvement: {((best_loss - bi_loss) / best_loss * 100):.1f}%\")\n",
        "# Attention Mechanism Concept\n",
        "print(\"\\nðŸŽ¯ Attention Mechanism (Conceptual):\")\n",
        "# Demonstrate the concept of attention weights\n",
        "def simple_attention(query, keys, values):\n",
        "\"\"\"Simple attention mechanism demonstration\"\"\"\n",
        "# Calculate attention scores\n",
        "scores = np.dot(keys, query)\n",
        "# Apply softmax\n",
        "attention_weights = np.exp(scores) / np.sum(np.exp(scores))\n",
        "# Weighted sum of values\n",
        "context_vector = np.dot(attention_weights, values)\n",
        "return context_vector, attention_weights\n",
        "# Example usage\n",
        "query = np.array([0.5, 0.3])\n",
        "keys = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])\n",
        "values = np.array([[1, 2], [3, 4], [5, 6]])\n",
        "context, weights = simple_attention(query, keys, values)\n",
        "print(f\"Attention weights: {weights}\")\n",
        "print(f\"Context vector: {context}\")\n",
        "print(\"âœ… Recurrent Neural Networks Analysis Complete!\")"
      ],
      "id": "WpC-PDIR0ofJ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vW6JKQtR0ofJ"
      },
      "source": [
        "## RNN Interview Questions & Answers\n",
        "**Q1: What is the vanishing/exploding gradient problem in RNNs?**\n",
        "**Answer:**\n",
        "- **Vanishing gradients:** Gradients become extremely small when backpropagating through many time steps\n",
        "- **Exploding gradients:** Gradients become extremely large, causing numerical instability\n",
        "- **Cause:** Repeated multiplication of the same weight matrix through time\n",
        "- **Impact:** Difficulty learning long-range dependencies\n",
        "- **Solutions:** LSTM/GRU architectures, gradient clipping, proper initialization\n",
        "**Q2: Compare LSTM and GRU architectures.**\n",
        "**Answer:**\n",
        "- **LSTM:** Three gates (input, forget, output), separate cell state\n",
        "- **GRU:** Two gates (update, reset), merged cell and hidden state\n",
        "- **Complexity:** LSTM has more parameters, GRU is computationally lighter\n",
        "- **Performance:** Similar performance in many tasks, GRU often faster to train\n",
        "- **Use cases:** LSTM for very long sequences, GRU for efficiency\n",
        "**Q3: What are bidirectional RNNs and when are they useful?**\n",
        "**Answer:**\n",
        "- **Bidirectional RNN:** Process sequence in both forward and backward directions\n",
        "- **Architecture:** Two separate hidden layers, one for each direction\n",
        "- **Benefits:** Access to both past and future context for each time step\n",
        "- **Use cases:**\n",
        "- Natural language processing (understanding context)\n",
        "- Speech recognition\n",
        "- Time series analysis with clear context\n",
        "- **Limitations:** Cannot be used for real-time prediction\n",
        "**Q4: How do you handle variable-length sequences in RNNs?**\n",
        "**Answer:**\n",
        "1. **Padding:** Add zeros to make sequences same length\n",
        "2. **Masking:** Ignore padded positions during computation\n",
        "3. **Dynamic RNNs:** Handle variable lengths natively (TensorFlow)\n",
        "4. **Bucketting:** Group sequences by similar lengths\n",
        "5. **Truncation:** Cut sequences to fixed maximum length\n",
        "**Q5: What is teacher forcing in RNN training?**\n",
        "**Answer:**\n",
        "- **Teacher forcing:** Use actual previous output instead of predicted output during training\n",
        "- **Benefits:** Faster convergence, more stable training\n",
        "- **Drawbacks:** Discrepancy between training and inference\n",
        "- **Scheduled sampling:** Gradually reduce teacher forcing during training\n",
        "- **Use cases:** Sequence generation, machine translation\n",
        "# 15. TRANSFORMERS (BERT, GPT)\n",
        "## Algorithm Background & Mathematical Foundation\n",
        "**Core Concept:** Transformers use self-attention mechanisms to process sequences in parallel, capturing global dependencies without recurrence.\n",
        "**Key Components:**\n",
        "**Self-Attention Mechanism:**\n",
        "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
        "Where:\n",
        "- $Q$: Query matrix (what I'm looking for)\n",
        "- $K$: Key matrix (what I can offer)\n",
        "- $V$: Value matrix (what I actually contain)\n",
        "- $d_k$: Dimension of key vectors\n",
        "**Multi-Head Attention:**\n",
        "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$$\n",
        "$$\\text{where head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$\n",
        "**Positional Encoding:**\n",
        "$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)$$\n",
        "$$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)$$\n",
        "**Transformer Architecture:**\n",
        "- Encoder: Multi-head attention â†’ Feed forward â†’ Layer normalization\n",
        "- Decoder: Masked multi-head attention â†’ Encoder-decoder attention â†’ Feed forward"
      ],
      "id": "vW6JKQtR0ofJ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqbF4bet0ofJ"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Cell 17: Transformers - Comprehensive Implementation\n",
        "print(\"ðŸš€ TRANSFORMERS: COMPREHENSIVE IMPLEMENTATION\\n\")\n",
        "# We'll use a simplified implementation to demonstrate transformer concepts\n",
        "import math\n",
        "class SimpleTransformer:\n",
        "\"\"\"Simplified transformer implementation for educational purposes\"\"\"\n",
        "def __init__(self, vocab_size, d_model=64, n_heads=4, ff_dim=128, max_len=100):\n",
        "self.vocab_size = vocab_size\n",
        "self.d_model = d_model\n",
        "self.n_heads = n_heads\n",
        "self.ff_dim = ff_dim\n",
        "self.max_len = max_len\n",
        "def positional_encoding(self, position, d_model):\n",
        "\"\"\"Generate positional encoding\"\"\"\n",
        "angle_rates = 1 / np.power(10000, (2 * (np.arange(d_model)[np.newaxis, :] // 2)) / np.float32(d_model))\n",
        "angle_rads = position[:, np.newaxis] * angle_rates\n",
        "# Apply sin to even indices, cos to odd indices\n",
        "angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "return angle_rads\n",
        "def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "\"\"\"Calculate scaled dot-product attention\"\"\"\n",
        "d_k = K.shape[-1]\n",
        "scores = np.matmul(Q, K.transpose(0, 2, 1)) / np.sqrt(d_k)\n",
        "if mask is not None:\n",
        "scores += (mask * -1e9)\n",
        "attention_weights = self.softmax(scores)\n",
        "output = np.matmul(attention_weights, V)\n",
        "return output, attention_weights\n",
        "def softmax(self, x):\n",
        "\"\"\"Softmax implementation\"\"\"\n",
        "exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
        "def multi_head_attention(self, x):\n",
        "\"\"\"Simplified multi-head attention\"\"\"\n",
        "batch_size, seq_len, d_model = x.shape\n",
        "# Split into multiple heads\n",
        "x_reshaped = x.reshape(batch_size, seq_len, self.n_heads, d_model // self.n_heads)\n",
        "x_reshaped = x_reshaped.transpose(0, 2, 1, 3) # (batch, heads, seq_len, depth)\n",
        "# Self-attention (using same input for Q, K, V)\n",
        "attention_output, attention_weights = self.scaled_dot_product_attention(\n",
        "x_reshaped, x_reshaped, x_reshaped\n",
        ")\n",
        "# Concatenate heads\n",
        "attention_output = attention_output.transpose(0, 2, 1, 3)\n",
        "attention_output = attention_output.reshape(batch_size, seq_len, d_model)\n",
        "return attention_output, attention_weights\n",
        "# Demonstrate transformer concepts\n",
        "print(\"ðŸ” TRANSFORMER CONCEPTS DEMONSTRATION:\")\n",
        "# Create sample data\n",
        "batch_size = 2\n",
        "seq_length = 5\n",
        "d_model = 64\n",
        "# Sample input (random embeddings)\n",
        "sample_input = np.random.randn(batch_size, seq_length, d_model)\n",
        "print(f\"Sample input shape: {sample_input.shape}\")\n",
        "# Initialize transformer\n",
        "transformer = SimpleTransformer(vocab_size=1000, d_model=d_model)\n",
        "# Positional encoding\n",
        "positions = np.arange(seq_length)[:, np.newaxis]\n",
        "pos_encoding = transformer.positional_encoding(positions, d_model)\n",
        "print(f\"Positional encoding shape: {pos_encoding.shape}\")\n",
        "# Add positional encoding to input\n",
        "input_with_pos = sample_input + pos_encoding[np.newaxis, :, :]\n",
        "# Multi-head attention\n",
        "attention_output, attention_weights = transformer.multi_head_attention(input_with_pos)\n",
        "print(f\"Attention output shape: {attention_output.shape}\")\n",
        "print(f\"Attention weights shape: {attention_weights.shape}\")\n",
        "# Visualize attention weights\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.imshow(attention_weights[0, 0], cmap='viridis', aspect='auto')\n",
        "plt.colorbar()\n",
        "plt.xlabel('Key Position')\n",
        "plt.ylabel('Query Position')\n",
        "plt.title('Self-Attention Weights (First Head, First Batch)')\n",
        "plt.show()\n",
        "# Using Hugging Face Transformers for Real Applications\n",
        "print(\"\\nðŸ¤— HUGGING FACE TRANSFORMERS IMPLEMENTATION:\")\n",
        "try:\n",
        "from transformers import AutoTokenizer, AutoModel, pipeline\n",
        "# Load pre-trained model and tokenizer\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "print(f\"Loaded model: {model_name}\")\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "# Text classification example\n",
        "classifier = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "sample_texts = [\n",
        "\"I love machine learning!\",\n",
        "\"This is terrible.\",\n",
        "\"The weather is nice today.\",\n",
        "\"I'm feeling neutral about this.\"\n",
        "]\n",
        "results = classifier(sample_texts)\n",
        "print(\"\\nðŸ“Š Sentiment Analysis Results:\")\n",
        "for text, result in zip(sample_texts, results):\n",
        "print(f\"'{text}' -> {result['label']} (confidence: {result['score']:.3f})\")\n",
        "# Text generation example\n",
        "generator = pipeline(\"text-generation\", model=\"gpt2\", max_length=50)\n",
        "prompt = \"The future of artificial intelligence\"\n",
        "generated = generator(prompt, num_return_sequences=1)\n",
        "print(f\"\\nðŸ¤– Text Generation:\")\n",
        "print(f\"Prompt: '{prompt}'\")\n",
        "print(f\"Generated: '{generated[0]['generated_text']}'\")\n",
        "except ImportError:\n",
        "print(\"Hugging Face transformers not available. Using simulated results.\")\n",
        "# Simulate sentiment analysis results\n",
        "sample_texts = [\n",
        "\"I love machine learning!\",\n",
        "\"This is terrible.\",\n",
        "\"The weather is nice today.\",\n",
        "\"I'm feeling neutral about this.\"\n",
        "]\n",
        "simulated_results = [\n",
        "{\"label\": \"POSITIVE\", \"score\": 0.98},\n",
        "{\"label\": \"NEGATIVE\", \"score\": 0.95},\n",
        "{\"label\": \"POSITIVE\", \"score\": 0.87},\n",
        "{\"label\": \"NEUTRAL\", \"score\": 0.65}\n",
        "]\n",
        "print(\"\\nðŸ“Š Simulated Sentiment Analysis Results:\")\n",
        "for text, result in zip(sample_texts, simulated_results):\n",
        "print(f\"'{text}' -> {result['label']} (confidence: {result['score']:.3f})\")\n",
        "# BERT for Text Classification\n",
        "print(\"\\nðŸ”¤ BERT for Text Classification:\")\n",
        "# Create synthetic text classification dataset\n",
        "texts = [\n",
        "\"The movie was fantastic and I loved every minute of it\",\n",
        "\"This product is terrible and does not work as advertised\",\n",
        "\"The weather today is beautiful and sunny\",\n",
        "\"I feel very disappointed with the service provided\",\n",
        "\"This book is amazing and well written\",\n",
        "\"The food was awful and overpriced\",\n",
        "\"Great customer service and fast delivery\",\n",
        "\"Poor quality materials used in this product\"\n",
        "]\n",
        "labels = [1, 0, 1, 0, 1, 0, 1, 0] # 1: positive, 0: negative\n",
        "# If transformers is available, demonstrate fine-tuning concept\n",
        "try:\n",
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
        "# Tokenize texts\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "encoded_inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"tf\")\n",
        "print(f\"Tokenized inputs shape: {encoded_inputs['input_ids'].shape}\")\n",
        "print(f\"Attention mask shape: {encoded_inputs['attention_mask'].shape}\")\n",
        "# Demonstrate BERT embeddings\n",
        "model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "with torch.no_grad():\n",
        "outputs = model(**encoded_inputs)\n",
        "embeddings = outputs.last_hidden_state\n",
        "print(f\"BERT embeddings shape: {embeddings.shape}\")\n",
        "# Pooled output (CLS token)\n",
        "pooled_output = embeddings[:, 0, :]\n",
        "print(f\"Pooled output shape: {pooled_output.shape}\")\n",
        "except ImportError:\n",
        "print(\"PyTorch not available for BERT demonstration.\")\n",
        "# Transformer Visualization\n",
        "print(\"\\nðŸ“Š TRANSFORMER ARCHITECTURE VISUALIZATION:\")\n",
        "# Create a visualization of transformer components\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "# 1. Positional Encoding Visualization\n",
        "positions = np.arange(50)\n",
        "d_model = 64\n",
        "pos_encoding = transformer.positional_encoding(positions, d_model)\n",
        "im = axes[0,0].imshow(pos_encoding.T, cmap='viridis', aspect='auto')\n",
        "axes[0,0].set_xlabel('Position')\n",
        "axes[0,0].set_ylabel('Dimension')\n",
        "axes[0,0].set_title('Positional Encoding')\n",
        "plt.colorbar(im, ax=axes[0,0])\n",
        "# 2. Attention Pattern Examples\n",
        "def create_attention_patterns(seq_length):\n",
        "\"\"\"Create different attention patterns\"\"\"\n",
        "patterns = {}\n",
        "# Causal attention (for GPT)\n",
        "causal = np.tril(np.ones((seq_length, seq_length)))\n",
        "patterns['Causal'] = causal\n",
        "# Full attention (for BERT)\n",
        "full = np.ones((seq_length, seq_length))\n",
        "patterns['Full'] = full\n",
        "# Local attention\n",
        "local = np.zeros((seq_length, seq_length))\n",
        "for i in range(seq_length):\n",
        "start = max(0, i-2)\n",
        "end = min(seq_length, i+3)\n",
        "local[i, start:end] = 1\n",
        "patterns['Local'] = local\n",
        "return patterns\n",
        "seq_len = 10\n",
        "patterns = create_attention_patterns(seq_len)\n",
        "for i, (name, pattern) in enumerate(patterns.items()):\n",
        "row = i // 2\n",
        "col = i % 2\n",
        "axes[0,1].imshow(pattern, cmap='viridis')\n",
        "axes[0,1].set_xlabel('Key Position')\n",
        "axes[0,1].set_ylabel('Query Position')\n",
        "axes[0,1].set_title(f'Attention Pattern: {name}')\n",
        "# 3. Multi-Head Attention Concept\n",
        "def visualize_multihead_attention():\n",
        "\"\"\"Visualize multi-head attention concept\"\"\"\n",
        "seq_len = 6\n",
        "n_heads = 4\n",
        "# Create sample attention weights for each head\n",
        "attention_heads = []\n",
        "for i in range(n_heads):\n",
        "# Each head focuses on different patterns\n",
        "if i == 0:\n",
        "# Head 0: Diagonal attention\n",
        "head = np.eye(seq_len)\n",
        "elif i == 1:\n",
        "# Head 1: Global attention\n",
        "head = np.ones((seq_len, seq_len)) / seq_len\n",
        "elif i == 2:\n",
        "# Head 2: Local attention\n",
        "head = np.zeros((seq_len, seq_len))\n",
        "for j in range(seq_len):\n",
        "start = max(0, j-1)\n",
        "end = min(seq_len, j+2)\n",
        "head[j, start:end] = 1.0 / (end - start)\n",
        "else:\n",
        "# Head 3: Random pattern\n",
        "head = np.random.rand(seq_len, seq_len)\n",
        "head = head / head.sum(axis=1, keepdims=True)\n",
        "attention_heads.append(head)\n",
        "return attention_heads\n",
        "attention_heads = visualize_multihead_attention()\n",
        "# Plot each attention head\n",
        "for i, head in enumerate(attention_heads):\n",
        "row = 1 + i // 2\n",
        "col = i % 2\n",
        "im = axes[row, col].imshow(head, cmap='viridis', aspect='auto')\n",
        "axes[row, col].set_xlabel('Key Position')\n",
        "axes[row, col].set_ylabel('Query Position')\n",
        "axes[row, col].set_title(f'Attention Head {i+1}')\n",
        "plt.colorbar(im, ax=axes[row, col])\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "# Transformer Applications\n",
        "print(\"\\nðŸŽ¯ TRANSFORMER APPLICATIONS:\")\n",
        "applications = {\n",
        "\"Text Classification\": \"BERT, RoBERTa, DistilBERT\",\n",
        "\"Text Generation\": \"GPT, GPT-2, GPT-3, GPT-4\",\n",
        "\"Machine Translation\": \"mBART, T5, MarianMT\",\n",
        "\"Question Answering\": \"BERT, RoBERTa, ALBERT\",\n",
        "\"Named Entity Recognition\": \"BERT, Spacy Transformers\",\n",
        "\"Text Summarization\": \"BART, T5, PEGASUS\",\n",
        "\"Sentiment Analysis\": \"DistilBERT, BERT\",\n",
        "\"Code Generation\": \"Codex, CodeGen, StarCoder\"\n",
        "}\n",
        "print(\"Common Transformer Applications:\")\n",
        "for app, models in applications.items():\n",
        "print(f\"â€¢ {app}: {models}\")\n",
        "# Fine-tuning Transformers\n",
        "print(\"\\nðŸ”§ FINE-TUNING TRANSFORMERS:\")\n",
        "fine_tuning_steps = [\n",
        "\"1. Choose pre-trained model (BERT, GPT, etc.)\",\n",
        "\"2. Prepare domain-specific dataset\",\n",
        "\"3. Add task-specific head (classification, generation)\",\n",
        "\"4. Freeze base layers or use gradual unfreezing\",\n",
        "\"5. Train with lower learning rate\",\n",
        "\"6. Evaluate on validation set\",\n",
        "\"7. Deploy fine-tuned model\"\n",
        "]\n",
        "print(\"Fine-tuning Steps:\")\n",
        "for step in fine_tuning_steps:\n",
        "print(step)\n",
        "# Transformer Limitations and Solutions\n",
        "print(\"\\nâš ï¸ TRANSFORMER LIMITATIONS:\")\n",
        "limitations = {\n",
        "\"Computational Complexity\": \"O(nÂ²) for sequence length\",\n",
        "\"Memory Usage\": \"High for long sequences\",\n",
        "\"Training Time\": \"Requires extensive pre-training\",\n",
        "\"Data Requirements\": \"Needs large datasets\",\n",
        "\"Interpretability\": \"Black-box nature\"\n",
        "}\n",
        "solutions = {\n",
        "\"Computational Complexity\": \"Sparse attention, Linformer, Performer\",\n",
        "\"Memory Usage\": \"Gradient checkpointing, model parallelism\",\n",
        "\"Training Time\": \"Distributed training, mixed precision\",\n",
        "\"Data Requirements\": \"Transfer learning, data augmentation\",\n",
        "\"Interpretability\": \"Attention visualization, probing\"\n",
        "}\n",
        "print(\"Limitations and Solutions:\")\n",
        "for limitation, solution in zip(limitations.items(), solutions # 14. RECURRENT NEURAL NETWORKS (RNN)\n",
        "## Algorithm Background & Mathematical Foundation\n",
        "**Core Concept:** RNNs are designed for sequential data by maintaining a hidden state that captures information about previous elements in the sequence.\n",
        "**Mathematical Formulation:**\n",
        "**Basic RNN:**\n",
        "- **Hidden state update:** $h_t = \\sigma(W_{hh}h_{t-1} + W_{xh}x_t + b_h)$\n",
        "- **Output:** $y_t = \\sigma(W_{hy}h_t + b_y)$\n",
        "Where:\n",
        "- $h_t$: hidden state at time t\n",
        "- $x_t$: input at time t\n",
        "- $y_t$: output at time t\n",
        "- $W$: weight matrices\n",
        "- $b$: bias vectors\n",
        "- $\\sigma$: activation function\n",
        "**Vanishing Gradient Problem:**\n",
        "- Gradients become exponentially small through time steps\n",
        "- Limits learning of long-range dependencies\n",
        "**LSTM (Long Short-Term Memory):**\n",
        "- **Forget gate:** $f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$\n",
        "- **Input gate:** $i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$\n",
        "- **Output gate:** $o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$\n",
        "- **Cell state:** $C_t = f_t \\odot C_{t-1} + i_t \\odot \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)$\n",
        "- **Hidden state:** $h_t = o_t \\odot \\tanh(C_t)$\n",
        "**GRU (Gated Recurrent Unit):**\n",
        "Simplified version with update and reset gates"
      ],
      "id": "xqbF4bet0ofJ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgGbMw1P0ofJ"
      },
      "source": [
        "# Cell 16: Recurrent Neural Networks - Comprehensive Implementation\n",
        "print(\"ðŸš€ RECURRENT NEURAL NETWORKS: COMPREHENSIVE IMPLEMENTATION\\n\")\n",
        "# Create synthetic time series data\n",
        "def generate_time_series_data(n_samples=1000, seq_length=50):\n",
        "\"\"\"Generate synthetic time series data with multiple patterns\"\"\"\n",
        "time = np.linspace(0, 100, seq_length)\n",
        "data = []\n",
        "for i in range(n_samples):\n",
        "# Combine multiple sine waves with different frequencies\n",
        "signal = (np.sin(0.1 * time + i * 0.01) +\n",
        "0.5 * np.sin(0.3 * time + i * 0.02) +\n",
        "0.3 * np.sin(0.7 * time + i * 0.03) +\n",
        "np.random.normal(0, 0.1, seq_length))\n",
        "data.append(signal)\n",
        "return np.array(data)\n",
        "# Generate data\n",
        "X_ts = generate_time_series_data(1000, 50)\n",
        "print(\"ðŸ“Š Time Series Data Overview:\")\n",
        "print(f\"â€¢ Samples: {X_ts.shape[0]}\")\n",
        "print(f\"â€¢ Sequence length: {X_ts.shape[1]}\")\n",
        "print(f\"â€¢ Data range: [{X_ts.min():.2f}, {X_ts.max():.2f}]\")\n",
        "# Create sequences for prediction (predict next value from previous 10)\n",
        "def create_sequences(data, seq_length=10):\n",
        "X, y = [], []\n",
        "for i in range(len(data) - seq_length):\n",
        "X.append(data[i:(i + seq_length)])\n",
        "y.append(data[i + seq_length])\n",
        "return np.array(X), np.array(y)\n",
        "seq_length = 10\n",
        "X_seq, y_seq = create_sequences(X_ts, seq_length)\n",
        "# Reshape for RNN (samples, time steps, features)\n",
        "X_seq = X_seq.reshape(X_seq.shape[0], X_seq.shape[1], 1)\n",
        "y_seq = y_seq.reshape(-1, 1)\n",
        "print(f\"â€¢ Sequence data shape: {X_seq.shape}\")\n",
        "print(f\"â€¢ Target shape: {y_seq.shape}\")\n",
        "# Split data\n",
        "X_train_seq, X_test_seq, y_train_seq, y_test_seq = model_selection.train_test_split(\n",
        "X_seq, y_seq, test_size=0.2, random_state=42\n",
        ")\n",
        "# Build different RNN architectures\n",
        "def create_simple_rnn():\n",
        "model = Sequential([\n",
        "layers.SimpleRNN(50, activation='relu', input_shape=(seq_length, 1)),\n",
        "layers.Dense(1)\n",
        "])\n",
        "return model\n",
        "def create_lstm_model():\n",
        "model = Sequential([\n",
        "layers.LSTM(50, activation='relu', input_shape=(seq_length, 1)),\n",
        "layers.Dense(1)\n",
        "])\n",
        "return model\n",
        "def create_gru_model():\n",
        "model = Sequential([\n",
        "layers.GRU(50, activation='relu', input_shape=(seq_length, 1)),\n",
        "layers.Dense(1)\n",
        "])\n",
        "return model\n",
        "def create_deep_rnn():\n",
        "model = Sequential([\n",
        "layers.LSTM(50, activation='relu', return_sequences=True, input_shape=(seq_length, 1)),\n",
        "layers.Dropout(0.2),\n",
        "layers.LSTM(25, activation='relu'),\n",
        "layers.Dropout(0.2),\n",
        "layers.Dense(1)\n",
        "])\n",
        "return model\n",
        "# Compile and train models\n",
        "rnn_models = {\n",
        "'Simple RNN': create_simple_rnn(),\n",
        "'LSTM': create_lstm_model(),\n",
        "'GRU': create_gru_model(),\n",
        "'Deep LSTM': create_deep_rnn()\n",
        "}\n",
        "rnn_histories = {}\n",
        "for name, model in rnn_models.items():\n",
        "print(f\"\\nðŸ“Š Training {name}...\")\n",
        "# Compile model\n",
        "model.compile(\n",
        "optimizer='adam',\n",
        "loss='mse',\n",
        "metrics=['mae']\n",
        ")\n",
        "# Train model\n",
        "history = model.fit(\n",
        "X_train_seq, y_train_seq,\n",
        "validation_data=(X_test_seq, y_test_seq),\n",
        "epochs=20,\n",
        "batch_size=32,\n",
        "verbose=0\n",
        ")\n",
        "rnn_histories[name] = {\n",
        "'model': model,\n",
        "'history': history\n",
        "}\n",
        "# Evaluate model\n",
        "test_loss, test_mae = model.evaluate(X_test_seq, y_test_seq, verbose=0)\n",
        "print(f\"â€¢ Test MSE: {test_loss:.4f}\")\n",
        "print(f\"â€¢ Test MAE: {test_mae:.4f}\")\n",
        "# Comprehensive Visualization\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "# 1. Training History - Loss\n",
        "for name, rnn_data in rnn_histories.items():\n",
        "history = rnn_data['history']\n",
        "axes[0,0].plot(history.history['loss'], label=f'{name} - Train')\n",
        "axes[0,0].plot(history.history['val_loss'], label=f'{name} - Val', linestyle='--')\n",
        "axes[0,0].set_xlabel('Epochs')\n",
        "axes[0,0].set_ylabel('MSE Loss')\n",
        "axes[0,0].set_title('Training and Validation Loss')\n",
        "axes[0,0].legend()\n",
        "axes[0,0].grid(True)\n",
        "# 2. Model Comparison\n",
        "models_rnn = list(rnn_histories.keys())\n",
        "final_losses = [rnn_histories[name]['history'].history['val_loss'][-1] for name in models_rnn]\n",
        "bars = axes[0,1].bar(models_rnn, final_losses, color=['blue', 'green', 'orange', 'red'])\n",
        "axes[0,1].set_ylabel('Final Validation MSE')\n",
        "axes[0,1].set_title('RNN Architectures Comparison')\n",
        "axes[0,1].tick_params(axis='x', rotation=45)\n",
        "for bar, loss in zip(bars, final_losses):\n",
        "axes[0,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
        "f'{loss:.3f}', ha='center', va='bottom')\n",
        "# 3. Prediction Visualization\n",
        "best_rnn_name = min(rnn_histories.keys(), key=lambda x: rnn_histories[x]['history'].history['val_loss'][-1])\n",
        "best_rnn_model = rnn_histories[best_rnn_name]['model']\n",
        "# Make predictions\n",
        "y_pred_seq = best_rnn_model.predict(X_test_seq)\n",
        "# Plot predictions vs actual for first few samples\n",
        "for i in range(3):\n",
        "axes[0,2].plot(y_test_seq[i], 'b-', alpha=0.7, label='Actual' if i == 0 else \"\")\n",
        "axes[0,2].plot(y_pred_seq[i], 'r--', alpha=0.7, label='Predicted' if i == 0 else \"\")\n",
        "axes[0,2].set_xlabel('Time Step')\n",
        "axes[0,2].set_ylabel('Value')\n",
        "axes[0,2].set_title(f'Predictions vs Actual ({best_rnn_name})')\n",
        "axes[0,2].legend()\n",
        "axes[0,2].grid(True)\n",
        "# 4. Multi-step Prediction\n",
        "def multi_step_prediction(model, initial_sequence, steps=20):\n",
        "\"\"\"Generate multi-step predictions\"\"\"\n",
        "current_sequence = initial_sequence.copy()\n",
        "predictions = []\n",
        "for _ in range(steps):\n",
        "# Predict next value\n",
        "next_pred = model.predict(current_sequence.reshape(1, seq_length, 1), verbose=0)[0, 0]\n",
        "predictions.append(next_pred)\n",
        "# Update sequence (remove first, add prediction)\n",
        "current_sequence = np.roll(current_sequence, -1)\n",
        "current_sequence[-1] = next_pred\n",
        "return np.array(predictions)\n",
        "# Test multi-step prediction\n",
        "initial_seq = X_test_seq[0]\n",
        "true_future = y_test_seq[0:20].flatten()\n",
        "pred_future = multi_step_prediction(best_rnn_model, initial_seq.flatten(), steps=20)\n",
        "axes[1,0].plot(range(len(initial_seq)), initial_seq.flatten(), 'g-', label='Input Sequence')\n",
        "axes[1,0].plot(range(len(initial_seq), len(initial_seq) + len(true_future)), true_future, 'b-', label='True Future')\n",
        "axes[1,0].plot(range(len(initial_seq), len(initial_seq) + len(pred_future)), pred_future, 'r--', label='Predicted Future')\n",
        "axes[1,0].set_xlabel('Time Step')\n",
        "axes[1,0].set_ylabel('Value')\n",
        "axes[1,0].set_title('Multi-step Prediction')\n",
        "axes[1,0].legend()\n",
        "axes[1,0].grid(True)\n",
        "# 5. Sequence Length Analysis\n",
        "sequence_lengths = [5, 10, 15, 20]\n",
        "seq_length_results = {}\n",
        "for seq_len in sequence_lengths:\n",
        "X_temp, y_temp = create_sequences(X_ts, seq_len)\n",
        "X_temp = X_temp.reshape(X_temp.shape[0], X_temp.shape[1], 1)\n",
        "X_train_temp, X_test_temp, y_train_temp, y_test_temp = model_selection.train_test_split(\n",
        "X_temp, y_temp, test_size=0.2, random_state=42\n",
        ")\n",
        "model_temp = create_lstm_model()\n",
        "model_temp.compile(optimizer='adam', loss='mse')\n",
        "history_temp = model_temp.fit(\n",
        "X_train_temp, y_train_temp,\n",
        "validation_data=(X_test_temp, y_test_temp),\n",
        "epochs=10,\n",
        "batch_size=32,\n",
        "verbose=0\n",
        ")\n",
        "final_loss = history_temp.history['val_loss'][-1]\n",
        "seq_length_results[seq_len] = final_loss\n",
        "axes[1,1].plot(sequence_lengths, [seq_length_results[sl] for sl in sequence_lengths], 'bo-')\n",
        "axes[1,1].set_xlabel('Sequence Length')\n",
        "axes[1,1].set_ylabel('Validation MSE')\n",
        "axes[1,1].set_title('Sequence Length vs Performance')\n",
        "axes[1,1].grid(True)\n",
        "# 6. Different Activation Functions\n",
        "activations = ['relu', 'tanh', 'sigmoid']\n",
        "activation_results = {}\n",
        "for activation in activations:\n",
        "model_temp = Sequential([\n",
        "layers.LSTM(50, activation=activation, input_shape=(seq_length, 1)),\n",
        "layers.Dense(1)\n",
        "])\n",
        "model_temp.compile(optimizer='adam', loss='mse')\n",
        "history_temp = model_temp.fit(\n",
        "X_train_seq, y_train_seq,\n",
        "validation_data=(X_test_seq, y_test_seq),\n",
        "epochs=10,\n",
        "batch_size=32,\n",
        "verbose=0\n",
        ")\n",
        "final_loss = history_temp.history['val_loss'][-1]\n",
        "activation_results[activation] = final_loss\n",
        "bars = axes[1,2].bar(activations, [activation_results[act] for act in activations],\n",
        "color=['blue', 'green', 'orange'])\n",
        "axes[1,2].set_ylabel('Validation MSE')\n",
        "axes[1,2].set_title('Activation Functions Comparison')\n",
        "for bar, loss in zip(bars, [activation_results[act] for act in activations]):\n",
        "axes[1,2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
        "f'{loss:.3f}', ha='center', va='bottom')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "# Text Generation Example\n",
        "print(\"\\nðŸ“ RNN for Text Generation:\")\n",
        "# Simple character-level text generation example\n",
        "text = \"\"\"Machine learning is a subset of artificial intelligence that provides systems the ability to automatically learn and improve from experience without being explicitly programmed.\"\"\"\n",
        "text = text.lower()\n",
        "# Create character mapping\n",
        "chars = sorted(list(set(text)))\n",
        "char_to_idx = {char: idx for idx, char in enumerate(chars)}\n",
        "idx_to_char = {idx: char for idx, char in enumerate(chars)}\n",
        "print(f\"â€¢ Unique characters: {len(chars)}\")\n",
        "print(f\"â€¢ Text length: {len(text)}\")\n",
        "# Prepare sequences for training\n",
        "max_sequence_length = 40\n",
        "step = 3\n",
        "sequences = []\n",
        "next_chars = []\n",
        "for i in range(0, len(text) - max_sequence_length, step):\n",
        "sequences.append(text[i:i + max_sequence_length])\n",
        "next_chars.append(text[i + max_sequence_length])\n",
        "print(f\"â€¢ Number of sequences: {len(sequences)}\")\n",
        "# Vectorize sequences\n",
        "X_text = np.zeros((len(sequences), max_sequence_length, len(chars)), dtype=bool)\n",
        "y_text = np.zeros((len(sequences), len(chars)), dtype=bool)\n",
        "for i, sequence in enumerate(sequences):\n",
        "for t, char in enumerate(sequence):\n",
        "X_text[i, t, char_to_idx[char]] = 1\n",
        "y_text[i, char_to_idx[next_chars[i]]] = 1\n",
        "# Build character-level RNN\n",
        "text_model = Sequential([\n",
        "layers.LSTM(128, input_shape=(max_sequence_length, len(chars))),\n",
        "layers.Dense(len(chars), activation='softmax')\n",
        "])\n",
        "text_model.compile(\n",
        "optimizer='adam',\n",
        "loss='categorical_crossentropy',\n",
        "metrics=['accuracy']\n",
        ")\n",
        "# Train for a few epochs\n",
        "history_text = text_model.fit(\n",
        "X_text, y_text,\n",
        "batch_size=128,\n",
        "epochs=50,\n",
        "verbose=0\n",
        ")\n",
        "print(f\"Text model final accuracy: {history_text.history['accuracy'][-1]:.4f}\")\n",
        "# Generate text function\n",
        "def generate_text(model, seed_text, length=100, temperature=1.0):\n",
        "generated = seed_text\n",
        "for _ in range(length):\n",
        "# Prepare input\n",
        "x = np.zeros((1, max_sequence_length, len(chars)))\n",
        "for t, char in enumerate(seed_text):\n",
        "if char in char_to_idx:\n",
        "x[0, t, char_to_idx[char]] = 1\n",
        "# Predict next character\n",
        "preds = model.predict(x, verbose=0)[0]\n",
        "# Apply temperature\n",
        "preds = np.log(preds) / temperature\n",
        "exp_preds = np.exp(preds)\n",
        "preds = exp_preds / np.sum(exp_preds)\n",
        "# Sample next character\n",
        "next_idx = np.random.choice(len(chars), p=preds)\n",
        "next_char = idx_to_char[next_idx]\n",
        "generated += next_char\n",
        "seed_text = seed_text[1:] + next_char\n",
        "return generated\n",
        "# Generate some text\n",
        "seed = \"machine learning is\"\n",
        "generated_text = generate_text(text_model, seed, length=100, temperature=0.5)\n",
        "print(f\"\\nGenerated text:\")\n",
        "print(f\"Seed: '{seed}'\")\n",
        "print(f\"Generated: '{generated_text}'\\n\")\n",
        "# Advanced: Bidirectional RNN\n",
        "print(\"\\nðŸ”„ Bidirectional RNN:\")\n",
        "def create_bidirectional_lstm():\n",
        "model = Sequential([\n",
        "layers.Bidirectional(layers.LSTM(25, activation='relu'), input_shape=(seq_length, 1)),\n",
        "layers.Dense(1)\n",
        "])\n",
        "return model\n",
        "bidirectional_model = create_bidirectional_lstm()\n",
        "bidirectional_model.compile(optimizer='adam', loss='mse')\n",
        "history_bi = bidirectional_model.fit(\n",
        "X_train_seq, y_train_seq,\n",
        "validation_data=(X_test_seq, y_test_seq),\n",
        "epochs=20,\n",
        "batch_size=32,\n",
        "verbose=0\n",
        ")\n",
        "bi_loss = history_bi.history['val_loss'][-1]\n",
        "best_loss = rnn_histories[best_rnn_name]['history'].history['val_loss'][-1]\n",
        "print(f\"Bidirectional LSTM Validation MSE: {bi_loss:.4f}\")\n",
        "print(f\"Best regular LSTM Validation MSE: {best_loss:.4f}\")\n",
        "print(f\"Improvement: {((best_loss - bi_loss) / best_loss * 100):.1f}%\")\n",
        "# Attention Mechanism Concept\n",
        "print(\"\\nðŸŽ¯ Attention Mechanism (Conceptual):\")\n",
        "# Demonstrate the concept of attention weights\n",
        "def simple_attention(query, keys, values):\n",
        "\"\"\"Simple attention mechanism demonstration\"\"\"\n",
        "# Calculate attention scores\n",
        "scores = np.dot(keys, query)\n",
        "# Apply softmax\n",
        "attention_weights = np.exp(scores) / np.sum(np.exp(scores))\n",
        "# Weighted sum of values\n",
        "context_vector = np.dot(attention_weights, values)\n",
        "return context_vector, attention_weights\n",
        "# Example usage\n",
        "query = np.array([0.5, 0.3])\n",
        "keys = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])\n",
        "values = np.array([[1, 2], [3, 4], [5, 6]])\n",
        "context, weights = simple_attention(query, keys, values)\n",
        "print(f\"Attention weights: {weights}\")\n",
        "print(f\"Context vector: {context}\")\n",
        "print(\"âœ… Recurrent Neural Networks Analysis Complete!\")"
      ],
      "id": "UgGbMw1P0ofJ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8OhAmEq0ofK"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "## RNN Interview Questions & Answers\n",
        "**Q1: What is the vanishing/exploding gradient problem in RNNs?**\n",
        "**Answer:**\n",
        "- **Vanishing gradients:** Gradients become extremely small when backpropagating through many time steps\n",
        "- **Exploding gradients:** Gradients become extremely large, causing numerical instability\n",
        "- **Cause:** Repeated multiplication of the same weight matrix through time\n",
        "- **Impact:** Difficulty learning long-range dependencies\n",
        "- **Solutions:** LSTM/GRU architectures, gradient clipping, proper initialization\n",
        "**Q2: Compare LSTM and GRU architectures.**\n",
        "**Answer:**\n",
        "- **LSTM:** Three gates (input, forget, output), separate cell state\n",
        "- **GRU:** Two gates (update, reset), merged cell and hidden state\n",
        "- **Complexity:** LSTM has more parameters, GRU is computationally lighter\n",
        "- **Performance:** Similar performance in many tasks, GRU often faster to train\n",
        "- **Use cases:** LSTM for very long sequences, GRU for efficiency\n",
        "**Q3: What are bidirectional RNNs and when are they useful?**\n",
        "**Answer:**\n",
        "- **Bidirectional RNN:** Process sequence in both forward and backward directions\n",
        "- **Architecture:** Two separate hidden layers, one for each direction\n",
        "- **Benefits:** Access to both past and future context for each time step\n",
        "- **Use cases:**\n",
        "- Natural language processing (understanding context)\n",
        "- Speech recognition\n",
        "- Time series analysis with clear context\n",
        "- **Limitations:** Cannot be used for real-time prediction\n",
        "**Q4: How do you handle variable-length sequences in RNNs?**\n",
        "**Answer:**\n",
        "1. **Padding:** Add zeros to make sequences same length\n",
        "2. **Masking:** Ignore padded positions during computation\n",
        "3. **Dynamic RNNs:** Handle variable lengths natively (TensorFlow)\n",
        "4. **Bucketting:** Group sequences by similar lengths\n",
        "5. **Truncation:** Cut sequences to fixed maximum length\n",
        "**Q5: What is teacher forcing in RNN training?**\n",
        "**Answer:**\n",
        "- **Teacher forcing:** Use actual previous output instead of predicted output during training\n",
        "- **Benefits:** Faster convergence, more stable training\n",
        "- **Drawbacks:** Discrepancy between training and inference\n",
        "- **Scheduled sampling:** Gradually reduce teacher forcing during training\n",
        "- **Use cases:** Sequence generation, machine translation\n",
        "# 15. TRANSFORMERS (BERT, GPT)\n",
        "## Algorithm Background & Mathematical Foundation\n",
        "**Core Concept:** Transformers use self-attention mechanisms to process sequences in parallel, capturing global dependencies without recurrence.\n",
        "**Key Components:**\n",
        "**Self-Attention Mechanism:**\n",
        "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
        "Where:\n",
        "- $Q$: Query matrix (what I'm looking for)\n",
        "- $K$: Key matrix (what I can offer)\n",
        "- $V$: Value matrix (what I actually contain)\n",
        "- $d_k$: Dimension of key vectors\n",
        "**Multi-Head Attention:**\n",
        "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$$\n",
        "$$\\text{where head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$\n",
        "**Positional Encoding:**\n",
        "$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)$$\n",
        "$$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)$$\n",
        "**Transformer Architecture:**\n",
        "- Encoder: Multi-head attention â†’ Feed forward â†’ Layer normalization\n",
        "- Decoder: Masked multi-head attention â†’ Encoder-decoder attention â†’ Feed forward"
      ],
      "id": "u8OhAmEq0ofK"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irvlNBcN0ofK"
      },
      "source": [
        "# Cell 17: Transformers - Comprehensive Implementation\n",
        "print(\"ðŸš€ TRANSFORMERS: COMPREHENSIVE IMPLEMENTATION\\n\")\n",
        "# We'll use a simplified implementation to demonstrate transformer concepts\n",
        "import math\n",
        "class SimpleTransformer:\n",
        "\"\"\"Simplified transformer implementation for educational purposes\"\"\"\n",
        "def __init__(self, vocab_size, d_model=64, n_heads=4, ff_dim=128, max_len=100):\n",
        "self.vocab_size = vocab_size\n",
        "self.d_model = d_model\n",
        "self.n_heads = n_heads\n",
        "self.ff_dim = ff_dim\n",
        "self.max_len = max_len\n",
        "def positional_encoding(self, position, d_model):\n",
        "\"\"\"Generate positional encoding\"\"\"\n",
        "angle_rates = 1 / np.power(10000, (2 * (np.arange(d_model)[np.newaxis, :] // 2)) / np.float32(d_model))\n",
        "angle_rads = position[:, np.newaxis] * angle_rates\n",
        "# Apply sin to even indices, cos to odd indices\n",
        "angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "return angle_rads\n",
        "def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "\"\"\"Calculate scaled dot-product attention\"\"\"\n",
        "d_k = K.shape[-1]\n",
        "scores = np.matmul(Q, K.transpose(0, 2, 1)) / np.sqrt(d_k)\n",
        "if mask is not None:\n",
        "scores += (mask * -1e9)\n",
        "attention_weights = self.softmax(scores)\n",
        "output = np.matmul(attention_weights, V)\n",
        "return output, attention_weights\n",
        "def softmax(self, x):\n",
        "\"\"\"Softmax implementation\"\"\"\n",
        "exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
        "def multi_head_attention(self, x):\n",
        "\"\"\"Simplified multi-head attention\"\"\"\n",
        "batch_size, seq_len, d_model = x.shape\n",
        "# Split into multiple heads\n",
        "x_reshaped = x.reshape(batch_size, seq_len, self.n_heads, d_model // self.n_heads)\n",
        "x_reshaped = x_reshaped.transpose(0, 2, 1, 3) # (batch, heads, seq_len, depth)\n",
        "# Self-attention (using same input for Q, K, V)\n",
        "attention_output, attention_weights = self.scaled_dot_product_attention(\n",
        "x_reshaped, x_reshaped, x_reshaped\n",
        ")\n",
        "# Concatenate heads\n",
        "attention_output = attention_output.transpose(0, 2, 1, 3)\n",
        "attention_output = attention_output.reshape(batch_size, seq_len, d_model)\n",
        "return attention_output, attention_weights\n",
        "# Demonstrate transformer concepts\n",
        "print(\"ðŸ” TRANSFORMER CONCEPTS DEMONSTRATION:\")\n",
        "# Create sample data\n",
        "batch_size = 2\n",
        "seq_length = 5\n",
        "d_model = 64\n",
        "# Sample input (random embeddings)\n",
        "sample_input = np.random.randn(batch_size, seq_length, d_model)\n",
        "print(f\"Sample input shape: {sample_input.shape}\")\n",
        "# Initialize transformer\n",
        "transformer = SimpleTransformer(vocab_size=1000, d_model=d_model)\n",
        "# Positional encoding\n",
        "positions = np.arange(seq_length)[:, np.newaxis]\n",
        "pos_encoding = transformer.positional_encoding(positions, d_model)\n",
        "print(f\"Positional encoding shape: {pos_encoding.shape}\")\n",
        "# Add positional encoding to input\n",
        "input_with_pos = sample_input + pos_encoding[np.newaxis, :, :]\n",
        "# Multi-head attention\n",
        "attention_output, attention_weights = transformer.multi_head_attention(input_with_pos)\n",
        "print(f\"Attention output shape: {attention_output.shape}\")\n",
        "print(f\"Attention weights shape: {attention_weights.shape}\")\n",
        "# Visualize attention weights\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.imshow(attention_weights[0, 0], cmap='viridis', aspect='auto')\n",
        "plt.colorbar()\n",
        "plt.xlabel('Key Position')\n",
        "plt.ylabel('Query Position')\n",
        "plt.title('Self-Attention Weights (First Head, First Batch)')\n",
        "plt.show()\n",
        "# Using Hugging Face Transformers for Real Applications\n",
        "print(\"\\nðŸ¤— HUGGING FACE TRANSFORMERS IMPLEMENTATION:\")\n",
        "try:\n",
        "from transformers import AutoTokenizer, AutoModel, pipeline\n",
        "# Load pre-trained model and tokenizer\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "print(f\"Loaded model: {model_name}\")\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "# Text classification example\n",
        "classifier = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "sample_texts = [\n",
        "\"I love machine learning!\",\n",
        "\"This is terrible.\",\n",
        "\"The weather is nice today.\",\n",
        "\"I'm feeling neutral about this.\"\n",
        "]\n",
        "results = classifier(sample_texts)\n",
        "print(\"\\nðŸ“Š Sentiment Analysis Results:\")\n",
        "for text, result in zip(sample_texts, results):\n",
        "print(f\"'{text}' -> {result['label']} (confidence: {result['score']:.3f})\")\n",
        "# Text generation example\n",
        "generator = pipeline(\"text-generation\", model=\"gpt2\", max_length=50)\n",
        "prompt = \"The future of artificial intelligence\"\n",
        "generated = generator(prompt, num_return_sequences=1)\n",
        "print(f\"\\nðŸ¤– Text Generation:\")\n",
        "print(f\"Prompt: '{prompt}'\")\n",
        "print(f\"Generated: '{generated[0]['generated_text']}'\")\n",
        "except ImportError:\n",
        "print(\"Hugging Face transformers not available. Using simulated results.\")\n",
        "# Simulate sentiment analysis results\n",
        "sample_texts = [\n",
        "\"I love machine learning!\",\n",
        "\"This is terrible.\",\n",
        "\"The weather is nice today.\",\n",
        "\"I'm feeling neutral about this.\"\n",
        "]\n",
        "simulated_results = [\n",
        "{\"label\": \"POSITIVE\", \"score\": 0.98},\n",
        "{\"label\": \"NEGATIVE\", \"score\": 0.95},\n",
        "{\"label\": \"POSITIVE\", \"score\": 0.87},\n",
        "{\"label\": \"NEUTRAL\", \"score\": 0.65}\n",
        "]\n",
        "print(\"\\nðŸ“Š Simulated Sentiment Analysis Results:\")\n",
        "for text, result in zip(sample_texts, simulated_results):\n",
        "print(f\"'{text}' -> {result['label']} (confidence: {result['score']:.3f})\")\n",
        "# BERT for Text Classification\n",
        "print(\"\\nðŸ”¤ BERT for Text Classification:\")\n",
        "# Create synthetic text classification dataset\n",
        "texts = [\n",
        "\"The movie was fantastic and I loved every minute of it\",\n",
        "\"This product is terrible and does not work as advertised\",\n",
        "\"The weather today is beautiful and sunny\",\n",
        "\"I feel very disappointed with the service provided\",\n",
        "\"This book is amazing and well written\",\n",
        "\"The food was awful and overpriced\",\n",
        "\"Great customer service and fast delivery\",\n",
        "\"Poor quality materials used in this product\"\n",
        "]\n",
        "labels = [1, 0, 1, 0, 1, 0, 1, 0] # 1: positive, 0: negative\n",
        "# If transformers is available, demonstrate fine-tuning concept\n",
        "try:\n",
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
        "# Tokenize texts\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "encoded_inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"tf\")\n",
        "print(f\"Tokenized inputs shape: {encoded_inputs['input_ids'].shape}\")\n",
        "print(f\"Attention mask shape: {encoded_inputs['attention_mask'].shape}\")\n",
        "# Demonstrate BERT embeddings\n",
        "model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "with torch.no_grad():\n",
        "outputs = model(**encoded_inputs)\n",
        "embeddings = outputs.last_hidden_state\n",
        "print(f\"BERT embeddings shape: {embeddings.shape}\")\n",
        "# Pooled output (CLS token)\n",
        "pooled_output = embeddings[:, 0, :]\n",
        "print(f\"Pooled output shape: {pooled_output.shape}\")\n",
        "except ImportError:\n",
        "print(\"PyTorch not available for BERT demonstration.\")\n",
        "# Transformer Visualization\n",
        "print(\"\\nðŸ“Š TRANSFORMER ARCHITECTURE VISUALIZATION:\")\n",
        "# Create a visualization of transformer components\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "# 1. Positional Encoding Visualization\n",
        "positions = np.arange(50)\n",
        "d_model = 64\n",
        "pos_encoding = transformer.positional_encoding(positions, d_model)\n",
        "im = axes[0,0].imshow(pos_encoding.T, cmap='viridis', aspect='auto')\n",
        "axes[0,0].set_xlabel('Position')\n",
        "axes[0,0].set_ylabel('Dimension')\n",
        "axes[0,0].set_title('Positional Encoding')\n",
        "plt.colorbar(im, ax=axes[0,0])\n",
        "# 2. Attention Pattern Examples\n",
        "def create_attention_patterns(seq_length):\n",
        "\"\"\"Create different attention patterns\"\"\"\n",
        "patterns = {}\n",
        "# Causal attention (for GPT)\n",
        "causal = np.tril(np.ones((seq_length, seq_length)))\n",
        "patterns['Causal'] = causal\n",
        "# Full attention (for BERT)\n",
        "full = np.ones((seq_length, seq_length))\n",
        "patterns['Full'] = full\n",
        "# Local attention\n",
        "local = np.zeros((seq_length, seq_length))\n",
        "for i in range(seq_length):\n",
        "start = max(0, i-2)\n",
        "end = min(seq_length, i+3)\n",
        "local[i, start:end] = 1\n",
        "patterns['Local'] = local\n",
        "return patterns\n",
        "seq_len = 10\n",
        "patterns = create_attention_patterns(seq_len)\n",
        "for i, (name, pattern) in enumerate(patterns.items()):\n",
        "row = i // 2\n",
        "col = i % 2\n",
        "axes[0,1].imshow(pattern, cmap='viridis')\n",
        "axes[0,1].set_xlabel('Key Position')\n",
        "axes[0,1].set_ylabel('Query Position')\n",
        "axes[0,1].set_title(f'Attention Pattern: {name}')\n",
        "# 3. Multi-Head Attention Concept\n",
        "def visualize_multihead_attention():\n",
        "\"\"\"Visualize multi-head attention concept\"\"\"\n",
        "seq_len = 6\n",
        "n_heads = 4\n",
        "# Create sample attention weights for each head\n",
        "attention_heads = []\n",
        "for i in range(n_heads):\n",
        "# Each head focuses on different patterns\n",
        "if i == 0:\n",
        "# Head 0: Diagonal attention\n",
        "head = np.eye(seq_len)\n",
        "elif i == 1:\n",
        "# Head 1: Global attention\n",
        "head = np.ones((seq_len, seq_len)) / seq_len\n",
        "elif i == 2:\n",
        "# Head 2: Local attention\n",
        "head = np.zeros((seq_len, seq_len))\n",
        "for j in range(seq_len):\n",
        "start = max(0, j-1)\n",
        "end = min(seq_len, j+2)\n",
        "head[j, start:end] = 1.0 / (end - start)\n",
        "else:\n",
        "# Head 3: Random pattern\n",
        "head = np.random.rand(seq_len, seq_len)\n",
        "head = head / head.sum(axis=1, keepdims=True)\n",
        "attention_heads.append(head)\n",
        "return attention_heads\n",
        "attention_heads = visualize_multihead_attention()\n",
        "# Plot each attention head\n",
        "for i, head in enumerate(attention_heads):\n",
        "row = 1 + i // 2\n",
        "col = i % 2\n",
        "im = axes[row, col].imshow(head, cmap='viridis', aspect='auto')\n",
        "axes[row, col].set_xlabel('Key Position')\n",
        "axes[row, col].set_ylabel('Query Position')\n",
        "axes[row, col].set_title(f'Attention Head {i+1}')\n",
        "plt.colorbar(im, ax=axes[row, col])\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "# Transformer Applications\n",
        "print(\"\\nðŸŽ¯ TRANSFORMER APPLICATIONS:\")\n",
        "applications = {\n",
        "\"Text Classification\": \"BERT, RoBERTa, DistilBERT\",\n",
        "\"Text Generation\": \"GPT, GPT-2, GPT-3, GPT-4\",\n",
        "\"Machine Translation\": \"mBART, T5, MarianMT\",\n",
        "\"Question Answering\": \"BERT, RoBERTa, ALBERT\",\n",
        "\"Named Entity Recognition\": \"BERT, Spacy Transformers\",\n",
        "\"Text Summarization\": \"BART, T5, PEGASUS\",\n",
        "\"Sentiment Analysis\": \"DistilBERT, BERT\",\n",
        "\"Code Generation\": \"Codex, CodeGen, StarCoder\"\n",
        "}\n",
        "print(\"Common Transformer Applications:\")\n",
        "for app, models in applications.items():\n",
        "print(f\"â€¢ {app}: {models}\")\n",
        "# Fine-tuning Transformers\n",
        "print(\"\\nðŸ”§ FINE-TUNING TRANSFORMERS:\")\n",
        "fine_tuning_steps = [\n",
        "\"1. Choose pre-trained model (BERT, GPT, etc.)\",\n",
        "\"2. Prepare domain-specific dataset\",\n",
        "\"3. Add task-specific head (classification, generation)\",\n",
        "\"4. Freeze base layers or use gradual unfreezing\",\n",
        "\"5. Train with lower learning rate\",\n",
        "\"6. Evaluate on validation set\",\n",
        "\"7. Deploy fine-tuned model\"\n",
        "]\n",
        "print(\"Fine-tuning Steps:\")\n",
        "for step in fine_tuning_steps:\n",
        "print(step)\n",
        "# Transformer Limitations and Solutions\n",
        "print(\"\\nâš ï¸ TRANSFORMER LIMITATIONS:\")\n",
        "limitations = {\n",
        "\"Computational Complexity\": \"O(nÂ²) for sequence length\",\n",
        "\"Memory Usage\": \"High for long sequences\",\n",
        "\"Training Time\": \"Requires extensive pre-training\",\n",
        "\"Data Requirements\": \"Needs large datasets\",\n",
        "\"Interpretability\": \"Black-box nature\"\n",
        "}\n",
        "solutions = {\n",
        "\"Computational Complexity\": \"Sparse attention, Linformer, Performer\",\n",
        "\"Memory Usage\": \"Gradient checkpointing, model parallelism\",\n",
        "\"Training Time\": \"Distributed training, mixed precision\",\n",
        "\"Data Requirements\": \"Transfer learning, data augmentation\",\n",
        "\"Interpretability\": \"Attention visualization, probing\"\n",
        "}\n",
        "print(\"Limitations and Solutions:\")\n",
        "for limitation, solution in zip(limitations.items(), solutions.items()):\n",
        "print(f\"â€¢ {limitation[0]}: {limitation[1]}\")\n",
        "print(f\" Solution: {solution[1]}\")\n",
        "print(\"âœ… Transformers Analysis Complete!\")"
      ],
      "id": "irvlNBcN0ofK"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9ejoxfV0ofK"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "## Transformers Interview Questions & Answers\n",
        "**Q1: What is the key innovation of transformers over RNNs?**\n",
        "**Answer:**\n",
        "- **Parallel processing:** Transformers process entire sequences simultaneously vs sequential processing in RNNs\n",
        "- **Self-attention:** Captures global dependencies in constant time vs RNN's sequential propagation\n",
        "- **No recurrence:** Eliminates vanishing gradient problem in long sequences\n",
        "- **Scalability:** Handles much longer sequences than RNNs\n",
        "- **Performance:** State-of-the-art results across NLP tasks\n",
        "**Q2: Explain the self-attention mechanism.**\n",
        "**Answer:**\n",
        "**Self-attention computes:**\n",
        "1. **Query, Key, Value vectors** from input embeddings\n",
        "2. **Attention scores** as dot product between Query and Key\n",
        "3. **Scaled scores** by dividing by $\\sqrt{d_k}$ for stability\n",
        "4. **Softmax** to get attention weights\n",
        "5. **Weighted sum** of Value vectors using attention weights\n",
        "**Intuition:** Each token attends to all other tokens, learning which are most relevant\n",
        "**Q3: What is the purpose of positional encoding?**\n",
        "**Answer:**\n",
        "- **Problem:** Self-attention is permutation invariant (no notion of order)\n",
        "- **Solution:** Add positional information to input embeddings\n",
        "- **Methods:**\n",
        "- **Sinusoidal encoding:** Fixed mathematical function\n",
        "- **Learned embeddings:** Trainable position vectors\n",
        "- **Properties:** Unique for each position, deterministic, generalizes to longer sequences\n",
        "**Q4: Compare encoder-only, decoder-only, and encoder-decoder architectures.**\n",
        "**Answer:**\n",
        "- **Encoder-only (BERT):** Bidirectional context, good for understanding tasks\n",
        "- Use cases: Classification, NER, QA\n",
        "- **Decoder-only (GPT):** Causal attention, good for generation tasks\n",
        "- Use cases: Text generation, completion\n",
        "- **Encoder-decoder (T5, BART):** Full transformer, good for sequence-to-sequence\n",
        "- Use cases: Translation, summarization\n",
        "**Q5: How do you handle long sequences with transformers?**\n",
        "**Answer:**\n",
        "**Challenges:** O(nÂ²) complexity limits sequence length\n",
        "**Solutions:**\n",
        "- **Sparse attention:** Only attend to subset of positions\n",
        "- **Linear transformers:** Approximate attention with kernels\n",
        "- **Block-wise attention:** Process sequences in chunks\n",
        "- **Memory-efficient attention:** Optimize memory usage\n",
        "- **Longformer, BigBird:** Specialized architectures for long sequences\n",
        "# 16. AUTOENCODERS\n",
        "## Algorithm Background & Mathematical Foundation\n",
        "**Core Concept:** Autoencoders are neural networks that learn efficient data encodings by training to reconstruct their input.\n",
        "**Mathematical Formulation:**\n",
        "**Architecture:**\n",
        "- **Encoder:** $h = f(x) = \\sigma(W_ex + b_e)$\n",
        "- **Bottleneck:** Latent representation $z$\n",
        "- **Decoder:** $\\hat{x} = g(h) = \\sigma(W_dh + b_d)$\n",
        "**Loss Function:**\n",
        "$$L(x, \\hat{x}) = \\|x - \\hat{x}\\|^2$$\n",
        "**Variational Autoencoder (VAE):**\n",
        "- **Encoder:** Learns $\\mu$ and $\\sigma$ of latent distribution\n",
        "- **Sampling:** $z = \\mu + \\sigma \\odot \\epsilon$ where $\\epsilon \\sim \\mathcal{N}(0, I)$\n",
        "- **Loss:** $L = \\mathbb{E}[\\log p(x|z)] - D_{KL}(q(z|x) \\| p(z))$\n",
        "**Types of Autoencoders:**\n",
        "- **Undercomplete:** Bottleneck smaller than input\n",
        "- **Sparse:** Sparse activations in bottleneck\n",
        "- **Denoising:** Train to reconstruct clean data from noisy input\n",
        "- **Contractive:** Regularize to be robust to small input variations"
      ],
      "id": "x9ejoxfV0ofK"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBDk_ZPT0ofK"
      },
      "source": [
        "# Cell 18: Autoencoders - Comprehensive Implementation\n",
        "print(\"ðŸš€ AUTOENCODERS: COMPREHENSIVE IMPLEMENTATION\\n\")\n",
        "# Load MNIST dataset for autoencoder demonstration\n",
        "(X_train_ae, _), (X_test_ae, _) = tf.keras.datasets.mnist.load_data()\n",
        "# Preprocess data\n",
        "X_train_ae = X_train_ae.astype('float32') / 255.0\n",
        "X_test_ae = X_test_ae.astype('float32') / 255.0\n",
        "# Flatten images for simple autoencoder\n",
        "X_train_flat = X_train_ae.reshape(-1, 784)\n",
        "X_test_flat = X_test_ae.reshape(-1, 784)\n",
        "print(\"ðŸ“Š MNIST Dataset for Autoencoders:\")\n",
        "print(f\"â€¢ Training samples: {X_train_flat.shape[0]}\")\n",
        "print(f\"â€¢ Test samples: {X_test_flat.shape[0]}\")\n",
        "print(f\"â€¢ Input dimension: {X_train_flat.shape[1]}\")\n",
        "# Build different autoencoder architectures\n",
        "def create_simple_autoencoder():\n",
        "\"\"\"Simple undercomplete autoencoder\"\"\"\n",
        "encoding_dim = 32\n",
        "# Encoder\n",
        "encoder = Sequential([\n",
        "layers.Dense(128, activation='relu', input_shape=(784,)),\n",
        "layers.Dense(64, activation='relu'),\n",
        "layers.Dense(encoding_dim, activation='relu')\n",
        "])\n",
        "# Decoder\n",
        "decoder = Sequential([\n",
        "layers.Dense(64, activation='relu', input_shape=(encoding_dim,)),\n",
        "layers.Dense(128, activation='relu'),\n",
        "layers.Dense(784, activation='sigmoid')\n",
        "])\n",
        "# Autoencoder\n",
        "autoencoder = Sequential([encoder, decoder])\n",
        "return autoencoder, encoder, decoder\n",
        "def create_conv_autoencoder():\n",
        "\"\"\"Convolutional autoencoder for images\"\"\"\n",
        "# Encoder\n",
        "encoder = Sequential([\n",
        "layers.Reshape((28, 28, 1), input_shape=(784,)),\n",
        "layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
        "layers.MaxPooling2D((2, 2), padding='same'),\n",
        "layers.Conv2D(16, (3, 3), activation='relu', padding='same'),\n",
        "layers.MaxPooling2D((2, 2), padding='same'),\n",
        "layers.Conv2D(8, (3, 3), activation='relu', padding='same'),\n",
        "layers.MaxPooling2D((2, 2), padding='same'),\n",
        "layers.Flatten()\n",
        "])\n",
        "# Decoder\n",
        "decoder = Sequential([\n",
        "layers.Dense(128, activation='relu', input_shape=(128,)),\n",
        "layers.Reshape((4, 4, 8)),\n",
        "layers.Conv2D(8, (3, 3), activation='relu', padding='same'),\n",
        "layers.UpSampling2D((2, 2)),\n",
        "layers.Conv2D(16, (3, 3), activation='relu', padding='same'),\n",
        "layers.UpSampling2D((2, 2)),\n",
        "layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
        "layers.UpSampling2D((2, 2)),\n",
        "layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same'),\n",
        "layers.Reshape((784,))\n",
        "])\n",
        "# Autoencoder\n",
        "autoencoder = Sequential([encoder, decoder])\n",
        "return autoencoder, encoder, decoder\n",
        "def create_denoising_autoencoder():\n",
        "\"\"\"Denoising autoencoder\"\"\"\n",
        "encoding_dim = 32\n",
        "# Encoder\n",
        "encoder = Sequential([\n",
        "layers.GaussianNoise(0.1, input_shape=(784,)),\n",
        "layers.Dense(128, activation='relu'),\n",
        "layers.Dense(64, activation='relu'),\n",
        "layers.Dense(encoding_dim, activation='relu')\n",
        "])\n",
        "# Decoder\n",
        "decoder = Sequential([\n",
        "layers.Dense(64, activation='relu', input_shape=(encoding_dim,)),\n",
        "layers.Dense(128, activation='relu'),\n",
        "layers.Dense(784, activation='sigmoid')\n",
        "])\n",
        "# Autoencoder\n",
        "autoencoder = Sequential([encoder, decoder])\n",
        "return autoencoder, encoder, decoder\n",
        "# Train autoencoders\n",
        "ae_models = {\n",
        "'Simple Autoencoder': create_simple_autoencoder(),\n",
        "'Convolutional Autoencoder': create_conv_autoencoder(),\n",
        "'Denoising Autoencoder': create_denoising_autoencoder()\n",
        "}\n",
        "ae_histories = {}\n",
        "for name, (autoencoder, encoder, decoder) in ae_models.items():\n",
        "print(f\"\\nðŸ“Š Training {name}...\")\n",
        "# Compile autoencoder\n",
        "autoencoder.compile(\n",
        "optimizer='adam',\n",
        "loss='mse',\n",
        "metrics=['mae']\n",
        ")\n",
        "# Prepare data (for denoising autoencoder, use noisy input)\n",
        "if 'Denoising' in name:\n",
        "# Add noise to training data\n",
        "X_train_noisy = X_train_flat + 0.1 * np.random.normal(0, 1, X_train_flat.shape)\n",
        "X_train_noisy = np.clip(X_train_noisy, 0., 1.)\n",
        "train_data = (X_train_noisy, X_train_flat)\n",
        "else:\n",
        "train_data = (X_train_flat, X_train_flat)\n",
        "# Train model\n",
        "history = autoencoder.fit(\n",
        "train_data[0], train_data[1],\n",
        "validation_data=(X_test_flat, X_test_flat),\n",
        "epochs=15,\n",
        "batch_size=128,\n",
        "verbose=0\n",
        ")\n",
        "ae_histories[name] = {\n",
        "'autoencoder': autoencoder,\n",
        "'encoder': encoder,\n",
        "'decoder': decoder,\n",
        "'history': history\n",
        "}\n",
        "# Evaluate model\n",
        "test_loss, test_mae = autoencoder.evaluate(X_test_flat, X_test_flat, verbose=0)\n",
        "print(f\"â€¢ Test MSE: {test_loss:.4f}\")\n",
        "print(f\"â€¢ Test MAE: {test_mae:.4f}\")\n",
        "# Comprehensive Visualization\n",
        "fig, axes = plt.subplots(3, 4, figsize=(18, 12))\n",
        "# 1. Training History\n",
        "for i, (name, ae_data) in enumerate(ae_histories.items()):\n",
        "history = ae_data['history']\n",
        "axes[0,i].plot(history.history['loss'], label='Train')\n",
        "axes[0,i].plot(history.history['val_loss'], label='Validation')\n",
        "axes[0,i].set_xlabel('Epochs')\n",
        "axes[0,i].set_ylabel('MSE Loss')\n",
        "axes[0,i].set_title(f'{name} - Training')\n",
        "axes[0,i].legend()\n",
        "axes[0,i].grid(True)\n",
        "# 2. Original vs Reconstructed Images\n",
        "n_examples = 5\n",
        "sample_indices = np.random.choice(len(X_test_flat), n_examples, replace=False)\n",
        "for i, idx in enumerate(sample_indices):\n",
        "# Original image\n",
        "original = X_test_flat[idx].reshape(28, 28)\n",
        "axes[1,i].imshow(original, cmap='gray')\n",
        "axes[1,i].set_title(f'Original {i+1}')\n",
        "axes[1,i].axis('off')\n",
        "# Reconstructed images from different autoencoders\n",
        "for j, (name, ae_data) in enumerate(ae_histories.items()):\n",
        "autoencoder = ae_data['autoencoder']\n",
        "reconstructed = autoencoder.predict(X_test_flat[idx:idx+1], verbose=0)[0].reshape(28, 28)\n",
        "axes[2+j,i].imshow(reconstructed, cmap='gray')\n",
        "axes[2+j,i].set_title(f'{name[:10]}...')\n",
        "axes[2+j,i].axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "# Latent Space Visualization\n",
        "print(\"\\nðŸ” LATENT SPACE VISUALIZATION:\")\n",
        "# Use the simple autoencoder for latent space analysis\n",
        "simple_ae_data = ae_histories['Simple Autoencoder']\n",
        "encoder = simple_ae_data['encoder']\n",
        "# Encode test images\n",
        "latent_representations = encoder.predict(X_test_flat, verbose=0)\n",
        "print(f\"Latent representations shape: {latent_representations.shape}\")\n",
        "# Reduce to 2D for visualization using PCA\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "latent_2d = pca.fit_transform(latent_representations)\n",
        "plt.figure(figsize=(10, 8))\n",
        "scatter = plt.scatter(latent_2d[:, 0], latent_2d[:, 1], c=y_test, cmap='tab10', alpha=0.6)\n",
        "plt.colorbar(scatter, label='Digit Class')\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.title('Autoencoder Latent Space (PCA projection)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "# Anomaly Detection with Autoencoders\n",
        "print(\"\\nðŸš¨ ANOMALY DETECTION WITH AUTOENCODERS:\")\n",
        "# Calculate reconstruction error for each test sample\n",
        "reconstruction_errors = []\n",
        "for ae_name, ae_data in ae_histories.items():\n",
        "autoencoder = ae_data['autoencoder']\n",
        "reconstructions = autoencoder.predict(X_test_flat, verbose=0)\n",
        "errors = np.mean((X_test_flat - reconstructions) ** 2, axis=1)\n",
        "reconstruction_errors.append((ae_name, errors))\n",
        "# Plot reconstruction error distribution\n",
        "plt.figure(figsize=(12, 6))\n",
        "for ae_name, errors in reconstruction_errors:\n",
        "plt.hist(errors, bins=50, alpha=0.6, label=ae_name)\n",
        "plt.xlabel('Reconstruction Error (MSE)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Reconstruction Error Distribution for Anomaly Detection')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "# Identify potential anomalies (high reconstruction error)\n",
        "threshold = np.percentile(reconstruction_errors[0][1], 95) # 95th percentile\n",
        "anomaly_indices = np.where(reconstruction_errors[0][1] > threshold)[0]\n",
        "print(f\"Anomalies detected: {len(anomaly_indices)} ({len(anomaly_indices)/len(X_test_flat)*100:.1f}%)\")\n",
        "# Show some anomalies\n",
        "plt.figure(figsize=(12, 4))\n",
        "for i, idx in enumerate(anomaly_indices[:8]):\n",
        "plt.subplot(2, 4, i+1)\n",
        "plt.imshow(X_test_flat[idx].reshape(28, 28), cmap='gray')\n",
        "plt.title(f'Error: {reconstruction_errors[0][1][idx]:.3f}')\n",
        "plt.axis('off')\n",
        "plt.suptitle('Detected Anomalies (High Reconstruction Error)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "# Variational Autoencoder (VAE) Implementation\n",
        "print(\"\\nðŸŽ¯ VARIATIONAL AUTOENCODER (VAE) IMPLEMENTATION:\")\n",
        "class Sampling(layers.Layer):\n",
        "\"\"\"Uses (z_mean, z_log_var) to sample z\"\"\"\n",
        "def call(self, inputs):\n",
        "z_mean, z_log_var = inputs\n",
        "batch = tf.shape(z_mean)[0]\n",
        "dim = tf.shape(z_mean)[1]\n",
        "epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "def create_vae(latent_dim=2):\n",
        "\"\"\"Create Variational Autoencoder\"\"\"\n",
        "# Encoder\n",
        "encoder_inputs = layers.Input(shape=(784,))\n",
        "x = layers.Dense(128, activation='relu')(encoder_inputs)\n",
        "x = layers.Dense(64, activation='relu')(x)\n",
        "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
        "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
        "z = Sampling()([z_mean, z_log_var])\n",
        "encoder = tf.keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "# Decoder\n",
        "latent_inputs = layers.Input(shape=(latent_dim,))\n",
        "x = layers.Dense(64, activation='relu')(latent_inputs)\n",
        "x = layers.Dense(128, activation='relu')(x)\n",
        "decoder_outputs = layers.Dense(784, activation='sigmoid')(x)\n",
        "decoder = tf.keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
        "# VAE\n",
        "outputs = decoder(encoder(encoder_inputs)[2])\n",
        "vae = tf.keras.Model(encoder_inputs, outputs, name=\"vae\")\n",
        "# Add KL divergence loss\n",
        "kl_loss = -0.5 * tf.reduce_mean(\n",
        "z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1\n",
        ")\n",
        "vae.add_loss(kl_loss)\n",
        "return vae, encoder, decoder\n",
        "# Create and train VAE\n",
        "vae, vae_encoder, vae_decoder = create_vae(latent_dim=2)\n",
        "vae.compile(optimizer='adam', loss='mse')\n",
        "vae_history = vae.fit(\n",
        "X_train_flat, X_train_flat,\n",
        "validation_data=(X_test_flat, X_test_flat),\n",
        "epochs=20,\n",
        "batch_size=128,\n",
        "verbose=0\n",
        ")\n",
        "print(\"VAE training completed!\")\n",
        "# VAE Latent Space Visualization\n",
        "z_mean, _, _ = vae_encoder.predict(X_test_flat, verbose=0)\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "scatter = plt.scatter(z_mean[:, 0], z_mean[:, 1], c=y_test, cmap='tab10', alpha=0.6)\n",
        "plt.colorbar(scatter, label='Digit Class')\n",
        "plt.xlabel('z[0]')\n",
        "plt.ylabel('z[1]')\n",
        "plt.title('VAE Latent Space')\n",
        "plt.grid(True, alpha=0.3)\n",
        "# Generate new samples from VAE\n",
        "plt.subplot(1, 2, 2)\n",
        "n = 15\n",
        "digit_size = 28\n",
        "figure = np.zeros((digit_size * n, digit_size * n))\n",
        "# Linearly spaced coordinates on the unit square\n",
        "grid_x = np.linspace(-2, 2, n)\n",
        "grid_y = np.linspace(-2, 2, n)[::-1]\n",
        "for i, yi in enumerate(grid_y):\n",
        "for j, xi in enumerate(grid_x):\n",
        "z_sample = np.array([[xi, yi]])\n",
        "x_decoded = vae_decoder.predict(z_sample, verbose=0)\n",
        "digit = x_decoded[0].reshape(digit_size, digit_size)\n",
        "figure[i * digit_size: (i + 1) * digit_size,\n",
        "j * digit_size: (j + 1) * digit_size] = digit\n",
        "plt.imshow(figure, cmap='gray')\n",
        "plt.title('VAE Generated Digits')\n",
        "plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "# Autoencoder Applications Summary\n",
        "print(\"\\nðŸŽ¯ AUTOENCODER APPLICATIONS:\")\n",
        "applications = {\n",
        "\"Dimensionality Reduction\": \"Learn compact representations\",\n",
        "\"Anomaly Detection\": \"High reconstruction error indicates anomalies\",\n",
        "\"Image Denoising\": \"Remove noise from images\",\n",
        "\"Data Generation\": \"VAEs can generate new samples\",\n",
        "\"Feature Learning\": \"Learn meaningful features unsupervised\",\n",
        "\"Data Compression\": \"Efficient encoding of data\"\n",
        "}\n",
        "print(\"Autoencoder Applications:\")\n",
        "for app, description in applications.items():\n",
        "print(f\"â€¢ {app}: {description}\")\n",
        "print(\"âœ… Autoencoders Analysis Complete!\")"
      ],
      "id": "FBDk_ZPT0ofK"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwfTMvNL0ofK"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "## Autoencoders Interview Questions & Answers\n",
        "**Q1: What is the difference between autoencoders and PCA?**\n",
        "**Answer:**\n",
        "- **Linearity:** PCA is linear, autoencoders can learn non-linear transformations\n",
        "- **Flexibility:** Autoencoders can use various architectures (CNN, LSTM)\n",
        "- **Representation:** Autoencoders can learn more complex feature hierarchies\n",
        "- **Training:** PCA has closed-form solution, autoencoders require gradient descent\n",
        "- **Use cases:** PCA for simple linear dimensionality reduction, autoencoders for complex non-linear data\n",
        "**Q2: What is the bottleneck and why is it important?**\n",
        "**Answer:**\n",
        "- **Bottleneck:** Middle layer with reduced dimensionality\n",
        "- **Purpose:** Forces network to learn compressed representation\n",
        "- **Undercomplete:** Bottleneck smaller than input - learns compression\n",
        "- **Overcomplete:** Bottleneck larger than input - needs regularization\n",
        "- **Optimal size:** Balance between reconstruction quality and compression\n",
        "**Q3: Explain Variational Autoencoders (VAEs).**\n",
        "**Answer:**\n",
        "- **Probabilistic approach:** Learn distribution of latent space\n",
        "- **Encoder output:** Mean and variance of latent distribution\n",
        "- **Sampling:** Generate new samples by sampling from latent distribution\n",
        "- **Loss function:** Reconstruction loss + KL divergence\n",
        "- **KL divergence:** Encourages latent distribution to match prior (usually Gaussian)\n",
        "- **Applications:** Data generation, interpolation in latent space\n",
        "**Q4: What are denoising autoencoders?**\n",
        "**Answer:**\n",
        "- **Training:** Learn to reconstruct clean data from corrupted input\n",
        "- **Corruption:** Add noise, mask pixels, or other transformations\n",
        "- **Benefits:**\n",
        "- Learns robust features\n",
        "- Prevents identity function learning\n",
        "- Better generalization\n",
        "- **Use cases:** Image denoising, robust feature learning\n",
        "**Q5: How do you evaluate autoencoder performance?**\n",
        "**Answer:**\n",
        "- **Reconstruction quality:** MSE, SSIM, perceptual metrics\n",
        "- **Latent space quality:** Clustering, visualization, interpretability\n",
        "- **Downstream tasks:** Performance on classification/regression using encoded features\n",
        "- **Generation quality:** For VAEs - quality of generated samples\n",
        "- **Anomaly detection:** ROC curves for reconstruction error thresholding\n",
        "# 17. DBSCAN\n",
        "## Algorithm Background & Mathematical Foundation\n",
        "**Core Concept:** DBSCAN (Density-Based Spatial Clustering of Applications with Noise) finds clusters based on density connectivity.\n",
        "**Key Definitions:**\n",
        "- **Îµ (eps):** Maximum distance between two samples to be considered neighbors\n",
        "- **MinPts:** Minimum number of points required to form a dense region\n",
        "- **Core point:** Point with at least MinPts points within Îµ distance\n",
        "- **Border point:** Point within Îµ distance of core point but doesn't have enough neighbors\n",
        "- **Noise point:** Point that is neither core nor border point\n",
        "**Algorithm Steps:**\n",
        "1. For each point, find points within Îµ distance\n",
        "2. Identify core points (â‰¥ MinPts neighbors)\n",
        "3. Form clusters from connected core points\n",
        "4. Assign border points to clusters\n",
        "5. Mark remaining points as noise\n",
        "**Mathematical Properties:**\n",
        "- **Density reachability:** Point p is density-reachable from q if there's a path of core points\n",
        "- **Density connectivity:** Points p and q are density-connected if there's point o that density-reaches both"
      ],
      "id": "fwfTMvNL0ofK"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzMLozM90ofO"
      },
      "source": [
        "# Cell 19: DBSCAN - Comprehensive Implementation\n",
        "print(\"ðŸš€ DBSCAN: COMPREHENSIVE IMPLEMENTATION\\n\")\n",
        "# Create dataset with complex cluster shapes\n",
        "from sklearn.datasets import make_moons, make_circles, make_blobs\n",
        "# Generate different dataset types\n",
        "datasets_dbscan = {\n",
        "'Moons': make_moons(n_samples=300, noise=0.05, random_state=42),\n",
        "'Circles': make_circles(n_samples=300, noise=0.05, factor=0.5, random_state=42),\n",
        "'Blobs': make_blobs(n_samples=300, centers=3, cluster_std=0.60, random_state=42),\n",
        "'Anisotropic': make_blobs(n_samples=300, centers=3, random_state=42)\n",
        "}\n",
        "# Make anisotropic data\n",
        "transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
        "X_aniso = np.dot(datasets_dbscan['Anisotropic'][0], transformation)\n",
        "datasets_dbscan['Anisotropic'] = (X_aniso, datasets_dbscan['Anisotropic'][1])\n",
        "# Add some noise to datasets\n",
        "for name, (X, y) in datasets_dbscan.items():\n",
        "# Add 10% noise\n",
        "n_noise = int(0.1 * len(X))\n",
        "noise_points = np.random.uniform(X.min(axis=0), X.max(axis=0), (n_noise, 2))\n",
        "X_noisy = np.vstack([X, noise_points])\n",
        "y_noisy = np.hstack([y, -1 * np.ones(n_noise)]) # -1 for noise\n",
        "datasets_dbscan[name] = (X_noisy, y_noisy)\n",
        "print(\"ðŸ“Š Dataset Overview for DBSCAN:\")\n",
        "for name, (X, y) in datasets_dbscan.items():\n",
        "print(f\"â€¢ {name}: {X.shape[0]} points, {len(np.unique(y))} true clusters\")\n",
        "# Scale datasets\n",
        "scaler_dbscan = StandardScaler()\n",
        "scaled_datasets = {}\n",
        "for name, (X, y) in datasets_dbscan.items():\n",
        "X_scaled = scaler_dbscan.fit_transform(X)\n",
        "scaled_datasets[name] = (X_scaled, y)\n",
        "# DBSCAN with different parameters\n",
        "dbscan_results = {}\n",
        "# Parameter grid\n",
        "eps_values = [0.1, 0.2, 0.3, 0.5]\n",
        "min_samples_values = [5, 10, 15]\n",
        "for dataset_name, (X, y_true) in scaled_datasets.items():\n",
        "print(f\"\\nðŸ” Analyzing {dataset_name} dataset...\")\n",
        "dataset_results = {}\n",
        "for eps in eps_values:\n",
        "for min_samples in min_samples_values:\n",
        "# Apply DBSCAN\n",
        "dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "labels = dbscan.fit_predict(X)\n",
        "# Calculate metrics (excluding noise for some metrics)\n",
        "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "n_noise = list(labels).count(-1)\n",
        "# Only calculate silhouette score if we have at least 2 clusters and not all noise\n",
        "if n_clusters > 1 and n_clusters < len(X) - 1:\n",
        "silhouette = silhouette_score(X, labels)\n",
        "else:\n",
        "silhouette = -1\n",
        "dataset_results[(eps, min_samples)] = {\n",
        "'labels': labels,\n",
        "'n_clusters': n_clusters,\n",
        "'n_noise': n_noise,\n",
        "'silhouette': silhouette\n",
        "}\n",
        "dbscan_results[dataset_name] = dataset_results\n",
        "# Find best parameters for each dataset\n",
        "best_params = {}\n",
        "for dataset_name, results in dbscan_results.items():\n",
        "# Find parameters with highest silhouette score (excluding invalid ones)\n",
        "valid_results = {k: v for k, v in results.items() if v['silhouette'] > 0}\n",
        "if valid_results:\n",
        "best_param = max(valid_results.keys(), key=lambda x: valid_results[x]['silhouette'])\n",
        "best_params[dataset_name] = best_param\n",
        "print(f\"â€¢ {dataset_name}: Best eps={best_param[0]}, min_samples={best_param[1]}, \"\n",
        "f\"Silhouette={valid_results[best_param]['silhouette']:.3f}\")\n",
        "# Comprehensive Visualization\n",
        "fig, axes = plt.subplots(4, 4, figsize=(20, 16))\n",
        "for i, (dataset_name, (X, y_true)) in enumerate(scaled_datasets.items()):\n",
        "# Original data with true labels\n",
        "scatter = axes[i,0].scatter(X[:, 0], X[:, 1], c=y_true, cmap='viridis', alpha=0.7)\n",
        "axes[i,0].set_title(f'{dataset_name}\\nTrue Clusters')\n",
        "axes[i,0].set_xlabel('Feature 1')\n",
        "axes[i,0].set_ylabel('Feature 2')\n",
        "axes[i,0].grid(True, alpha=0.3)\n",
        "# K-means for comparison\n",
        "kmeans = KMeans(n_clusters=len(np.unique(y_true[y_true != -1])), random_state=42)\n",
        "kmeans_labels = kmeans.fit_predict(X)\n",
        "axes[i,1].scatter(X[:, 0], X[:, 1], c=kmeans_labels, cmap='viridis', alpha=0.7)\n",
        "axes[i,1].set_title(f'K-means\\nSilhouette: {silhouette_score(X, kmeans_labels):.3f}')\n",
        "axes[i,1].set_xlabel('Feature 1')\n",
        "axes[i,1].set_ylabel('Feature 2')\n",
        "axes[i,1].grid(True, alpha=0.3)\n",
        "# DBSCAN with best parameters\n",
        "if dataset_name in best_params:\n",
        "eps, min_samples = best_params[dataset_name]\n",
        "best_result = dbscan_results[dataset_name][(eps, min_samples)]\n",
        "labels = best_result['labels']\n",
        "# Create color map that distinguishes noise (gray)\n",
        "colors = ['gray' if label == -1 else plt.cm.viridis(label / max(1, best_result['n_clusters']))\n",
        "for label in labels]\n",
        "axes[i,2].scatter(X[:, 0], X[:, 1], c=colors, alpha=0.7)\n",
        "axes[i,2].set_title(f'DBSCAN (Best)\\neps={eps}, min_samples={min_samples}\\n'\n",
        "f'Clusters: {best_result[\"n_clusters\"]}, Noise: {best_result[\"n_noise\"]}\\n'\n",
        "f'Silhouette: {best_result[\"silhouette\"]:.3f}')\n",
        "axes[i,2].set_xlabel('Feature 1')\n",
        "axes[i,2].set_ylabel('Feature 2')\n",
        "axes[i,2].grid(True, alpha=0.3)\n",
        "# DBSCAN parameter sensitivity\n",
        "eps_for_plot = 0.3\n",
        "min_samples_for_plot = 10\n",
        "if (eps_for_plot, min_samples_for_plot) in dbscan_results[dataset_name]:\n",
        "result = dbscan_results[dataset_name][(eps_for_plot, min_samples_for_plot)]\n",
        "labels = result['labels']\n",
        "colors = ['gray' if label == -1 else plt.cm.viridis(label / max(1, result['n_clusters']))\n",
        "for label in labels]\n",
        "axes[i,3].scatter(X[:, 0], X[:, 1], c=colors, alpha=0.7)\n",
        "axes[i,3].set_title(f'DBSCAN (eps=0.3, min_samples=10)\\n'\n",
        "f'Clusters: {result[\"n_clusters\"]}, Noise: {result[\"n_noise\"]}')\n",
        "axes[i,3].set_xlabel('Feature 1')\n",
        "axes[i,3].set_ylabel('Feature 2')\n",
        "axes[i,3].grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "# DBSCAN Parameter Analysis\n",
        "print(\"\\nðŸ”§ DBSCAN PARAMETER ANALYSIS:\")\n",
        "# Analyze parameter sensitivity for one dataset\n",
        "dataset_to_analyze = 'Moons'\n",
        "X_analyze, y_analyze = scaled_datasets[dataset_to_analyze]\n",
        "# Create parameter grid\n",
        "eps_range = np.linspace(0.1, 0.5, 20)\n",
        "min_samples_range = [5, 10, 15, 20]\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "# 1. Number of clusters vs eps for different min_samples\n",
        "for min_samples in min_samples_range:\n",
        "n_clusters_list = []\n",
        "for eps in eps_range:\n",
        "dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "labels = dbscan.fit_predict(X_analyze)\n",
        "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "n_clusters_list.append(n_clusters)\n",
        "axes[0,0].plot(eps_range, n_clusters_list, 'o-', label=f'min_samples={min_samples}')\n",
        "axes[0,0].set_xlabel('eps')\n",
        "axes[0,0].set_ylabel('Number of Clusters')\n",
        "axes[0,0].set_title('Number of Clusters vs eps')\n",
        "axes[0,0].legend()\n",
        "axes[0,0].grid(True)\n",
        "# 2. Noise points vs eps for different min_samples\n",
        "for min_samples in min_samples_range:\n",
        "noise_list = []\n",
        "for eps in eps_range:\n",
        "dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "labels = dbscan.fit_predict(X_analyze)\n",
        "n_noise = list(labels).count(-1)\n",
        "noise_list.append(n_noise)\n",
        "axes[0,1].plot(eps_range, noise_list, 'o-', label=f'min_samples={min_samples}')\n",
        "axes[0,1].set_xlabel('eps')\n",
        "axes[0,1].set_ylabel('Number of Noise Points')\n",
        "axes[0,1].set_title('Noise Points vs eps')\n",
        "axes[0,1].legend()\n",
        "axes[0,1].grid(True)\n",
        "# 3. Silhouette score vs eps for different min_samples\n",
        "for min_samples in min_samples_range:\n",
        "silhouette_list = []\n",
        "for eps in eps_range:\n",
        "dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "labels = dbscan.fit_predict(X_analyze)\n",
        "# Only calculate silhouette if we have reasonable clustering\n",
        "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "if n_clusters > 1 and n_clusters < len(X_analyze) - 1:\n",
        "silhouette = silhouette_score(X_analyze, labels)\n",
        "else:\n",
        "silhouette = -1\n",
        "silhouette_list.append(silhouette)\n",
        "axes[1,0].plot(eps_range, silhouette_list, 'o-', label=f'min_samples={min_samples}')\n",
        "axes[1,0].set_xlabel('eps')\n",
        "axes[1,0].set_ylabel('Silhouette Score')\n",
        "axes[1,0].set_title('Silhouette Score vs eps')\n",
        "axes[1,0].legend()\n",
        "axes[1,0].grid(True)\n",
        "# 4. Reachability plot (k-distance graph)\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "# Calculate k-distances for different k\n",
        "k_values = [5, 10, 15]\n",
        "for k in k_values:\n",
        "neighbors = NearestNeighbors(n_neighbors=k)\n",
        "neighbors_fit = neighbors.fit(X_analyze)\n",
        "distances, indices = neighbors_fit.kneighbors(X_analyze)\n",
        "k_distances = np.sort(distances[:, k-1])\n",
        "axes[1,1].plot(k_distances, label=f'k={k}')\n",
        "axes[1,1].set_xlabel('Points sorted by distance')\n",
        "axes[1,1].set_ylabel(f'k-distance')\n",
        "axes[1,1].set_title('K-distance Graph (for eps selection)')\n",
        "axes[1,1].legend()\n",
        "axes[1,1].grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "# DBSCAN vs Other Clustering Algorithms\n",
        "print(\"\\nðŸ†š DBSCAN vs OTHER CLUSTERING ALGORITHMS:\")\n",
        "comparison_datasets = {\n",
        "'Complex Shapes': scaled_datasets['Moons'],\n",
        "'Noisy Data': scaled_datasets['Circles']\n",
        "}\n",
        "algorithms = {\n",
        "'K-means': KMeans(n_clusters=2, random_state=42),\n",
        "'Agglomerative': AgglomerativeClustering(n_clusters=2),\n",
        "'DBSCAN': DBSCAN(eps=0.3, min_samples=10)\n",
        "}\n",
        "fig, axes = plt.subplots(2, 4, figsize=(20, 8))\n",
        "for i, (dataset_name, (X, y_true)) in enumerate(comparison_datasets.items()):\n",
        "# True clusters\n",
        "axes[i,0].scatter(X[:, 0], X[:, 1], c=y_true, cmap='viridis', alpha=0.7)\n",
        "axes[i,0].set_title(f'{dataset_name}\\nTrue Clusters')\n",
        "axes[i,0].set_xlabel('Feature 1')\n",
        "axes[i,0].set_ylabel('Feature 2')\n",
        "axes[i,0].grid(True, alpha=0.3)\n",
        "# Each algorithm\n",
        "for j, (algo_name, algorithm) in enumerate(algorithms.items()):\n",
        "if algo_name == 'DBSCAN':\n",
        "labels = algorithm.fit_predict(X)\n",
        "# Handle noise points\n",
        "colors = ['gray' if label == -1 else plt.cm.viridis(label / max(1, len(set(labels))-1))\n",
        "for label in labels]\n",
        "else:\n",
        "if dataset_name == 'Complex Shapes':\n",
        "algorithm.set_params(n_clusters=2)\n",
        "else:\n",
        "algorithm.set_params(n_clusters=2)\n",
        "labels = algorithm.fit_predict(X)\n",
        "colors = plt.cm.viridis(labels / max(1, len(set(labels))))\n",
        "axes[i,j+1].scatter(X[:, 0], X[:, 1], c=colors, alpha=0.7)\n",
        "# Calculate metrics\n",
        "if len(set(labels)) > 1 and -1 not in labels or labels[labels != -1].size > 0:\n",
        "silhouette = silhouette_score(X, labels)\n",
        "axes[i,j+1].set_title(f'{algo_name}\\nSilhouette: {silhouette:.3f}')\n",
        "else:\n",
        "axes[i,j+1].set_title(f'{algo_name}\\nInvalid clustering')\n",
        "axes[i,j+1].set_xlabel('Feature 1')\n",
        "axes[i,j+1].set_ylabel('Feature 2')\n",
        "axes[i,j+1].grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "# Real-world Application: Customer Segmentation\n",
        "print(\"\\nðŸ‘¥ CUSTOMER SEGMENTATION WITH DBSCAN:\")\n",
        "# Create synthetic customer data\n",
        "np.random.seed(42)\n",
        "n_customers = 1000\n",
        "# Generate customer features with different densities\n",
        "age = np.concatenate([\n",
        "np.random.normal(25, 5, 200), # Young customers\n",
        "np.random.normal(45, 8, 300), # Middle-aged\n",
        "np.random.normal(65, 6, 200), # Senior\n",
        "np.random.uniform(18, 80, 300) # Noise/unsegmented\n",
        "])\n",
        "income = np.concatenate([\n",
        "np.random.normal(30000, 5000, 200), # Low income\n",
        "np.random.normal(60000, 10000, 300), # Middle income\n",
        "np.random.normal(100000, 20000, 200), # High income\n",
        "np.random.uniform(20000, 120000, 300) # Noise\n",
        "])\n",
        "spending_score = np.concatenate([\n",
        "np.random.normal(80, 10, 200), # High spenders\n",
        "np.random.normal(50, 15, 300), # Moderate spenders\n",
        "np.random.normal(20, 8, 200), # Low spenders\n",
        "np.random.uniform(1, 100, 300) # Noise\n",
        "])\n",
        "customer_data = np.column_stack([age, income, spending_score])\n",
        "# Scale data\n",
        "scaler_customer = StandardScaler()\n",
        "customer_data_scaled = scaler_customer.fit_transform(customer_data)\n",
        "# Apply DBSCAN\n",
        "dbscan_customer = DBSCAN(eps=0.5, min_samples=20)\n",
        "customer_labels = dbscan_customer.fit_predict(customer_data_scaled)\n",
        "# Analyze results\n",
        "n_clusters = len(set(customer_labels)) - (1 if -1 in customer_labels else 0)\n",
        "n_noise = list(customer_labels).count(-1)\n",
        "print(f\"Customer Segmentation Results:\")\n",
        "print(f\"â€¢ Number of clusters: {n_clusters}\")\n",
        "print(f\"â€¢ Number of noise points: {n_noise}\")\n",
        "print(f\"â€¢ Percentage of customers segmented: {(len(customer_labels) - n_noise) / len(customer_labels) * 100:.1f}%\")\n",
        "# Visualize customer segments\n",
        "fig = plt.figure(figsize=(15, 5))\n",
        "# Age vs Income\n",
        "plt.subplot(1, 3, 1)\n",
        "colors = ['gray' if label == -1 else plt.cm.viridis(label / max(1, n_clusters))\n",
        "for label in customer_labels]\n",
        "plt.scatter(age, income, c=colors, alpha=0.6)\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Income ($)')\n",
        "plt.title('Customer Segments: Age vs Income')\n",
        "plt.grid(True, alpha=0.3)\n",
        "# Age vs Spending\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.scatter(age, spending_score, c=colors, alpha=0.6)\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Spending Score')\n",
        "plt.title('Customer Segments: Age vs Spending')\n",
        "plt.grid(True, alpha=0.3)\n",
        "# Income vs Spending\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.scatter(income, spending_score, c=colors, alpha=0.6)\n",
        "plt.xlabel('Income ($)')\n",
        "plt.ylabel('Spending Score')\n",
        "plt.title('Customer Segments: Income vs Spending')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "# Cluster analysis\n",
        "print(\"\\nðŸ“ˆ CUSTOMER SEGMENT ANALYSIS:\")\n",
        "for cluster_id in range(n_clusters):\n",
        "cluster_mask = customer_labels == cluster_id\n",
        "cluster_size = np.sum(cluster_mask)\n",
        "if cluster_size > 0:\n",
        "avg_age = np.mean(age[cluster_mask])\n",
        "avg_income = np.mean(income[cluster_mask])\n",
        "avg_spending = np.mean(spending_score[cluster_mask])\n",
        "print(f\"\\nSegment {cluster_id} (Size: {cluster_size}):\")\n",
        "print(f\"â€¢ Average Age: {avg_age:.1f} years\")\n",
        "print(f\"â€¢ Average Income: ${avg_income:.0f}\")\n",
        "print(f\"â€¢ Average Spending: {avg_spending:.1f}\")\n",
        "print(\"âœ… DBSCAN Analysis Complete!\")"
      ],
      "id": "HzMLozM90ofO"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVYpa2120ofO"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "## DBSCAN Interview Questions & Answers\n",
        "**Q1: How do you choose the optimal eps and min_samples parameters?**\n",
        "**Answer:**\n",
        "- **K-distance graph:** Plot sorted k-nearest neighbor distances, choose eps at the \"elbow\"\n",
        "- **Domain knowledge:** Use understanding of data scale and expected cluster density\n",
        "- **Grid search:** Try different combinations and evaluate with silhouette score\n",
        "- **Rule of thumb:** min_samples â‰¥ dimensionality + 1\n",
        "- **Iterative approach:** Start with default values and adjust based on results\n",
        "**Q2: What are the advantages of DBSCAN over K-means?**\n",
        "**Answer:**\n",
        "- **Arbitrary cluster shapes:** Can find non-spherical clusters\n",
        "- **Noise handling:** Identifies outliers explicitly\n",
        "- **No need for K:** Discovers number of clusters automatically\n",
        "- **Density-based:** Finds clusters of varying densities\n",
        "- **Order invariance:** Result doesn't depend on point order\n",
        "**Q3: What are the limitations of DBSCAN?**\n",
        "**Answer:**\n",
        "- **Parameter sensitivity:** Results highly dependent on eps and min_samples\n",
        "- **Varying densities:** Struggles with clusters of significantly different densities\n",
        "- **High-dimensional data:** Distance measures become less meaningful\n",
        "- **Border points:** May assign border points arbitrarily to clusters\n",
        "- **Chain clusters:** Can create elongated clusters due to single linkage\n",
        "**Q4: How does DBSCAN handle clusters of different densities?**\n",
        "**Answer:**\n",
        "- **Challenge:** Single eps value may not work for all densities\n",
        "- **Solutions:**\n",
        "- **OPTICS:** Extension that handles varying densities\n",
        "- **HDBSCAN:** Hierarchical version that extracts clusters at different density levels\n",
        "- **Multiple runs:** Run DBSCAN with different parameters for different density regions\n",
        "- **Limitation:** Standard DBSCAN requires manual parameter tuning for each density level\n",
        "**Q5: When should you use DBSCAN vs other clustering algorithms?**\n",
        "**Answer:**\n",
        "**Use DBSCAN when:**\n",
        "- Data has noise/outliers\n",
        "- Clusters have arbitrary shapes\n",
        "- Don't know number of clusters in advance\n",
        "- Need to identify outliers explicitly\n",
        "- Clusters have similar densities\n",
        "**Use other algorithms when:**\n",
        "- Spherical clusters expected (K-means)\n",
        "- Hierarchical structure needed (Agglomerative)\n",
        "- Very high-dimensional data (spectral clustering)\n",
        "- Clusters have widely varying densities (HDBSCAN)\n",
        "- Need deterministic results (K-means++)\n",
        "---\n",
        "# ðŸŽ‰ COMPREHENSIVE MACHINE LEARNING GUIDE COMPLETE!\n",
        "This guide has covered all 17 major machine learning algorithms from your cheatsheet with:\n",
        "## ðŸ“š What We Covered:\n",
        "1. **Linear Regression** - Foundation of predictive modeling\n",
        "2. **Logistic Regression** - Probabilistic classification\n",
        "3. **Decision Tree** - Interpretable rule-based learning\n",
        "4. **Random Forest** - Robust ensemble method\n",
        "5. **Gradient Boosting** - State-of-the-art performance\n",
        "6. **SVM** - Maximum margin classification\n",
        "7. **KNN** - Simple instance-based learning\n",
        "8. **Naive Bayes** - Fast probabilistic classification\n",
        "9. **K-means** - Popular partitioning clustering\n",
        "10. **Hierarchical Clustering** - Tree-based cluster discovery\n",
        "11. **PCA** - Dimensionality reduction\n",
        "12. **Neural Networks (MLP)** - Universal function approximators\n",
        "13. **CNN** - Spatial pattern recognition\n",
        "14. **RNN** - Sequential data processing\n",
        "15. **Transformers** - Attention-based sequence modeling\n",
        "16. **Autoencoders** - Unsupervised representation learning\n",
        "17. **DBSCAN** - Density-based clustering\n",
        "## ðŸ› ï¸ For Each Algorithm:\n",
        "- **Mathematical foundations** with formulas\n",
        "- **Complete runnable code** with real datasets\n",
        "- **Visualizations** and diagrams\n",
        "- **Performance comparisons**\n",
        "- **Hyperparameter tuning**\n",
        "- **Real-world applications**\n",
        "- **Comprehensive interview Q&A**\n",
        "## ðŸš€ Next Steps:\n",
        "1. **Practice** with the provided code examples\n",
        "2. **Experiment** with different datasets and parameters\n",
        "3. **Combine algorithms** in ensemble methods\n",
        "4. **Explore advanced topics** like reinforcement learning, GANs, etc.\n",
        "5. **Build projects** to apply these algorithms to real problems\n",
        "This comprehensive guide provides everything needed to understand, implement, and discuss these machine learning algorithms effectively!"
      ],
      "id": "hVYpa2120ofO"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}